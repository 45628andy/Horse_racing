{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PL_9WoqZraP"
      },
      "source": [
        "# Data Classification Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BroVzL0h9nC"
      },
      "source": [
        "## Step 1: Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This assignment aims to predict the winner of a horse race given the race track condition and characteristics of the horses. This is a part of our larger project on horse gambling prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Data Collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSqjMgSrFEO7"
      },
      "source": [
        "We are using two seperate csv files merged together from a competition on kaggle. The first pdf, races.csv, includes the information about the race. The second pdf, runs.csv, includes information about individual horses. The data is from https://www.kaggle.com/datasets/lantanacamara/hong-kong-horse-racing and includes all horse races in Hong Kong. This is the best dataset for this problem given that better datasets are hidden behind paywall. The documentation for this dataset can be found on the kaggle page."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk1N5yrkar4U"
      },
      "source": [
        "## Step 3: Preprocessing (and Feature Engineering)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "pd.set_option('display.max_columns', 500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CJxyDV8Th8v9"
      },
      "outputs": [],
      "source": [
        "data_races = pd.read_csv('races.csv')\n",
        "data_runs = pd.read_csv('runs.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>race_id</th>\n",
              "      <th>date</th>\n",
              "      <th>venue</th>\n",
              "      <th>race_no</th>\n",
              "      <th>config</th>\n",
              "      <th>surface</th>\n",
              "      <th>distance</th>\n",
              "      <th>going</th>\n",
              "      <th>horse_ratings</th>\n",
              "      <th>prize</th>\n",
              "      <th>race_class</th>\n",
              "      <th>sec_time1</th>\n",
              "      <th>sec_time2</th>\n",
              "      <th>sec_time3</th>\n",
              "      <th>sec_time4</th>\n",
              "      <th>sec_time5</th>\n",
              "      <th>sec_time6</th>\n",
              "      <th>sec_time7</th>\n",
              "      <th>time1</th>\n",
              "      <th>time2</th>\n",
              "      <th>time3</th>\n",
              "      <th>time4</th>\n",
              "      <th>time5</th>\n",
              "      <th>time6</th>\n",
              "      <th>time7</th>\n",
              "      <th>place_combination1</th>\n",
              "      <th>place_combination2</th>\n",
              "      <th>place_combination3</th>\n",
              "      <th>place_combination4</th>\n",
              "      <th>place_dividend1</th>\n",
              "      <th>place_dividend2</th>\n",
              "      <th>place_dividend3</th>\n",
              "      <th>place_dividend4</th>\n",
              "      <th>win_combination1</th>\n",
              "      <th>win_dividend1</th>\n",
              "      <th>win_combination2</th>\n",
              "      <th>win_dividend2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1997-06-02</td>\n",
              "      <td>ST</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>0</td>\n",
              "      <td>1400</td>\n",
              "      <td>GOOD TO FIRM</td>\n",
              "      <td>40-15</td>\n",
              "      <td>485000.0</td>\n",
              "      <td>5</td>\n",
              "      <td>13.53</td>\n",
              "      <td>21.59</td>\n",
              "      <td>23.94</td>\n",
              "      <td>23.58</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.53</td>\n",
              "      <td>35.12</td>\n",
              "      <td>59.06</td>\n",
              "      <td>82.64</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8</td>\n",
              "      <td>11</td>\n",
              "      <td>6.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>36.5</td>\n",
              "      <td>25.5</td>\n",
              "      <td>18.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8</td>\n",
              "      <td>121.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1997-06-02</td>\n",
              "      <td>ST</td>\n",
              "      <td>2</td>\n",
              "      <td>A</td>\n",
              "      <td>0</td>\n",
              "      <td>1200</td>\n",
              "      <td>GOOD TO FIRM</td>\n",
              "      <td>40-15</td>\n",
              "      <td>485000.0</td>\n",
              "      <td>5</td>\n",
              "      <td>24.05</td>\n",
              "      <td>22.64</td>\n",
              "      <td>23.70</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>24.05</td>\n",
              "      <td>46.69</td>\n",
              "      <td>70.39</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>13</td>\n",
              "      <td>4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>12.5</td>\n",
              "      <td>47.0</td>\n",
              "      <td>33.5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>23.5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1997-06-02</td>\n",
              "      <td>ST</td>\n",
              "      <td>3</td>\n",
              "      <td>A</td>\n",
              "      <td>0</td>\n",
              "      <td>1400</td>\n",
              "      <td>GOOD TO FIRM</td>\n",
              "      <td>60-40</td>\n",
              "      <td>625000.0</td>\n",
              "      <td>4</td>\n",
              "      <td>13.77</td>\n",
              "      <td>22.22</td>\n",
              "      <td>24.88</td>\n",
              "      <td>22.82</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.77</td>\n",
              "      <td>35.99</td>\n",
              "      <td>60.87</td>\n",
              "      <td>83.69</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>13.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>23.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>59.5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>11</td>\n",
              "      <td>70.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1997-06-02</td>\n",
              "      <td>ST</td>\n",
              "      <td>4</td>\n",
              "      <td>A</td>\n",
              "      <td>0</td>\n",
              "      <td>1200</td>\n",
              "      <td>GOOD TO FIRM</td>\n",
              "      <td>120-95</td>\n",
              "      <td>1750000.0</td>\n",
              "      <td>1</td>\n",
              "      <td>24.33</td>\n",
              "      <td>22.47</td>\n",
              "      <td>22.09</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>24.33</td>\n",
              "      <td>46.80</td>\n",
              "      <td>68.89</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>10.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>14.0</td>\n",
              "      <td>24.5</td>\n",
              "      <td>16.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>52.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1997-06-02</td>\n",
              "      <td>ST</td>\n",
              "      <td>5</td>\n",
              "      <td>A</td>\n",
              "      <td>0</td>\n",
              "      <td>1600</td>\n",
              "      <td>GOOD TO FIRM</td>\n",
              "      <td>60-40</td>\n",
              "      <td>625000.0</td>\n",
              "      <td>4</td>\n",
              "      <td>25.45</td>\n",
              "      <td>23.52</td>\n",
              "      <td>23.31</td>\n",
              "      <td>23.56</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>25.45</td>\n",
              "      <td>48.97</td>\n",
              "      <td>72.28</td>\n",
              "      <td>95.84</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15.5</td>\n",
              "      <td>28.0</td>\n",
              "      <td>17.5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>36.5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   race_id        date venue  race_no config  surface  distance         going  \\\n",
              "0        0  1997-06-02    ST        1      A        0      1400  GOOD TO FIRM   \n",
              "1        1  1997-06-02    ST        2      A        0      1200  GOOD TO FIRM   \n",
              "2        2  1997-06-02    ST        3      A        0      1400  GOOD TO FIRM   \n",
              "3        3  1997-06-02    ST        4      A        0      1200  GOOD TO FIRM   \n",
              "4        4  1997-06-02    ST        5      A        0      1600  GOOD TO FIRM   \n",
              "\n",
              "  horse_ratings      prize  race_class  sec_time1  sec_time2  sec_time3  \\\n",
              "0         40-15   485000.0           5      13.53      21.59      23.94   \n",
              "1         40-15   485000.0           5      24.05      22.64      23.70   \n",
              "2         60-40   625000.0           4      13.77      22.22      24.88   \n",
              "3        120-95  1750000.0           1      24.33      22.47      22.09   \n",
              "4         60-40   625000.0           4      25.45      23.52      23.31   \n",
              "\n",
              "   sec_time4  sec_time5  sec_time6  sec_time7  time1  time2  time3  time4  \\\n",
              "0      23.58        NaN        NaN        NaN  13.53  35.12  59.06  82.64   \n",
              "1        NaN        NaN        NaN        NaN  24.05  46.69  70.39    NaN   \n",
              "2      22.82        NaN        NaN        NaN  13.77  35.99  60.87  83.69   \n",
              "3        NaN        NaN        NaN        NaN  24.33  46.80  68.89    NaN   \n",
              "4      23.56        NaN        NaN        NaN  25.45  48.97  72.28  95.84   \n",
              "\n",
              "   time5  time6  time7  place_combination1  place_combination2  \\\n",
              "0    NaN    NaN    NaN                   8                  11   \n",
              "1    NaN    NaN    NaN                   5                  13   \n",
              "2    NaN    NaN    NaN                  11                   1   \n",
              "3    NaN    NaN    NaN                   5                   3   \n",
              "4    NaN    NaN    NaN                   2                  10   \n",
              "\n",
              "   place_combination3  place_combination4  place_dividend1  place_dividend2  \\\n",
              "0                 6.0                 NaN             36.5             25.5   \n",
              "1                 4.0                 NaN             12.5             47.0   \n",
              "2                13.0                 NaN             23.0             23.0   \n",
              "3                10.0                 NaN             14.0             24.5   \n",
              "4                 1.0                 NaN             15.5             28.0   \n",
              "\n",
              "   place_dividend3  place_dividend4  win_combination1  win_dividend1  \\\n",
              "0             18.0              NaN                 8          121.0   \n",
              "1             33.5              NaN                 5           23.5   \n",
              "2             59.5              NaN                11           70.0   \n",
              "3             16.0              NaN                 5           52.0   \n",
              "4             17.5              NaN                 2           36.5   \n",
              "\n",
              "   win_combination2  win_dividend2  \n",
              "0               NaN            NaN  \n",
              "1               NaN            NaN  \n",
              "2               NaN            NaN  \n",
              "3               NaN            NaN  \n",
              "4               NaN            NaN  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_races.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>race_id</th>\n",
              "      <th>race_no</th>\n",
              "      <th>surface</th>\n",
              "      <th>distance</th>\n",
              "      <th>prize</th>\n",
              "      <th>race_class</th>\n",
              "      <th>sec_time1</th>\n",
              "      <th>sec_time2</th>\n",
              "      <th>sec_time3</th>\n",
              "      <th>sec_time4</th>\n",
              "      <th>sec_time5</th>\n",
              "      <th>sec_time6</th>\n",
              "      <th>sec_time7</th>\n",
              "      <th>time1</th>\n",
              "      <th>time2</th>\n",
              "      <th>time3</th>\n",
              "      <th>time4</th>\n",
              "      <th>time5</th>\n",
              "      <th>time6</th>\n",
              "      <th>time7</th>\n",
              "      <th>place_combination1</th>\n",
              "      <th>place_combination2</th>\n",
              "      <th>place_combination3</th>\n",
              "      <th>place_combination4</th>\n",
              "      <th>place_dividend1</th>\n",
              "      <th>place_dividend2</th>\n",
              "      <th>place_dividend3</th>\n",
              "      <th>place_dividend4</th>\n",
              "      <th>win_combination1</th>\n",
              "      <th>win_dividend1</th>\n",
              "      <th>win_combination2</th>\n",
              "      <th>win_dividend2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>6349.000000</td>\n",
              "      <td>6349.000000</td>\n",
              "      <td>6349.000000</td>\n",
              "      <td>6349.000000</td>\n",
              "      <td>5.887000e+03</td>\n",
              "      <td>6349.000000</td>\n",
              "      <td>6349.000000</td>\n",
              "      <td>6349.000000</td>\n",
              "      <td>6349.000000</td>\n",
              "      <td>3634.000000</td>\n",
              "      <td>821.000000</td>\n",
              "      <td>115.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6349.000000</td>\n",
              "      <td>6349.000000</td>\n",
              "      <td>6349.000000</td>\n",
              "      <td>3634.000000</td>\n",
              "      <td>821.000000</td>\n",
              "      <td>115.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6349.00000</td>\n",
              "      <td>6349.000000</td>\n",
              "      <td>6324.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>6349.000000</td>\n",
              "      <td>6349.000000</td>\n",
              "      <td>6324.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>6349.000000</td>\n",
              "      <td>6349.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>12.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3174.000000</td>\n",
              "      <td>5.226807</td>\n",
              "      <td>0.109151</td>\n",
              "      <td>1419.113246</td>\n",
              "      <td>1.134790e+06</td>\n",
              "      <td>3.893684</td>\n",
              "      <td>20.699466</td>\n",
              "      <td>22.826749</td>\n",
              "      <td>23.830743</td>\n",
              "      <td>23.852854</td>\n",
              "      <td>23.868685</td>\n",
              "      <td>23.912261</td>\n",
              "      <td>NaN</td>\n",
              "      <td>20.699466</td>\n",
              "      <td>43.526215</td>\n",
              "      <td>67.356959</td>\n",
              "      <td>91.735793</td>\n",
              "      <td>112.479695</td>\n",
              "      <td>140.349739</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.23862</td>\n",
              "      <td>6.366357</td>\n",
              "      <td>6.525933</td>\n",
              "      <td>8.434783</td>\n",
              "      <td>27.778469</td>\n",
              "      <td>32.802536</td>\n",
              "      <td>38.969292</td>\n",
              "      <td>22.013043</td>\n",
              "      <td>6.238620</td>\n",
              "      <td>96.096944</td>\n",
              "      <td>8.166667</td>\n",
              "      <td>101.416667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1832.942761</td>\n",
              "      <td>2.795019</td>\n",
              "      <td>0.311853</td>\n",
              "      <td>281.468745</td>\n",
              "      <td>1.749156e+06</td>\n",
              "      <td>1.992868</td>\n",
              "      <td>5.880319</td>\n",
              "      <td>1.044998</td>\n",
              "      <td>0.870355</td>\n",
              "      <td>0.820277</td>\n",
              "      <td>0.758860</td>\n",
              "      <td>0.667664</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.880319</td>\n",
              "      <td>6.657225</td>\n",
              "      <td>6.978638</td>\n",
              "      <td>7.814997</td>\n",
              "      <td>5.452576</td>\n",
              "      <td>4.910272</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.61858</td>\n",
              "      <td>3.662594</td>\n",
              "      <td>3.731685</td>\n",
              "      <td>2.982160</td>\n",
              "      <td>25.175925</td>\n",
              "      <td>30.275423</td>\n",
              "      <td>37.015938</td>\n",
              "      <td>16.757802</td>\n",
              "      <td>3.620321</td>\n",
              "      <td>131.221259</td>\n",
              "      <td>3.325749</td>\n",
              "      <td>101.672566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>4.850000e+05</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>12.390000</td>\n",
              "      <td>20.060000</td>\n",
              "      <td>21.200000</td>\n",
              "      <td>21.400000</td>\n",
              "      <td>21.810000</td>\n",
              "      <td>21.770000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>12.390000</td>\n",
              "      <td>33.110000</td>\n",
              "      <td>55.160000</td>\n",
              "      <td>80.430000</td>\n",
              "      <td>105.830000</td>\n",
              "      <td>132.840000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>10.100000</td>\n",
              "      <td>10.100000</td>\n",
              "      <td>10.100000</td>\n",
              "      <td>10.100000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>10.500000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>12.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1587.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1200.000000</td>\n",
              "      <td>6.750000e+05</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>13.690000</td>\n",
              "      <td>22.140000</td>\n",
              "      <td>23.210000</td>\n",
              "      <td>23.300000</td>\n",
              "      <td>23.410000</td>\n",
              "      <td>23.550000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.690000</td>\n",
              "      <td>35.950000</td>\n",
              "      <td>59.830000</td>\n",
              "      <td>83.410000</td>\n",
              "      <td>109.010000</td>\n",
              "      <td>137.290000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.00000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>6.500000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>18.500000</td>\n",
              "      <td>10.100000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>34.500000</td>\n",
              "      <td>5.500000</td>\n",
              "      <td>25.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>3174.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1400.000000</td>\n",
              "      <td>8.400000e+05</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>23.720000</td>\n",
              "      <td>22.800000</td>\n",
              "      <td>23.720000</td>\n",
              "      <td>23.770000</td>\n",
              "      <td>23.830000</td>\n",
              "      <td>24.020000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>23.720000</td>\n",
              "      <td>46.410000</td>\n",
              "      <td>69.720000</td>\n",
              "      <td>94.300000</td>\n",
              "      <td>110.330000</td>\n",
              "      <td>138.360000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.00000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>27.500000</td>\n",
              "      <td>15.500000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>58.500000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>50.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>4761.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1650.000000</td>\n",
              "      <td>1.060000e+06</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>24.700000</td>\n",
              "      <td>23.450000</td>\n",
              "      <td>24.380000</td>\n",
              "      <td>24.330000</td>\n",
              "      <td>24.280000</td>\n",
              "      <td>24.295000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>24.700000</td>\n",
              "      <td>48.070000</td>\n",
              "      <td>72.100000</td>\n",
              "      <td>100.030000</td>\n",
              "      <td>113.150000</td>\n",
              "      <td>140.205000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9.00000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>36.500000</td>\n",
              "      <td>44.000000</td>\n",
              "      <td>24.750000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>105.500000</td>\n",
              "      <td>11.250000</td>\n",
              "      <td>178.625000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>6348.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2400.000000</td>\n",
              "      <td>2.500000e+07</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>30.030000</td>\n",
              "      <td>27.410000</td>\n",
              "      <td>27.580000</td>\n",
              "      <td>28.920000</td>\n",
              "      <td>26.500000</td>\n",
              "      <td>25.920000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>30.030000</td>\n",
              "      <td>56.220000</td>\n",
              "      <td>81.290000</td>\n",
              "      <td>107.810000</td>\n",
              "      <td>134.310000</td>\n",
              "      <td>158.490000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>14.00000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>410.500000</td>\n",
              "      <td>627.000000</td>\n",
              "      <td>420.500000</td>\n",
              "      <td>68.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>2687.500000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>282.500000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           race_id      race_no      surface     distance         prize  \\\n",
              "count  6349.000000  6349.000000  6349.000000  6349.000000  5.887000e+03   \n",
              "mean   3174.000000     5.226807     0.109151  1419.113246  1.134790e+06   \n",
              "std    1832.942761     2.795019     0.311853   281.468745  1.749156e+06   \n",
              "min       0.000000     1.000000     0.000000  1000.000000  4.850000e+05   \n",
              "25%    1587.000000     3.000000     0.000000  1200.000000  6.750000e+05   \n",
              "50%    3174.000000     5.000000     0.000000  1400.000000  8.400000e+05   \n",
              "75%    4761.000000     7.000000     0.000000  1650.000000  1.060000e+06   \n",
              "max    6348.000000    11.000000     1.000000  2400.000000  2.500000e+07   \n",
              "\n",
              "        race_class    sec_time1    sec_time2    sec_time3    sec_time4  \\\n",
              "count  6349.000000  6349.000000  6349.000000  6349.000000  3634.000000   \n",
              "mean      3.893684    20.699466    22.826749    23.830743    23.852854   \n",
              "std       1.992868     5.880319     1.044998     0.870355     0.820277   \n",
              "min       0.000000    12.390000    20.060000    21.200000    21.400000   \n",
              "25%       3.000000    13.690000    22.140000    23.210000    23.300000   \n",
              "50%       4.000000    23.720000    22.800000    23.720000    23.770000   \n",
              "75%       4.000000    24.700000    23.450000    24.380000    24.330000   \n",
              "max      13.000000    30.030000    27.410000    27.580000    28.920000   \n",
              "\n",
              "        sec_time5   sec_time6  sec_time7        time1        time2  \\\n",
              "count  821.000000  115.000000        0.0  6349.000000  6349.000000   \n",
              "mean    23.868685   23.912261        NaN    20.699466    43.526215   \n",
              "std      0.758860    0.667664        NaN     5.880319     6.657225   \n",
              "min     21.810000   21.770000        NaN    12.390000    33.110000   \n",
              "25%     23.410000   23.550000        NaN    13.690000    35.950000   \n",
              "50%     23.830000   24.020000        NaN    23.720000    46.410000   \n",
              "75%     24.280000   24.295000        NaN    24.700000    48.070000   \n",
              "max     26.500000   25.920000        NaN    30.030000    56.220000   \n",
              "\n",
              "             time3        time4       time5       time6  time7  \\\n",
              "count  6349.000000  3634.000000  821.000000  115.000000    0.0   \n",
              "mean     67.356959    91.735793  112.479695  140.349739    NaN   \n",
              "std       6.978638     7.814997    5.452576    4.910272    NaN   \n",
              "min      55.160000    80.430000  105.830000  132.840000    NaN   \n",
              "25%      59.830000    83.410000  109.010000  137.290000    NaN   \n",
              "50%      69.720000    94.300000  110.330000  138.360000    NaN   \n",
              "75%      72.100000   100.030000  113.150000  140.205000    NaN   \n",
              "max      81.290000   107.810000  134.310000  158.490000    NaN   \n",
              "\n",
              "       place_combination1  place_combination2  place_combination3  \\\n",
              "count          6349.00000         6349.000000         6324.000000   \n",
              "mean              6.23862            6.366357            6.525933   \n",
              "std               3.61858            3.662594            3.731685   \n",
              "min               1.00000            1.000000            1.000000   \n",
              "25%               3.00000            3.000000            3.000000   \n",
              "50%               6.00000            6.000000            6.000000   \n",
              "75%               9.00000            9.000000           10.000000   \n",
              "max              14.00000           14.000000           14.000000   \n",
              "\n",
              "       place_combination4  place_dividend1  place_dividend2  place_dividend3  \\\n",
              "count           23.000000      6349.000000      6349.000000      6324.000000   \n",
              "mean             8.434783        27.778469        32.802536        38.969292   \n",
              "std              2.982160        25.175925        30.275423        37.015938   \n",
              "min              4.000000        10.100000        10.100000        10.100000   \n",
              "25%              6.500000        15.000000        17.000000        18.500000   \n",
              "50%              8.000000        20.000000        24.000000        27.500000   \n",
              "75%             11.000000        31.000000        36.500000        44.000000   \n",
              "max             14.000000       410.500000       627.000000       420.500000   \n",
              "\n",
              "       place_dividend4  win_combination1  win_dividend1  win_combination2  \\\n",
              "count        23.000000       6349.000000    6349.000000         12.000000   \n",
              "mean         22.013043          6.238620      96.096944          8.166667   \n",
              "std          16.757802          3.620321     131.221259          3.325749   \n",
              "min          10.100000          1.000000      10.500000          3.000000   \n",
              "25%          10.100000          3.000000      34.500000          5.500000   \n",
              "50%          15.500000          6.000000      58.500000          8.000000   \n",
              "75%          24.750000          9.000000     105.500000         11.250000   \n",
              "max          68.000000         14.000000    2687.500000         12.000000   \n",
              "\n",
              "       win_dividend2  \n",
              "count      12.000000  \n",
              "mean      101.416667  \n",
              "std       101.672566  \n",
              "min        12.000000  \n",
              "25%        25.250000  \n",
              "50%        50.250000  \n",
              "75%       178.625000  \n",
              "max       282.500000  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check for outliers and missing values\n",
        "data_races.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "race_id                  0\n",
              "date                     0\n",
              "venue                    0\n",
              "race_no                  0\n",
              "config                   0\n",
              "surface                  0\n",
              "distance                 0\n",
              "going                    0\n",
              "horse_ratings            0\n",
              "prize                  462\n",
              "race_class               0\n",
              "sec_time1                0\n",
              "sec_time2                0\n",
              "sec_time3                0\n",
              "sec_time4             2715\n",
              "sec_time5             5528\n",
              "sec_time6             6234\n",
              "sec_time7             6349\n",
              "time1                    0\n",
              "time2                    0\n",
              "time3                    0\n",
              "time4                 2715\n",
              "time5                 5528\n",
              "time6                 6234\n",
              "time7                 6349\n",
              "place_combination1       0\n",
              "place_combination2       0\n",
              "place_combination3      25\n",
              "place_combination4    6326\n",
              "place_dividend1          0\n",
              "place_dividend2          0\n",
              "place_dividend3         25\n",
              "place_dividend4       6326\n",
              "win_combination1         0\n",
              "win_dividend1            0\n",
              "win_combination2      6337\n",
              "win_dividend2         6337\n",
              "dtype: int64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_races.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>race_id</th>\n",
              "      <th>horse_no</th>\n",
              "      <th>horse_id</th>\n",
              "      <th>result</th>\n",
              "      <th>won</th>\n",
              "      <th>lengths_behind</th>\n",
              "      <th>horse_age</th>\n",
              "      <th>horse_country</th>\n",
              "      <th>horse_type</th>\n",
              "      <th>horse_rating</th>\n",
              "      <th>horse_gear</th>\n",
              "      <th>declared_weight</th>\n",
              "      <th>actual_weight</th>\n",
              "      <th>draw</th>\n",
              "      <th>position_sec1</th>\n",
              "      <th>position_sec2</th>\n",
              "      <th>position_sec3</th>\n",
              "      <th>position_sec4</th>\n",
              "      <th>position_sec5</th>\n",
              "      <th>position_sec6</th>\n",
              "      <th>behind_sec1</th>\n",
              "      <th>behind_sec2</th>\n",
              "      <th>behind_sec3</th>\n",
              "      <th>behind_sec4</th>\n",
              "      <th>behind_sec5</th>\n",
              "      <th>behind_sec6</th>\n",
              "      <th>time1</th>\n",
              "      <th>time2</th>\n",
              "      <th>time3</th>\n",
              "      <th>time4</th>\n",
              "      <th>time5</th>\n",
              "      <th>time6</th>\n",
              "      <th>finish_time</th>\n",
              "      <th>win_odds</th>\n",
              "      <th>place_odds</th>\n",
              "      <th>trainer_id</th>\n",
              "      <th>jockey_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3917</td>\n",
              "      <td>10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.00</td>\n",
              "      <td>3</td>\n",
              "      <td>AUS</td>\n",
              "      <td>Gelding</td>\n",
              "      <td>60</td>\n",
              "      <td>--</td>\n",
              "      <td>1020.0</td>\n",
              "      <td>133</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>10.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.00</td>\n",
              "      <td>2.00</td>\n",
              "      <td>1.50</td>\n",
              "      <td>8.00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.85</td>\n",
              "      <td>21.59</td>\n",
              "      <td>23.86</td>\n",
              "      <td>24.62</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>83.92</td>\n",
              "      <td>9.7</td>\n",
              "      <td>3.7</td>\n",
              "      <td>118</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2157</td>\n",
              "      <td>8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.75</td>\n",
              "      <td>3</td>\n",
              "      <td>NZ</td>\n",
              "      <td>Gelding</td>\n",
              "      <td>60</td>\n",
              "      <td>--</td>\n",
              "      <td>980.0</td>\n",
              "      <td>133</td>\n",
              "      <td>12</td>\n",
              "      <td>12</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>8.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.50</td>\n",
              "      <td>9.00</td>\n",
              "      <td>5.00</td>\n",
              "      <td>5.75</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>14.57</td>\n",
              "      <td>21.99</td>\n",
              "      <td>23.30</td>\n",
              "      <td>23.70</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>83.56</td>\n",
              "      <td>16.0</td>\n",
              "      <td>4.9</td>\n",
              "      <td>164</td>\n",
              "      <td>57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>858</td>\n",
              "      <td>7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.75</td>\n",
              "      <td>3</td>\n",
              "      <td>NZ</td>\n",
              "      <td>Gelding</td>\n",
              "      <td>60</td>\n",
              "      <td>--</td>\n",
              "      <td>1082.0</td>\n",
              "      <td>132</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>4.75</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.69</td>\n",
              "      <td>21.59</td>\n",
              "      <td>23.90</td>\n",
              "      <td>24.22</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>83.40</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.5</td>\n",
              "      <td>137</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1853</td>\n",
              "      <td>9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.25</td>\n",
              "      <td>3</td>\n",
              "      <td>SAF</td>\n",
              "      <td>Gelding</td>\n",
              "      <td>60</td>\n",
              "      <td>--</td>\n",
              "      <td>1118.0</td>\n",
              "      <td>127</td>\n",
              "      <td>13</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>11</td>\n",
              "      <td>9.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.50</td>\n",
              "      <td>5.00</td>\n",
              "      <td>3.50</td>\n",
              "      <td>6.25</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>14.09</td>\n",
              "      <td>21.83</td>\n",
              "      <td>23.70</td>\n",
              "      <td>24.00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>83.62</td>\n",
              "      <td>39.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>80</td>\n",
              "      <td>59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>2796</td>\n",
              "      <td>6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.75</td>\n",
              "      <td>3</td>\n",
              "      <td>GB</td>\n",
              "      <td>Gelding</td>\n",
              "      <td>60</td>\n",
              "      <td>--</td>\n",
              "      <td>972.0</td>\n",
              "      <td>131</td>\n",
              "      <td>14</td>\n",
              "      <td>13</td>\n",
              "      <td>12</td>\n",
              "      <td>12</td>\n",
              "      <td>6.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7.75</td>\n",
              "      <td>8.75</td>\n",
              "      <td>4.25</td>\n",
              "      <td>3.75</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>14.77</td>\n",
              "      <td>21.75</td>\n",
              "      <td>23.22</td>\n",
              "      <td>23.50</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>83.24</td>\n",
              "      <td>50.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>9</td>\n",
              "      <td>154</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   race_id  horse_no  horse_id  result  won  lengths_behind  horse_age  \\\n",
              "0        0         1      3917      10  0.0            8.00          3   \n",
              "1        0         2      2157       8  0.0            5.75          3   \n",
              "2        0         3       858       7  0.0            4.75          3   \n",
              "3        0         4      1853       9  0.0            6.25          3   \n",
              "4        0         5      2796       6  0.0            3.75          3   \n",
              "\n",
              "  horse_country horse_type  horse_rating horse_gear  declared_weight  \\\n",
              "0           AUS    Gelding            60         --           1020.0   \n",
              "1            NZ    Gelding            60         --            980.0   \n",
              "2            NZ    Gelding            60         --           1082.0   \n",
              "3           SAF    Gelding            60         --           1118.0   \n",
              "4            GB    Gelding            60         --            972.0   \n",
              "\n",
              "   actual_weight  draw  position_sec1  position_sec2  position_sec3  \\\n",
              "0            133     7              6              4              6   \n",
              "1            133    12             12             13             13   \n",
              "2            132     8              3              2              2   \n",
              "3            127    13              8              8             11   \n",
              "4            131    14             13             12             12   \n",
              "\n",
              "   position_sec4  position_sec5  position_sec6  behind_sec1  behind_sec2  \\\n",
              "0           10.0            NaN            NaN         2.00         2.00   \n",
              "1            8.0            NaN            NaN         6.50         9.00   \n",
              "2            7.0            NaN            NaN         1.00         1.00   \n",
              "3            9.0            NaN            NaN         3.50         5.00   \n",
              "4            6.0            NaN            NaN         7.75         8.75   \n",
              "\n",
              "   behind_sec3  behind_sec4  behind_sec5  behind_sec6  time1  time2  time3  \\\n",
              "0         1.50         8.00          NaN          NaN  13.85  21.59  23.86   \n",
              "1         5.00         5.75          NaN          NaN  14.57  21.99  23.30   \n",
              "2         0.75         4.75          NaN          NaN  13.69  21.59  23.90   \n",
              "3         3.50         6.25          NaN          NaN  14.09  21.83  23.70   \n",
              "4         4.25         3.75          NaN          NaN  14.77  21.75  23.22   \n",
              "\n",
              "   time4  time5  time6  finish_time  win_odds  place_odds  trainer_id  \\\n",
              "0  24.62    NaN    NaN        83.92       9.7         3.7         118   \n",
              "1  23.70    NaN    NaN        83.56      16.0         4.9         164   \n",
              "2  24.22    NaN    NaN        83.40       3.5         1.5         137   \n",
              "3  24.00    NaN    NaN        83.62      39.0        11.0          80   \n",
              "4  23.50    NaN    NaN        83.24      50.0        14.0           9   \n",
              "\n",
              "   jockey_id  \n",
              "0          2  \n",
              "1         57  \n",
              "2         18  \n",
              "3         59  \n",
              "4        154  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_runs.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>race_id</th>\n",
              "      <th>horse_no</th>\n",
              "      <th>horse_id</th>\n",
              "      <th>result</th>\n",
              "      <th>won</th>\n",
              "      <th>lengths_behind</th>\n",
              "      <th>horse_age</th>\n",
              "      <th>horse_rating</th>\n",
              "      <th>declared_weight</th>\n",
              "      <th>actual_weight</th>\n",
              "      <th>draw</th>\n",
              "      <th>position_sec1</th>\n",
              "      <th>position_sec2</th>\n",
              "      <th>position_sec3</th>\n",
              "      <th>position_sec4</th>\n",
              "      <th>position_sec5</th>\n",
              "      <th>position_sec6</th>\n",
              "      <th>behind_sec1</th>\n",
              "      <th>behind_sec2</th>\n",
              "      <th>behind_sec3</th>\n",
              "      <th>behind_sec4</th>\n",
              "      <th>behind_sec5</th>\n",
              "      <th>behind_sec6</th>\n",
              "      <th>time1</th>\n",
              "      <th>time2</th>\n",
              "      <th>time3</th>\n",
              "      <th>time4</th>\n",
              "      <th>time5</th>\n",
              "      <th>time6</th>\n",
              "      <th>finish_time</th>\n",
              "      <th>win_odds</th>\n",
              "      <th>place_odds</th>\n",
              "      <th>trainer_id</th>\n",
              "      <th>jockey_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>79447.000000</td>\n",
              "      <td>79447.000000</td>\n",
              "      <td>79447.000000</td>\n",
              "      <td>79447.000000</td>\n",
              "      <td>79447.000000</td>\n",
              "      <td>79447.000000</td>\n",
              "      <td>79447.000000</td>\n",
              "      <td>79447.000000</td>\n",
              "      <td>79447.000000</td>\n",
              "      <td>79447.000000</td>\n",
              "      <td>79447.000000</td>\n",
              "      <td>79447.000000</td>\n",
              "      <td>79447.000000</td>\n",
              "      <td>79447.000000</td>\n",
              "      <td>46226.000000</td>\n",
              "      <td>10079.000000</td>\n",
              "      <td>1296.000000</td>\n",
              "      <td>79447.000000</td>\n",
              "      <td>79447.000000</td>\n",
              "      <td>79447.000000</td>\n",
              "      <td>46226.000000</td>\n",
              "      <td>10079.000000</td>\n",
              "      <td>1296.000000</td>\n",
              "      <td>79447.000000</td>\n",
              "      <td>79447.000000</td>\n",
              "      <td>79447.000000</td>\n",
              "      <td>46226.000000</td>\n",
              "      <td>10079.000000</td>\n",
              "      <td>1296.000000</td>\n",
              "      <td>79447.000000</td>\n",
              "      <td>79447.000000</td>\n",
              "      <td>75712.000000</td>\n",
              "      <td>79447.000000</td>\n",
              "      <td>79447.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3173.352814</td>\n",
              "      <td>6.905623</td>\n",
              "      <td>2204.410525</td>\n",
              "      <td>6.838597</td>\n",
              "      <td>0.080053</td>\n",
              "      <td>6.108901</td>\n",
              "      <td>3.339346</td>\n",
              "      <td>61.034904</td>\n",
              "      <td>1104.953568</td>\n",
              "      <td>122.729656</td>\n",
              "      <td>6.876005</td>\n",
              "      <td>6.849837</td>\n",
              "      <td>6.846791</td>\n",
              "      <td>6.843443</td>\n",
              "      <td>6.945550</td>\n",
              "      <td>6.748388</td>\n",
              "      <td>6.253086</td>\n",
              "      <td>3.378768</td>\n",
              "      <td>4.083972</td>\n",
              "      <td>4.509457</td>\n",
              "      <td>5.992076</td>\n",
              "      <td>6.026654</td>\n",
              "      <td>10.638735</td>\n",
              "      <td>21.135438</td>\n",
              "      <td>22.928985</td>\n",
              "      <td>23.864054</td>\n",
              "      <td>24.039662</td>\n",
              "      <td>24.105221</td>\n",
              "      <td>24.350216</td>\n",
              "      <td>85.322914</td>\n",
              "      <td>28.812977</td>\n",
              "      <td>7.423177</td>\n",
              "      <td>79.793007</td>\n",
              "      <td>85.832341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1833.101494</td>\n",
              "      <td>3.760711</td>\n",
              "      <td>1275.049375</td>\n",
              "      <td>3.730498</td>\n",
              "      <td>0.271378</td>\n",
              "      <td>33.636209</td>\n",
              "      <td>0.876763</td>\n",
              "      <td>11.748788</td>\n",
              "      <td>62.347597</td>\n",
              "      <td>6.305496</td>\n",
              "      <td>3.747589</td>\n",
              "      <td>3.734348</td>\n",
              "      <td>3.733014</td>\n",
              "      <td>3.732055</td>\n",
              "      <td>3.792138</td>\n",
              "      <td>3.702360</td>\n",
              "      <td>3.426108</td>\n",
              "      <td>4.282529</td>\n",
              "      <td>2.691107</td>\n",
              "      <td>16.541538</td>\n",
              "      <td>33.991084</td>\n",
              "      <td>31.754623</td>\n",
              "      <td>67.791252</td>\n",
              "      <td>6.930518</td>\n",
              "      <td>3.599727</td>\n",
              "      <td>3.571163</td>\n",
              "      <td>4.663367</td>\n",
              "      <td>1.127963</td>\n",
              "      <td>1.314755</td>\n",
              "      <td>18.512883</td>\n",
              "      <td>30.097375</td>\n",
              "      <td>8.823430</td>\n",
              "      <td>45.118874</td>\n",
              "      <td>54.338105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.500000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>693.000000</td>\n",
              "      <td>103.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.150000</td>\n",
              "      <td>0.150000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>12.390000</td>\n",
              "      <td>19.990000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>21.200000</td>\n",
              "      <td>21.420000</td>\n",
              "      <td>21.500000</td>\n",
              "      <td>55.160000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1586.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1085.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.750000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>1062.000000</td>\n",
              "      <td>118.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.500000</td>\n",
              "      <td>1.750000</td>\n",
              "      <td>1.750000</td>\n",
              "      <td>1.750000</td>\n",
              "      <td>1.750000</td>\n",
              "      <td>1.750000</td>\n",
              "      <td>14.120000</td>\n",
              "      <td>22.290000</td>\n",
              "      <td>23.230000</td>\n",
              "      <td>23.320000</td>\n",
              "      <td>23.410000</td>\n",
              "      <td>23.570000</td>\n",
              "      <td>70.590000</td>\n",
              "      <td>7.700000</td>\n",
              "      <td>2.400000</td>\n",
              "      <td>47.000000</td>\n",
              "      <td>39.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>3174.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>2209.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>1102.000000</td>\n",
              "      <td>123.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.750000</td>\n",
              "      <td>3.750000</td>\n",
              "      <td>3.750000</td>\n",
              "      <td>3.750000</td>\n",
              "      <td>4.250000</td>\n",
              "      <td>24.180000</td>\n",
              "      <td>22.870000</td>\n",
              "      <td>23.760000</td>\n",
              "      <td>23.890000</td>\n",
              "      <td>23.960000</td>\n",
              "      <td>24.120000</td>\n",
              "      <td>83.350000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>4.100000</td>\n",
              "      <td>75.000000</td>\n",
              "      <td>76.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>4764.500000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>3308.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.750000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>1146.000000</td>\n",
              "      <td>128.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.750000</td>\n",
              "      <td>5.750000</td>\n",
              "      <td>6.250000</td>\n",
              "      <td>6.500000</td>\n",
              "      <td>7.750000</td>\n",
              "      <td>25.360000</td>\n",
              "      <td>23.520000</td>\n",
              "      <td>24.410000</td>\n",
              "      <td>24.560000</td>\n",
              "      <td>24.630000</td>\n",
              "      <td>24.820000</td>\n",
              "      <td>100.780000</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>8.600000</td>\n",
              "      <td>118.000000</td>\n",
              "      <td>138.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>6348.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>4404.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>138.000000</td>\n",
              "      <td>1369.000000</td>\n",
              "      <td>133.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>60.250000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>49.570000</td>\n",
              "      <td>34.150000</td>\n",
              "      <td>163.580000</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>101.000000</td>\n",
              "      <td>175.000000</td>\n",
              "      <td>185.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            race_id      horse_no      horse_id        result           won  \\\n",
              "count  79447.000000  79447.000000  79447.000000  79447.000000  79447.000000   \n",
              "mean    3173.352814      6.905623   2204.410525      6.838597      0.080053   \n",
              "std     1833.101494      3.760711   1275.049375      3.730498      0.271378   \n",
              "min        0.000000      1.000000      0.000000      1.000000      0.000000   \n",
              "25%     1586.000000      4.000000   1085.000000      4.000000      0.000000   \n",
              "50%     3174.000000      7.000000   2209.000000      7.000000      0.000000   \n",
              "75%     4764.500000     10.000000   3308.000000     10.000000      0.000000   \n",
              "max     6348.000000     14.000000   4404.000000     14.000000      1.000000   \n",
              "\n",
              "       lengths_behind     horse_age  horse_rating  declared_weight  \\\n",
              "count    79447.000000  79447.000000  79447.000000     79447.000000   \n",
              "mean         6.108901      3.339346     61.034904      1104.953568   \n",
              "std         33.636209      0.876763     11.748788        62.347597   \n",
              "min         -0.500000      2.000000     10.000000       693.000000   \n",
              "25%          1.750000      3.000000     60.000000      1062.000000   \n",
              "50%          4.000000      3.000000     60.000000      1102.000000   \n",
              "75%          6.750000      3.000000     60.000000      1146.000000   \n",
              "max        999.000000     10.000000    138.000000      1369.000000   \n",
              "\n",
              "       actual_weight          draw  position_sec1  position_sec2  \\\n",
              "count   79447.000000  79447.000000   79447.000000   79447.000000   \n",
              "mean      122.729656      6.876005       6.849837       6.846791   \n",
              "std         6.305496      3.747589       3.734348       3.733014   \n",
              "min       103.000000      1.000000       1.000000       1.000000   \n",
              "25%       118.000000      4.000000       4.000000       4.000000   \n",
              "50%       123.000000      7.000000       7.000000       7.000000   \n",
              "75%       128.000000     10.000000      10.000000      10.000000   \n",
              "max       133.000000     15.000000      14.000000      14.000000   \n",
              "\n",
              "       position_sec3  position_sec4  position_sec5  position_sec6  \\\n",
              "count   79447.000000   46226.000000   10079.000000    1296.000000   \n",
              "mean        6.843443       6.945550       6.748388       6.253086   \n",
              "std         3.732055       3.792138       3.702360       3.426108   \n",
              "min         1.000000       1.000000       1.000000       1.000000   \n",
              "25%         4.000000       4.000000       4.000000       3.000000   \n",
              "50%         7.000000       7.000000       7.000000       6.000000   \n",
              "75%        10.000000      10.000000      10.000000       9.000000   \n",
              "max        14.000000      14.000000      14.000000      14.000000   \n",
              "\n",
              "        behind_sec1   behind_sec2   behind_sec3   behind_sec4   behind_sec5  \\\n",
              "count  79447.000000  79447.000000  79447.000000  46226.000000  10079.000000   \n",
              "mean       3.378768      4.083972      4.509457      5.992076      6.026654   \n",
              "std        4.282529      2.691107     16.541538     33.991084     31.754623   \n",
              "min        0.150000      0.150000      0.000000      0.000000      0.100000   \n",
              "25%        1.500000      1.750000      1.750000      1.750000      1.750000   \n",
              "50%        3.000000      3.750000      3.750000      3.750000      3.750000   \n",
              "75%        5.000000      5.750000      5.750000      6.250000      6.500000   \n",
              "max      999.000000     60.250000    999.000000    999.000000    999.000000   \n",
              "\n",
              "       behind_sec6         time1         time2         time3         time4  \\\n",
              "count  1296.000000  79447.000000  79447.000000  79447.000000  46226.000000   \n",
              "mean     10.638735     21.135438     22.928985     23.864054     24.039662   \n",
              "std      67.791252      6.930518      3.599727      3.571163      4.663367   \n",
              "min       0.000000     12.390000     19.990000     21.000000     21.200000   \n",
              "25%       1.750000     14.120000     22.290000     23.230000     23.320000   \n",
              "50%       4.250000     24.180000     22.870000     23.760000     23.890000   \n",
              "75%       7.750000     25.360000     23.520000     24.410000     24.560000   \n",
              "max     999.000000    999.000000    999.000000    999.000000    999.000000   \n",
              "\n",
              "              time5        time6   finish_time      win_odds    place_odds  \\\n",
              "count  10079.000000  1296.000000  79447.000000  79447.000000  75712.000000   \n",
              "mean      24.105221    24.350216     85.322914     28.812977      7.423177   \n",
              "std        1.127963     1.314755     18.512883     30.097375      8.823430   \n",
              "min       21.420000    21.500000     55.160000      1.000000      1.000000   \n",
              "25%       23.410000    23.570000     70.590000      7.700000      2.400000   \n",
              "50%       23.960000    24.120000     83.350000     15.000000      4.100000   \n",
              "75%       24.630000    24.820000    100.780000     38.000000      8.600000   \n",
              "max       49.570000    34.150000    163.580000     99.000000    101.000000   \n",
              "\n",
              "         trainer_id     jockey_id  \n",
              "count  79447.000000  79447.000000  \n",
              "mean      79.793007     85.832341  \n",
              "std       45.118874     54.338105  \n",
              "min        0.000000      0.000000  \n",
              "25%       47.000000     39.000000  \n",
              "50%       75.000000     76.000000  \n",
              "75%      118.000000    138.000000  \n",
              "max      175.000000    185.000000  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check for outliers and missing values\n",
        "data_runs.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "race_id                0\n",
              "horse_no               0\n",
              "horse_id               0\n",
              "result                 0\n",
              "won                    0\n",
              "lengths_behind         0\n",
              "horse_age              0\n",
              "horse_country          2\n",
              "horse_type             2\n",
              "horse_rating           0\n",
              "horse_gear             0\n",
              "declared_weight        0\n",
              "actual_weight          0\n",
              "draw                   0\n",
              "position_sec1          0\n",
              "position_sec2          0\n",
              "position_sec3          0\n",
              "position_sec4      33221\n",
              "position_sec5      69368\n",
              "position_sec6      78151\n",
              "behind_sec1            0\n",
              "behind_sec2            0\n",
              "behind_sec3            0\n",
              "behind_sec4        33221\n",
              "behind_sec5        69368\n",
              "behind_sec6        78151\n",
              "time1                  0\n",
              "time2                  0\n",
              "time3                  0\n",
              "time4              33221\n",
              "time5              69368\n",
              "time6              78151\n",
              "finish_time            0\n",
              "win_odds               0\n",
              "place_odds          3735\n",
              "trainer_id             0\n",
              "jockey_id              0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_runs.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UMq4HmSPAA-1"
      },
      "outputs": [],
      "source": [
        "# Only include variables that are relevant to the prediction\n",
        "data_races = data_races[['race_id', 'venue', 'config', 'surface', 'distance',\n",
        "       'going', 'horse_ratings', 'prize', 'race_class', 'place_combination1']]\n",
        "\n",
        "# Impute missing prize only since other not important\n",
        "mean_value = data_races['prize'].mean()\n",
        "for i in range(len(data_races['prize'])):\n",
        "    if pd.isna(data_races.iloc[i,7]):\n",
        "        data_races.iloc[i,7] = mean_value\n",
        "\n",
        "# Only include variables that are relevant to the prediction\n",
        "data_runs = data_runs[['race_id', 'horse_no', 'horse_id', 'won',\n",
        "       'horse_age', 'horse_country', 'horse_type', 'horse_rating',\n",
        "       'declared_weight', 'actual_weight', 'win_odds','place_odds']]\n",
        "\n",
        "data_runs = data_runs.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AUS    28404\n",
            "NZ     26390\n",
            "IRE     9947\n",
            "GB      6029\n",
            "USA     2382\n",
            "FR      1155\n",
            "SAF      655\n",
            "GER      336\n",
            "ARG      125\n",
            "CAN       91\n",
            "JPN       77\n",
            "ITY       56\n",
            "GR        33\n",
            "BRZ       18\n",
            "ZIM       12\n",
            "Name: horse_country, dtype: int64\n",
            "Gelding    71909\n",
            "Brown       1977\n",
            "Horse       1068\n",
            "Colt         275\n",
            "Mare         230\n",
            "Rig          150\n",
            "Roan          42\n",
            "Filly         42\n",
            "Grey          17\n",
            "Name: horse_type, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Binning horse country and horse type\n",
        "\n",
        "print(data_runs.horse_country.value_counts())\n",
        "print(data_runs.horse_type.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Binning horse country and horse type\n",
        "\n",
        "\n",
        "def helper_country(country):\n",
        "\n",
        "    if(country not in ['AUS','NZ','IRE','GB','USA','FR']):\n",
        "        return 'other'\n",
        "    return country\n",
        "\n",
        "def helper_type(type):\n",
        "\n",
        "    if(type not in ['Gelding','Brown']):\n",
        "        return 'other'\n",
        "    return type\n",
        "\n",
        "data_runs.horse_country = data_runs.horse_country.apply(helper_country)\n",
        "data_runs.horse_type = data_runs.horse_type.apply(helper_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# New Feature: number of previous participation and wins\n",
        "\n",
        "old_data_runs = data_runs.copy()\n",
        "\n",
        "data_runs['previous_races'] = 0\n",
        "data_runs['previous_wins'] = 0\n",
        "\n",
        "def previous_records(horse_id, race_id):\n",
        "\n",
        "    races = 0\n",
        "    wins = 0\n",
        "\n",
        "    df_horse = old_data_runs[(old_data_runs['horse_id'] == horse_id) \n",
        "                             & (old_data_runs['race_id'] < race_id)]\n",
        "    \n",
        "    races = len(df_horse)\n",
        "    wins = df_horse['won'].sum()\n",
        "    \n",
        "    return races, wins\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "75710it [00:30, 2505.10it/s]\n"
          ]
        }
      ],
      "source": [
        "for index, row in tqdm(old_data_runs.iterrows()):\n",
        "\n",
        "    races, wins = previous_records(row['horse_id'],row['race_id'])\n",
        "    data_runs.loc[index, 'previous_races'] = races\n",
        "    data_runs.loc[index, 'previous_wins'] = wins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# New Feature: number of horses in a race\n",
        "\n",
        "old_data_runs = data_runs.copy()\n",
        "\n",
        "race_count = old_data_runs.groupby(by = 'race_id').count()['horse_no']\n",
        "\n",
        "data_races['horse_count'] = race_count    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Merging and Further Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "id": "L4pXPOEKBe4m",
        "outputId": "23e572af-2dbc-4755-b58f-cdcb138ab204"
      },
      "outputs": [],
      "source": [
        "# Merging the two dataset\n",
        "\n",
        "df_horse_racing = pd.merge(data_runs, data_races, on=\"race_id\", how=\"left\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encode categorical variables\n",
        "\n",
        "categorical_variables = ['horse_ratings','horse_country', 'horse_type',\n",
        "                         'venue', 'config', 'going', 'race_class']\n",
        "horse_racing_predictors = df_horse_racing\n",
        "horse_racing_predictors = pd.get_dummies(horse_racing_predictors, columns = categorical_variables)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "YUu9-jDlVR_J",
        "outputId": "d832b34a-25b2-4714-887d-e5625b3c6cab"
      },
      "outputs": [],
      "source": [
        "# Selecting variables for horses and race\n",
        "\n",
        "horsestats = [\n",
        "              #'race_id','horse_id',\n",
        "              'horse_no','horse_age',\n",
        "              'horse_rating','declared_weight','actual_weight', 'win_odds','place_odds']\n",
        "for col in horse_racing_predictors.columns:\n",
        "  if('horse_country' in col):\n",
        "    horsestats.append(col)\n",
        "  if('horse_type' in col):\n",
        "    horsestats.append(col)\n",
        "#horsestats.append('won')\n",
        "\n",
        "racestats = [\n",
        "             'race_id',\n",
        "             'distance',\n",
        "             'place_combination1',\n",
        "             'horse_count'\n",
        "             ]\n",
        "for col in horse_racing_predictors.columns:\n",
        "  if('horse_ratings' in col):\n",
        "    racestats.append(col)\n",
        "  if('venue' in col):\n",
        "    racestats.append(col)\n",
        "  if('config' in col):\n",
        "    racestats.append(col)\n",
        "  if('going' in col):\n",
        "    racestats.append(col)\n",
        "  if('race_class' in col):\n",
        "    racestats.append(col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_28888\\948370532.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  race_predictors[new_cols] = -1\n",
            "100%|██████████| 75710/75710 [00:44<00:00, 1705.01it/s]\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Xy = horse_racing_predictors[racestats+horsestats]\n",
        "\n",
        "Xy = Xy[Xy['horse_count'] == 14].drop(columns = ['horse_count','race_id'])\n",
        "'''\n",
        "\n",
        "# Group by everything at race level\n",
        "\n",
        "race_predictors = horse_racing_predictors[racestats].groupby('race_id').min()\n",
        "\n",
        "horsestats_temp = horsestats[2:]\n",
        "\n",
        "for j in range(1,15):\n",
        "\n",
        "    new_cols = [i + '_' + str(j) for i in horsestats_temp]\n",
        "    race_predictors[new_cols] = -1\n",
        "\n",
        "index_list = list(race_predictors.index)\n",
        "\n",
        "for index in tqdm(range(0,len(horse_racing_predictors))):\n",
        "    row = horse_racing_predictors.iloc[index,:]\n",
        "    race_id = row['race_id']\n",
        "    horse_no = row['horse_no']\n",
        "    info = row[horsestats_temp]\n",
        "    racestats_len = len(racestats)-1\n",
        "    index_id = index_list.index(race_id)\n",
        "    race_predictors.iloc[index_id\n",
        "        ,int(racestats_len+len(horsestats_temp)*(horse_no-1)):int(racestats_len+len(horsestats_temp)*horse_no)] = info\n",
        "    \n",
        "# Only Keeping races with exactly 14 horses\n",
        "\n",
        "race_predictors_14 = race_predictors.replace(-1, np.NaN)\n",
        "race_predictors_14 = race_predictors_14.dropna()\n",
        "Xy = race_predictors_14\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK7tp4SJcXII"
      },
      "source": [
        "## Step 4: Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>distance</th>\n",
              "      <th>place_combination1</th>\n",
              "      <th>horse_count</th>\n",
              "      <th>horse_ratings_100+</th>\n",
              "      <th>horse_ratings_100-75</th>\n",
              "      <th>horse_ratings_100-80</th>\n",
              "      <th>horse_ratings_105-80</th>\n",
              "      <th>horse_ratings_105-85</th>\n",
              "      <th>horse_ratings_110-80</th>\n",
              "      <th>horse_ratings_110-85</th>\n",
              "      <th>horse_ratings_110-90</th>\n",
              "      <th>horse_ratings_115-90</th>\n",
              "      <th>horse_ratings_115-95</th>\n",
              "      <th>horse_ratings_120-100</th>\n",
              "      <th>horse_ratings_120-95</th>\n",
              "      <th>horse_ratings_40-0</th>\n",
              "      <th>horse_ratings_40-10</th>\n",
              "      <th>horse_ratings_40-15</th>\n",
              "      <th>horse_ratings_40-20</th>\n",
              "      <th>horse_ratings_60-35</th>\n",
              "      <th>horse_ratings_60-40</th>\n",
              "      <th>horse_ratings_65-40</th>\n",
              "      <th>horse_ratings_75-55</th>\n",
              "      <th>horse_ratings_80+</th>\n",
              "      <th>horse_ratings_80-55</th>\n",
              "      <th>horse_ratings_80-60</th>\n",
              "      <th>horse_ratings_85+</th>\n",
              "      <th>horse_ratings_85-60</th>\n",
              "      <th>horse_ratings_90+</th>\n",
              "      <th>horse_ratings_90-70</th>\n",
              "      <th>horse_ratings_95+</th>\n",
              "      <th>horse_ratings_95-75</th>\n",
              "      <th>horse_ratings_G</th>\n",
              "      <th>venue_HV</th>\n",
              "      <th>venue_ST</th>\n",
              "      <th>config_A</th>\n",
              "      <th>config_A+3</th>\n",
              "      <th>config_B</th>\n",
              "      <th>config_B+2</th>\n",
              "      <th>config_C</th>\n",
              "      <th>config_C+3</th>\n",
              "      <th>going_FAST</th>\n",
              "      <th>going_GOOD</th>\n",
              "      <th>going_GOOD TO FIRM</th>\n",
              "      <th>going_GOOD TO YIELDING</th>\n",
              "      <th>going_SLOW</th>\n",
              "      <th>going_SOFT</th>\n",
              "      <th>going_WET FAST</th>\n",
              "      <th>going_WET SLOW</th>\n",
              "      <th>going_YIELDING</th>\n",
              "      <th>going_YIELDING TO SOFT</th>\n",
              "      <th>race_class_0</th>\n",
              "      <th>race_class_1</th>\n",
              "      <th>race_class_2</th>\n",
              "      <th>race_class_3</th>\n",
              "      <th>race_class_4</th>\n",
              "      <th>race_class_5</th>\n",
              "      <th>race_class_6</th>\n",
              "      <th>race_class_11</th>\n",
              "      <th>race_class_12</th>\n",
              "      <th>race_class_13</th>\n",
              "      <th>horse_rating_1</th>\n",
              "      <th>declared_weight_1</th>\n",
              "      <th>actual_weight_1</th>\n",
              "      <th>win_odds_1</th>\n",
              "      <th>place_odds_1</th>\n",
              "      <th>horse_country_AUS_1</th>\n",
              "      <th>horse_country_FR_1</th>\n",
              "      <th>horse_country_GB_1</th>\n",
              "      <th>horse_country_IRE_1</th>\n",
              "      <th>horse_country_NZ_1</th>\n",
              "      <th>horse_country_USA_1</th>\n",
              "      <th>horse_country_other_1</th>\n",
              "      <th>horse_type_Brown_1</th>\n",
              "      <th>horse_type_Gelding_1</th>\n",
              "      <th>horse_type_other_1</th>\n",
              "      <th>horse_rating_2</th>\n",
              "      <th>declared_weight_2</th>\n",
              "      <th>actual_weight_2</th>\n",
              "      <th>win_odds_2</th>\n",
              "      <th>place_odds_2</th>\n",
              "      <th>horse_country_AUS_2</th>\n",
              "      <th>horse_country_FR_2</th>\n",
              "      <th>horse_country_GB_2</th>\n",
              "      <th>horse_country_IRE_2</th>\n",
              "      <th>horse_country_NZ_2</th>\n",
              "      <th>horse_country_USA_2</th>\n",
              "      <th>horse_country_other_2</th>\n",
              "      <th>horse_type_Brown_2</th>\n",
              "      <th>horse_type_Gelding_2</th>\n",
              "      <th>horse_type_other_2</th>\n",
              "      <th>horse_rating_3</th>\n",
              "      <th>declared_weight_3</th>\n",
              "      <th>actual_weight_3</th>\n",
              "      <th>win_odds_3</th>\n",
              "      <th>place_odds_3</th>\n",
              "      <th>horse_country_AUS_3</th>\n",
              "      <th>horse_country_FR_3</th>\n",
              "      <th>horse_country_GB_3</th>\n",
              "      <th>horse_country_IRE_3</th>\n",
              "      <th>horse_country_NZ_3</th>\n",
              "      <th>horse_country_USA_3</th>\n",
              "      <th>horse_country_other_3</th>\n",
              "      <th>horse_type_Brown_3</th>\n",
              "      <th>horse_type_Gelding_3</th>\n",
              "      <th>horse_type_other_3</th>\n",
              "      <th>horse_rating_4</th>\n",
              "      <th>declared_weight_4</th>\n",
              "      <th>actual_weight_4</th>\n",
              "      <th>win_odds_4</th>\n",
              "      <th>place_odds_4</th>\n",
              "      <th>horse_country_AUS_4</th>\n",
              "      <th>horse_country_FR_4</th>\n",
              "      <th>horse_country_GB_4</th>\n",
              "      <th>horse_country_IRE_4</th>\n",
              "      <th>horse_country_NZ_4</th>\n",
              "      <th>horse_country_USA_4</th>\n",
              "      <th>horse_country_other_4</th>\n",
              "      <th>horse_type_Brown_4</th>\n",
              "      <th>horse_type_Gelding_4</th>\n",
              "      <th>horse_type_other_4</th>\n",
              "      <th>horse_rating_5</th>\n",
              "      <th>declared_weight_5</th>\n",
              "      <th>actual_weight_5</th>\n",
              "      <th>win_odds_5</th>\n",
              "      <th>place_odds_5</th>\n",
              "      <th>horse_country_AUS_5</th>\n",
              "      <th>horse_country_FR_5</th>\n",
              "      <th>horse_country_GB_5</th>\n",
              "      <th>horse_country_IRE_5</th>\n",
              "      <th>horse_country_NZ_5</th>\n",
              "      <th>horse_country_USA_5</th>\n",
              "      <th>horse_country_other_5</th>\n",
              "      <th>horse_type_Brown_5</th>\n",
              "      <th>horse_type_Gelding_5</th>\n",
              "      <th>horse_type_other_5</th>\n",
              "      <th>horse_rating_6</th>\n",
              "      <th>declared_weight_6</th>\n",
              "      <th>actual_weight_6</th>\n",
              "      <th>win_odds_6</th>\n",
              "      <th>place_odds_6</th>\n",
              "      <th>horse_country_AUS_6</th>\n",
              "      <th>horse_country_FR_6</th>\n",
              "      <th>horse_country_GB_6</th>\n",
              "      <th>horse_country_IRE_6</th>\n",
              "      <th>horse_country_NZ_6</th>\n",
              "      <th>horse_country_USA_6</th>\n",
              "      <th>horse_country_other_6</th>\n",
              "      <th>horse_type_Brown_6</th>\n",
              "      <th>horse_type_Gelding_6</th>\n",
              "      <th>horse_type_other_6</th>\n",
              "      <th>horse_rating_7</th>\n",
              "      <th>declared_weight_7</th>\n",
              "      <th>actual_weight_7</th>\n",
              "      <th>win_odds_7</th>\n",
              "      <th>place_odds_7</th>\n",
              "      <th>horse_country_AUS_7</th>\n",
              "      <th>horse_country_FR_7</th>\n",
              "      <th>horse_country_GB_7</th>\n",
              "      <th>horse_country_IRE_7</th>\n",
              "      <th>horse_country_NZ_7</th>\n",
              "      <th>horse_country_USA_7</th>\n",
              "      <th>horse_country_other_7</th>\n",
              "      <th>horse_type_Brown_7</th>\n",
              "      <th>horse_type_Gelding_7</th>\n",
              "      <th>horse_type_other_7</th>\n",
              "      <th>horse_rating_8</th>\n",
              "      <th>declared_weight_8</th>\n",
              "      <th>actual_weight_8</th>\n",
              "      <th>win_odds_8</th>\n",
              "      <th>place_odds_8</th>\n",
              "      <th>horse_country_AUS_8</th>\n",
              "      <th>horse_country_FR_8</th>\n",
              "      <th>horse_country_GB_8</th>\n",
              "      <th>horse_country_IRE_8</th>\n",
              "      <th>horse_country_NZ_8</th>\n",
              "      <th>horse_country_USA_8</th>\n",
              "      <th>horse_country_other_8</th>\n",
              "      <th>horse_type_Brown_8</th>\n",
              "      <th>horse_type_Gelding_8</th>\n",
              "      <th>horse_type_other_8</th>\n",
              "      <th>horse_rating_9</th>\n",
              "      <th>declared_weight_9</th>\n",
              "      <th>actual_weight_9</th>\n",
              "      <th>win_odds_9</th>\n",
              "      <th>place_odds_9</th>\n",
              "      <th>horse_country_AUS_9</th>\n",
              "      <th>horse_country_FR_9</th>\n",
              "      <th>horse_country_GB_9</th>\n",
              "      <th>horse_country_IRE_9</th>\n",
              "      <th>horse_country_NZ_9</th>\n",
              "      <th>horse_country_USA_9</th>\n",
              "      <th>horse_country_other_9</th>\n",
              "      <th>horse_type_Brown_9</th>\n",
              "      <th>horse_type_Gelding_9</th>\n",
              "      <th>horse_type_other_9</th>\n",
              "      <th>horse_rating_10</th>\n",
              "      <th>declared_weight_10</th>\n",
              "      <th>actual_weight_10</th>\n",
              "      <th>win_odds_10</th>\n",
              "      <th>place_odds_10</th>\n",
              "      <th>horse_country_AUS_10</th>\n",
              "      <th>horse_country_FR_10</th>\n",
              "      <th>horse_country_GB_10</th>\n",
              "      <th>horse_country_IRE_10</th>\n",
              "      <th>horse_country_NZ_10</th>\n",
              "      <th>horse_country_USA_10</th>\n",
              "      <th>horse_country_other_10</th>\n",
              "      <th>horse_type_Brown_10</th>\n",
              "      <th>horse_type_Gelding_10</th>\n",
              "      <th>horse_type_other_10</th>\n",
              "      <th>horse_rating_11</th>\n",
              "      <th>declared_weight_11</th>\n",
              "      <th>actual_weight_11</th>\n",
              "      <th>win_odds_11</th>\n",
              "      <th>place_odds_11</th>\n",
              "      <th>horse_country_AUS_11</th>\n",
              "      <th>horse_country_FR_11</th>\n",
              "      <th>horse_country_GB_11</th>\n",
              "      <th>horse_country_IRE_11</th>\n",
              "      <th>horse_country_NZ_11</th>\n",
              "      <th>horse_country_USA_11</th>\n",
              "      <th>horse_country_other_11</th>\n",
              "      <th>horse_type_Brown_11</th>\n",
              "      <th>horse_type_Gelding_11</th>\n",
              "      <th>horse_type_other_11</th>\n",
              "      <th>horse_rating_12</th>\n",
              "      <th>declared_weight_12</th>\n",
              "      <th>actual_weight_12</th>\n",
              "      <th>win_odds_12</th>\n",
              "      <th>place_odds_12</th>\n",
              "      <th>horse_country_AUS_12</th>\n",
              "      <th>horse_country_FR_12</th>\n",
              "      <th>horse_country_GB_12</th>\n",
              "      <th>horse_country_IRE_12</th>\n",
              "      <th>horse_country_NZ_12</th>\n",
              "      <th>horse_country_USA_12</th>\n",
              "      <th>horse_country_other_12</th>\n",
              "      <th>horse_type_Brown_12</th>\n",
              "      <th>horse_type_Gelding_12</th>\n",
              "      <th>horse_type_other_12</th>\n",
              "      <th>horse_rating_13</th>\n",
              "      <th>declared_weight_13</th>\n",
              "      <th>actual_weight_13</th>\n",
              "      <th>win_odds_13</th>\n",
              "      <th>place_odds_13</th>\n",
              "      <th>horse_country_AUS_13</th>\n",
              "      <th>horse_country_FR_13</th>\n",
              "      <th>horse_country_GB_13</th>\n",
              "      <th>horse_country_IRE_13</th>\n",
              "      <th>horse_country_NZ_13</th>\n",
              "      <th>horse_country_USA_13</th>\n",
              "      <th>horse_country_other_13</th>\n",
              "      <th>horse_type_Brown_13</th>\n",
              "      <th>horse_type_Gelding_13</th>\n",
              "      <th>horse_type_other_13</th>\n",
              "      <th>horse_rating_14</th>\n",
              "      <th>declared_weight_14</th>\n",
              "      <th>actual_weight_14</th>\n",
              "      <th>win_odds_14</th>\n",
              "      <th>place_odds_14</th>\n",
              "      <th>horse_country_AUS_14</th>\n",
              "      <th>horse_country_FR_14</th>\n",
              "      <th>horse_country_GB_14</th>\n",
              "      <th>horse_country_IRE_14</th>\n",
              "      <th>horse_country_NZ_14</th>\n",
              "      <th>horse_country_USA_14</th>\n",
              "      <th>horse_country_other_14</th>\n",
              "      <th>horse_type_Brown_14</th>\n",
              "      <th>horse_type_Gelding_14</th>\n",
              "      <th>horse_type_other_14</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>race_id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1400</td>\n",
              "      <td>8</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1020.0</td>\n",
              "      <td>133.0</td>\n",
              "      <td>9.7</td>\n",
              "      <td>3.7</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>980.0</td>\n",
              "      <td>133.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>4.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1082.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1118.0</td>\n",
              "      <td>127.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>972.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1114.0</td>\n",
              "      <td>127.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>978.0</td>\n",
              "      <td>123.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1170.0</td>\n",
              "      <td>128.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1126.0</td>\n",
              "      <td>123.0</td>\n",
              "      <td>38.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1072.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1135.0</td>\n",
              "      <td>123.0</td>\n",
              "      <td>8.6</td>\n",
              "      <td>2.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1018.0</td>\n",
              "      <td>123.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>8.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1089.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>5.4</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1027.0</td>\n",
              "      <td>113.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>3.9</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1200</td>\n",
              "      <td>5</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1078.0</td>\n",
              "      <td>128.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>4.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1257.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>8.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1037.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1.7</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1168.0</td>\n",
              "      <td>126.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>3.3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1148.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>2.3</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1057.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>6.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1064.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>3.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1132.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1081.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>4.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1059.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>2.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1106.0</td>\n",
              "      <td>118.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1095.0</td>\n",
              "      <td>103.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>7.4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1060.0</td>\n",
              "      <td>113.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>4.7</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1110.0</td>\n",
              "      <td>108.0</td>\n",
              "      <td>47.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1400</td>\n",
              "      <td>11</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1115.0</td>\n",
              "      <td>133.0</td>\n",
              "      <td>9.2</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1091.0</td>\n",
              "      <td>126.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>128.0</td>\n",
              "      <td>81.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1106.0</td>\n",
              "      <td>127.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1031.0</td>\n",
              "      <td>126.0</td>\n",
              "      <td>4.4</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1068.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>65.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1151.0</td>\n",
              "      <td>123.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1052.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>6.7</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1122.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>7.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1016.0</td>\n",
              "      <td>124.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1149.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1028.0</td>\n",
              "      <td>116.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1029.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>5.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1157.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1600</td>\n",
              "      <td>2</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1076.0</td>\n",
              "      <td>133.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1044.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1065.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>8.2</td>\n",
              "      <td>2.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1161.0</td>\n",
              "      <td>126.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>8.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1040.0</td>\n",
              "      <td>128.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1106.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1019.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1009.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>4.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1011.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>6.1</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>999.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>2.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1011.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>6.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>988.0</td>\n",
              "      <td>106.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>9.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1157.0</td>\n",
              "      <td>113.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1152.0</td>\n",
              "      <td>103.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>7.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1200</td>\n",
              "      <td>9</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1143.0</td>\n",
              "      <td>133.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>4.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1096.0</td>\n",
              "      <td>133.0</td>\n",
              "      <td>1.8</td>\n",
              "      <td>1.1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1169.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>6.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1120.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>2.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1136.0</td>\n",
              "      <td>126.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>2.9</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1235.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1015.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1114.0</td>\n",
              "      <td>124.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1170.0</td>\n",
              "      <td>124.0</td>\n",
              "      <td>6.1</td>\n",
              "      <td>1.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1015.0</td>\n",
              "      <td>123.0</td>\n",
              "      <td>8.5</td>\n",
              "      <td>2.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1087.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>5.9</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1098.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1064.0</td>\n",
              "      <td>115.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>5.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1155.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6123</th>\n",
              "      <td>1400</td>\n",
              "      <td>5</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1056.0</td>\n",
              "      <td>133.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>3.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>59.0</td>\n",
              "      <td>1084.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>1109.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>8.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>1106.0</td>\n",
              "      <td>127.0</td>\n",
              "      <td>4.2</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>1127.0</td>\n",
              "      <td>129.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>1007.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>7.7</td>\n",
              "      <td>2.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>1104.0</td>\n",
              "      <td>127.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>1061.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>86.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1150.0</td>\n",
              "      <td>123.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>1037.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>3.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>47.0</td>\n",
              "      <td>1045.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>3.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>46.0</td>\n",
              "      <td>1197.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>8.4</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>46.0</td>\n",
              "      <td>1133.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>53.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>1113.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6125</th>\n",
              "      <td>1000</td>\n",
              "      <td>7</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>1118.0</td>\n",
              "      <td>133.0</td>\n",
              "      <td>7.5</td>\n",
              "      <td>2.3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>1163.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>4.1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>1063.0</td>\n",
              "      <td>127.0</td>\n",
              "      <td>95.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>1249.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>71.0</td>\n",
              "      <td>999.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>71.0</td>\n",
              "      <td>1146.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>1079.0</td>\n",
              "      <td>123.0</td>\n",
              "      <td>2.9</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>1076.0</td>\n",
              "      <td>123.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>3.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>1186.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>95.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>1111.0</td>\n",
              "      <td>118.0</td>\n",
              "      <td>3.9</td>\n",
              "      <td>1.4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>1110.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>3.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>1196.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>5.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>63.0</td>\n",
              "      <td>1196.0</td>\n",
              "      <td>116.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>4.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>1122.0</td>\n",
              "      <td>115.0</td>\n",
              "      <td>9.6</td>\n",
              "      <td>2.7</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6126</th>\n",
              "      <td>1400</td>\n",
              "      <td>7</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>1216.0</td>\n",
              "      <td>133.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>6.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>76.0</td>\n",
              "      <td>1121.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>5.9</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>76.0</td>\n",
              "      <td>1030.0</td>\n",
              "      <td>129.0</td>\n",
              "      <td>9.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>1073.0</td>\n",
              "      <td>128.0</td>\n",
              "      <td>2.7</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>71.0</td>\n",
              "      <td>1059.0</td>\n",
              "      <td>124.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>71.0</td>\n",
              "      <td>1285.0</td>\n",
              "      <td>124.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>3.7</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>69.0</td>\n",
              "      <td>1270.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>4.8</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>1098.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>8.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>1050.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>93.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>1193.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>7.5</td>\n",
              "      <td>2.1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>65.0</td>\n",
              "      <td>993.0</td>\n",
              "      <td>111.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>7.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>63.0</td>\n",
              "      <td>1100.0</td>\n",
              "      <td>116.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>63.0</td>\n",
              "      <td>1168.0</td>\n",
              "      <td>116.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>61.0</td>\n",
              "      <td>1084.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>3.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6127</th>\n",
              "      <td>1000</td>\n",
              "      <td>7</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>1231.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>6.5</td>\n",
              "      <td>2.1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>1089.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>7.9</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>90.0</td>\n",
              "      <td>1142.0</td>\n",
              "      <td>127.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>88.0</td>\n",
              "      <td>1084.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>5.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>1192.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>6.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>86.0</td>\n",
              "      <td>1103.0</td>\n",
              "      <td>123.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>2.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>85.0</td>\n",
              "      <td>1128.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>2.8</td>\n",
              "      <td>1.4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>85.0</td>\n",
              "      <td>1101.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>84.0</td>\n",
              "      <td>1105.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>55.0</td>\n",
              "      <td>8.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>84.0</td>\n",
              "      <td>1115.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>84.0</td>\n",
              "      <td>1167.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>85.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>83.0</td>\n",
              "      <td>1097.0</td>\n",
              "      <td>113.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>3.9</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>83.0</td>\n",
              "      <td>1092.0</td>\n",
              "      <td>118.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>81.0</td>\n",
              "      <td>1098.0</td>\n",
              "      <td>111.0</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6128</th>\n",
              "      <td>1400</td>\n",
              "      <td>4</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>1078.0</td>\n",
              "      <td>133.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>77.0</td>\n",
              "      <td>1110.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>2.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>980.0</td>\n",
              "      <td>127.0</td>\n",
              "      <td>7.9</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>73.0</td>\n",
              "      <td>1153.0</td>\n",
              "      <td>126.0</td>\n",
              "      <td>1.7</td>\n",
              "      <td>1.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>73.0</td>\n",
              "      <td>1117.0</td>\n",
              "      <td>124.0</td>\n",
              "      <td>71.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>1166.0</td>\n",
              "      <td>123.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>1116.0</td>\n",
              "      <td>123.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>1068.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>55.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>1091.0</td>\n",
              "      <td>118.0</td>\n",
              "      <td>7.9</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>65.0</td>\n",
              "      <td>1141.0</td>\n",
              "      <td>116.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>5.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>1066.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>63.0</td>\n",
              "      <td>1115.0</td>\n",
              "      <td>109.0</td>\n",
              "      <td>8.2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>63.0</td>\n",
              "      <td>1180.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>59.0</td>\n",
              "      <td>9.9</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>63.0</td>\n",
              "      <td>1151.0</td>\n",
              "      <td>116.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2220 rows × 271 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         distance  place_combination1  horse_count  horse_ratings_100+  \\\n",
              "race_id                                                                  \n",
              "0            1400                   8         14.0                   0   \n",
              "1            1200                   5         14.0                   0   \n",
              "2            1400                  11         14.0                   0   \n",
              "4            1600                   2         14.0                   0   \n",
              "5            1200                   9         14.0                   0   \n",
              "...           ...                 ...          ...                 ...   \n",
              "6123         1400                   5         14.0                   0   \n",
              "6125         1000                   7         14.0                   0   \n",
              "6126         1400                   7         14.0                   0   \n",
              "6127         1000                   7         14.0                   0   \n",
              "6128         1400                   4         14.0                   0   \n",
              "\n",
              "         horse_ratings_100-75  horse_ratings_100-80  horse_ratings_105-80  \\\n",
              "race_id                                                                     \n",
              "0                           0                     0                     0   \n",
              "1                           0                     0                     0   \n",
              "2                           0                     0                     0   \n",
              "4                           0                     0                     0   \n",
              "5                           0                     0                     0   \n",
              "...                       ...                   ...                   ...   \n",
              "6123                        0                     0                     0   \n",
              "6125                        0                     0                     0   \n",
              "6126                        0                     0                     0   \n",
              "6127                        0                     1                     0   \n",
              "6128                        0                     0                     0   \n",
              "\n",
              "         horse_ratings_105-85  horse_ratings_110-80  horse_ratings_110-85  \\\n",
              "race_id                                                                     \n",
              "0                           0                     0                     0   \n",
              "1                           0                     0                     0   \n",
              "2                           0                     0                     0   \n",
              "4                           0                     0                     0   \n",
              "5                           0                     0                     0   \n",
              "...                       ...                   ...                   ...   \n",
              "6123                        0                     0                     0   \n",
              "6125                        0                     0                     0   \n",
              "6126                        0                     0                     0   \n",
              "6127                        0                     0                     0   \n",
              "6128                        0                     0                     0   \n",
              "\n",
              "         horse_ratings_110-90  horse_ratings_115-90  horse_ratings_115-95  \\\n",
              "race_id                                                                     \n",
              "0                           0                     0                     0   \n",
              "1                           0                     0                     0   \n",
              "2                           0                     0                     0   \n",
              "4                           0                     0                     0   \n",
              "5                           0                     0                     0   \n",
              "...                       ...                   ...                   ...   \n",
              "6123                        0                     0                     0   \n",
              "6125                        0                     0                     0   \n",
              "6126                        0                     0                     0   \n",
              "6127                        0                     0                     0   \n",
              "6128                        0                     0                     0   \n",
              "\n",
              "         horse_ratings_120-100  horse_ratings_120-95  horse_ratings_40-0  \\\n",
              "race_id                                                                    \n",
              "0                            0                     0                   0   \n",
              "1                            0                     0                   0   \n",
              "2                            0                     0                   0   \n",
              "4                            0                     0                   0   \n",
              "5                            0                     0                   0   \n",
              "...                        ...                   ...                 ...   \n",
              "6123                         0                     0                   0   \n",
              "6125                         0                     0                   0   \n",
              "6126                         0                     0                   0   \n",
              "6127                         0                     0                   0   \n",
              "6128                         0                     0                   0   \n",
              "\n",
              "         horse_ratings_40-10  horse_ratings_40-15  horse_ratings_40-20  \\\n",
              "race_id                                                                  \n",
              "0                          0                    1                    0   \n",
              "1                          0                    1                    0   \n",
              "2                          0                    0                    0   \n",
              "4                          0                    0                    0   \n",
              "5                          0                    0                    0   \n",
              "...                      ...                  ...                  ...   \n",
              "6123                       0                    0                    0   \n",
              "6125                       0                    0                    0   \n",
              "6126                       0                    0                    0   \n",
              "6127                       0                    0                    0   \n",
              "6128                       0                    0                    0   \n",
              "\n",
              "         horse_ratings_60-35  horse_ratings_60-40  horse_ratings_65-40  \\\n",
              "race_id                                                                  \n",
              "0                          0                    0                    0   \n",
              "1                          0                    0                    0   \n",
              "2                          0                    1                    0   \n",
              "4                          0                    1                    0   \n",
              "5                          0                    1                    0   \n",
              "...                      ...                  ...                  ...   \n",
              "6123                       0                    1                    0   \n",
              "6125                       0                    0                    0   \n",
              "6126                       0                    0                    0   \n",
              "6127                       0                    0                    0   \n",
              "6128                       0                    0                    0   \n",
              "\n",
              "         horse_ratings_75-55  horse_ratings_80+  horse_ratings_80-55  \\\n",
              "race_id                                                                \n",
              "0                          0                  0                    0   \n",
              "1                          0                  0                    0   \n",
              "2                          0                  0                    0   \n",
              "4                          0                  0                    0   \n",
              "5                          0                  0                    0   \n",
              "...                      ...                ...                  ...   \n",
              "6123                       0                  0                    0   \n",
              "6125                       0                  0                    0   \n",
              "6126                       0                  0                    0   \n",
              "6127                       0                  0                    0   \n",
              "6128                       0                  0                    0   \n",
              "\n",
              "         horse_ratings_80-60  horse_ratings_85+  horse_ratings_85-60  \\\n",
              "race_id                                                                \n",
              "0                          0                  0                    0   \n",
              "1                          0                  0                    0   \n",
              "2                          0                  0                    0   \n",
              "4                          0                  0                    0   \n",
              "5                          0                  0                    0   \n",
              "...                      ...                ...                  ...   \n",
              "6123                       0                  0                    0   \n",
              "6125                       1                  0                    0   \n",
              "6126                       1                  0                    0   \n",
              "6127                       0                  0                    0   \n",
              "6128                       1                  0                    0   \n",
              "\n",
              "         horse_ratings_90+  horse_ratings_90-70  horse_ratings_95+  \\\n",
              "race_id                                                              \n",
              "0                        0                    0                  0   \n",
              "1                        0                    0                  0   \n",
              "2                        0                    0                  0   \n",
              "4                        0                    0                  0   \n",
              "5                        0                    0                  0   \n",
              "...                    ...                  ...                ...   \n",
              "6123                     0                    0                  0   \n",
              "6125                     0                    0                  0   \n",
              "6126                     0                    0                  0   \n",
              "6127                     0                    0                  0   \n",
              "6128                     0                    0                  0   \n",
              "\n",
              "         horse_ratings_95-75  horse_ratings_G  venue_HV  venue_ST  config_A  \\\n",
              "race_id                                                                       \n",
              "0                          0                0         0         1         1   \n",
              "1                          0                0         0         1         1   \n",
              "2                          0                0         0         1         1   \n",
              "4                          0                0         0         1         1   \n",
              "5                          0                0         0         1         1   \n",
              "...                      ...              ...       ...       ...       ...   \n",
              "6123                       0                0         0         1         0   \n",
              "6125                       0                0         0         1         0   \n",
              "6126                       0                0         0         1         0   \n",
              "6127                       0                0         0         1         0   \n",
              "6128                       0                0         0         1         0   \n",
              "\n",
              "         config_A+3  config_B  config_B+2  config_C  config_C+3  going_FAST  \\\n",
              "race_id                                                                       \n",
              "0                 0         0           0         0           0           0   \n",
              "1                 0         0           0         0           0           0   \n",
              "2                 0         0           0         0           0           0   \n",
              "4                 0         0           0         0           0           0   \n",
              "5                 0         0           0         0           0           0   \n",
              "...             ...       ...         ...       ...         ...         ...   \n",
              "6123              0         0           0         0           1           0   \n",
              "6125              0         0           0         0           1           0   \n",
              "6126              0         0           0         0           1           0   \n",
              "6127              0         0           0         0           1           0   \n",
              "6128              0         0           0         0           1           0   \n",
              "\n",
              "         going_GOOD  going_GOOD TO FIRM  going_GOOD TO YIELDING  going_SLOW  \\\n",
              "race_id                                                                       \n",
              "0                 0                   1                       0           0   \n",
              "1                 0                   1                       0           0   \n",
              "2                 0                   1                       0           0   \n",
              "4                 0                   1                       0           0   \n",
              "5                 0                   1                       0           0   \n",
              "...             ...                 ...                     ...         ...   \n",
              "6123              0                   1                       0           0   \n",
              "6125              0                   1                       0           0   \n",
              "6126              0                   1                       0           0   \n",
              "6127              0                   1                       0           0   \n",
              "6128              0                   1                       0           0   \n",
              "\n",
              "         going_SOFT  going_WET FAST  going_WET SLOW  going_YIELDING  \\\n",
              "race_id                                                               \n",
              "0                 0               0               0               0   \n",
              "1                 0               0               0               0   \n",
              "2                 0               0               0               0   \n",
              "4                 0               0               0               0   \n",
              "5                 0               0               0               0   \n",
              "...             ...             ...             ...             ...   \n",
              "6123              0               0               0               0   \n",
              "6125              0               0               0               0   \n",
              "6126              0               0               0               0   \n",
              "6127              0               0               0               0   \n",
              "6128              0               0               0               0   \n",
              "\n",
              "         going_YIELDING TO SOFT  race_class_0  race_class_1  race_class_2  \\\n",
              "race_id                                                                     \n",
              "0                             0             0             0             0   \n",
              "1                             0             0             0             0   \n",
              "2                             0             0             0             0   \n",
              "4                             0             0             0             0   \n",
              "5                             0             0             0             0   \n",
              "...                         ...           ...           ...           ...   \n",
              "6123                          0             0             0             0   \n",
              "6125                          0             0             0             0   \n",
              "6126                          0             0             0             0   \n",
              "6127                          0             0             0             1   \n",
              "6128                          0             0             0             0   \n",
              "\n",
              "         race_class_3  race_class_4  race_class_5  race_class_6  \\\n",
              "race_id                                                           \n",
              "0                   0             0             1             0   \n",
              "1                   0             0             1             0   \n",
              "2                   0             1             0             0   \n",
              "4                   0             1             0             0   \n",
              "5                   0             1             0             0   \n",
              "...               ...           ...           ...           ...   \n",
              "6123                0             1             0             0   \n",
              "6125                1             0             0             0   \n",
              "6126                1             0             0             0   \n",
              "6127                0             0             0             0   \n",
              "6128                1             0             0             0   \n",
              "\n",
              "         race_class_11  race_class_12  race_class_13  horse_rating_1  \\\n",
              "race_id                                                                \n",
              "0                    0              0              0            60.0   \n",
              "1                    0              0              0            60.0   \n",
              "2                    0              0              0            60.0   \n",
              "4                    0              0              0            60.0   \n",
              "5                    0              0              0            60.0   \n",
              "...                ...            ...            ...             ...   \n",
              "6123                 0              0              0            60.0   \n",
              "6125                 0              0              0            80.0   \n",
              "6126                 0              0              0            80.0   \n",
              "6127                 0              0              0            94.0   \n",
              "6128                 0              0              0            80.0   \n",
              "\n",
              "         declared_weight_1  actual_weight_1  win_odds_1  place_odds_1  \\\n",
              "race_id                                                                 \n",
              "0                   1020.0            133.0         9.7           3.7   \n",
              "1                   1078.0            128.0        14.0           4.2   \n",
              "2                   1115.0            133.0         9.2           2.3   \n",
              "4                   1076.0            133.0         5.2           1.7   \n",
              "5                   1143.0            133.0        17.0           4.8   \n",
              "...                    ...              ...         ...           ...   \n",
              "6123                1056.0            133.0        13.0           3.8   \n",
              "6125                1118.0            133.0         7.5           2.3   \n",
              "6126                1216.0            133.0        26.0           6.6   \n",
              "6127                1231.0            131.0         6.5           2.1   \n",
              "6128                1078.0            133.0        99.0          19.0   \n",
              "\n",
              "         horse_country_AUS_1  horse_country_FR_1  horse_country_GB_1  \\\n",
              "race_id                                                                \n",
              "0                        1.0                 0.0                 0.0   \n",
              "1                        0.0                 0.0                 0.0   \n",
              "2                        0.0                 0.0                 0.0   \n",
              "4                        0.0                 0.0                 0.0   \n",
              "5                        0.0                 0.0                 0.0   \n",
              "...                      ...                 ...                 ...   \n",
              "6123                     0.0                 0.0                 0.0   \n",
              "6125                     1.0                 0.0                 0.0   \n",
              "6126                     0.0                 0.0                 0.0   \n",
              "6127                     1.0                 0.0                 0.0   \n",
              "6128                     0.0                 0.0                 0.0   \n",
              "\n",
              "         horse_country_IRE_1  horse_country_NZ_1  horse_country_USA_1  \\\n",
              "race_id                                                                 \n",
              "0                        0.0                 0.0                  0.0   \n",
              "1                        0.0                 1.0                  0.0   \n",
              "2                        0.0                 1.0                  0.0   \n",
              "4                        0.0                 1.0                  0.0   \n",
              "5                        0.0                 1.0                  0.0   \n",
              "...                      ...                 ...                  ...   \n",
              "6123                     0.0                 1.0                  0.0   \n",
              "6125                     0.0                 0.0                  0.0   \n",
              "6126                     0.0                 1.0                  0.0   \n",
              "6127                     0.0                 0.0                  0.0   \n",
              "6128                     1.0                 0.0                  0.0   \n",
              "\n",
              "         horse_country_other_1  horse_type_Brown_1  horse_type_Gelding_1  \\\n",
              "race_id                                                                    \n",
              "0                          0.0                 0.0                   1.0   \n",
              "1                          0.0                 0.0                   1.0   \n",
              "2                          0.0                 0.0                   1.0   \n",
              "4                          0.0                 0.0                   1.0   \n",
              "5                          0.0                 0.0                   1.0   \n",
              "...                        ...                 ...                   ...   \n",
              "6123                       0.0                 0.0                   1.0   \n",
              "6125                       0.0                 0.0                   1.0   \n",
              "6126                       0.0                 0.0                   1.0   \n",
              "6127                       0.0                 0.0                   1.0   \n",
              "6128                       0.0                 0.0                   1.0   \n",
              "\n",
              "         horse_type_other_1  horse_rating_2  declared_weight_2  \\\n",
              "race_id                                                          \n",
              "0                       0.0            60.0              980.0   \n",
              "1                       0.0            60.0             1257.0   \n",
              "2                       0.0            60.0             1091.0   \n",
              "4                       0.0            60.0             1044.0   \n",
              "5                       0.0            60.0             1096.0   \n",
              "...                     ...             ...                ...   \n",
              "6123                    0.0            59.0             1084.0   \n",
              "6125                    0.0            75.0             1163.0   \n",
              "6126                    0.0            76.0             1121.0   \n",
              "6127                    0.0            94.0             1089.0   \n",
              "6128                    0.0            77.0             1110.0   \n",
              "\n",
              "         actual_weight_2  win_odds_2  place_odds_2  horse_country_AUS_2  \\\n",
              "race_id                                                                   \n",
              "0                  133.0        16.0           4.9                  0.0   \n",
              "1                  132.0        28.0           8.5                  0.0   \n",
              "2                  126.0        99.0          36.0                  0.0   \n",
              "4                  132.0         3.6           1.5                  1.0   \n",
              "5                  133.0         1.8           1.1                  1.0   \n",
              "...                  ...         ...           ...                  ...   \n",
              "6123               132.0         3.1           1.6                  1.0   \n",
              "6125               121.0        11.0           4.1                  1.0   \n",
              "6126               122.0        21.0           5.9                  1.0   \n",
              "6127               131.0        39.0           7.9                  1.0   \n",
              "6128               130.0        13.0           2.9                  0.0   \n",
              "\n",
              "         horse_country_FR_2  horse_country_GB_2  horse_country_IRE_2  \\\n",
              "race_id                                                                \n",
              "0                       0.0                 0.0                  0.0   \n",
              "1                       0.0                 0.0                  0.0   \n",
              "2                       0.0                 0.0                  0.0   \n",
              "4                       0.0                 0.0                  0.0   \n",
              "5                       0.0                 0.0                  0.0   \n",
              "...                     ...                 ...                  ...   \n",
              "6123                    0.0                 0.0                  0.0   \n",
              "6125                    0.0                 0.0                  0.0   \n",
              "6126                    0.0                 0.0                  0.0   \n",
              "6127                    0.0                 0.0                  0.0   \n",
              "6128                    0.0                 0.0                  0.0   \n",
              "\n",
              "         horse_country_NZ_2  horse_country_USA_2  horse_country_other_2  \\\n",
              "race_id                                                                   \n",
              "0                       1.0                  0.0                    0.0   \n",
              "1                       1.0                  0.0                    0.0   \n",
              "2                       1.0                  0.0                    0.0   \n",
              "4                       0.0                  0.0                    0.0   \n",
              "5                       0.0                  0.0                    0.0   \n",
              "...                     ...                  ...                    ...   \n",
              "6123                    0.0                  0.0                    0.0   \n",
              "6125                    0.0                  0.0                    0.0   \n",
              "6126                    0.0                  0.0                    0.0   \n",
              "6127                    0.0                  0.0                    0.0   \n",
              "6128                    1.0                  0.0                    0.0   \n",
              "\n",
              "         horse_type_Brown_2  horse_type_Gelding_2  horse_type_other_2  \\\n",
              "race_id                                                                 \n",
              "0                       0.0                   1.0                 0.0   \n",
              "1                       0.0                   1.0                 0.0   \n",
              "2                       0.0                   1.0                 0.0   \n",
              "4                       0.0                   1.0                 0.0   \n",
              "5                       0.0                   1.0                 0.0   \n",
              "...                     ...                   ...                 ...   \n",
              "6123                    0.0                   1.0                 0.0   \n",
              "6125                    0.0                   1.0                 0.0   \n",
              "6126                    0.0                   1.0                 0.0   \n",
              "6127                    0.0                   1.0                 0.0   \n",
              "6128                    0.0                   1.0                 0.0   \n",
              "\n",
              "         horse_rating_3  declared_weight_3  actual_weight_3  win_odds_3  \\\n",
              "race_id                                                                   \n",
              "0                  60.0             1082.0            132.0         3.5   \n",
              "1                  60.0             1037.0            130.0         7.0   \n",
              "2                  60.0             1000.0            128.0        81.0   \n",
              "4                  60.0             1065.0            132.0         8.2   \n",
              "5                  60.0             1169.0            132.0        28.0   \n",
              "...                 ...                ...              ...         ...   \n",
              "6123               57.0             1109.0            130.0        29.0   \n",
              "6125               74.0             1063.0            127.0        95.0   \n",
              "6126               76.0             1030.0            129.0         9.5   \n",
              "6127               90.0             1142.0            127.0        14.0   \n",
              "6128               74.0              980.0            127.0         7.9   \n",
              "\n",
              "         place_odds_3  horse_country_AUS_3  horse_country_FR_3  \\\n",
              "race_id                                                          \n",
              "0                 1.5                  0.0                 0.0   \n",
              "1                 1.7                  1.0                 0.0   \n",
              "2                22.0                  0.0                 0.0   \n",
              "4                 2.5                  0.0                 0.0   \n",
              "5                 6.9                  0.0                 0.0   \n",
              "...               ...                  ...                 ...   \n",
              "6123              8.2                  0.0                 0.0   \n",
              "6125             23.0                  0.0                 0.0   \n",
              "6126              3.0                  1.0                 0.0   \n",
              "6127              3.0                  0.0                 0.0   \n",
              "6128              2.0                  0.0                 0.0   \n",
              "\n",
              "         horse_country_GB_3  horse_country_IRE_3  horse_country_NZ_3  \\\n",
              "race_id                                                                \n",
              "0                       0.0                  0.0                 1.0   \n",
              "1                       0.0                  0.0                 0.0   \n",
              "2                       1.0                  0.0                 0.0   \n",
              "4                       0.0                  0.0                 1.0   \n",
              "5                       0.0                  0.0                 1.0   \n",
              "...                     ...                  ...                 ...   \n",
              "6123                    0.0                  1.0                 0.0   \n",
              "6125                    0.0                  1.0                 0.0   \n",
              "6126                    0.0                  0.0                 0.0   \n",
              "6127                    0.0                  0.0                 1.0   \n",
              "6128                    0.0                  1.0                 0.0   \n",
              "\n",
              "         horse_country_USA_3  horse_country_other_3  horse_type_Brown_3  \\\n",
              "race_id                                                                   \n",
              "0                        0.0                    0.0                 0.0   \n",
              "1                        0.0                    0.0                 0.0   \n",
              "2                        0.0                    0.0                 0.0   \n",
              "4                        0.0                    0.0                 0.0   \n",
              "5                        0.0                    0.0                 0.0   \n",
              "...                      ...                    ...                 ...   \n",
              "6123                     0.0                    0.0                 0.0   \n",
              "6125                     0.0                    0.0                 0.0   \n",
              "6126                     0.0                    0.0                 0.0   \n",
              "6127                     0.0                    0.0                 0.0   \n",
              "6128                     0.0                    0.0                 0.0   \n",
              "\n",
              "         horse_type_Gelding_3  horse_type_other_3  horse_rating_4  \\\n",
              "race_id                                                             \n",
              "0                         1.0                 0.0            60.0   \n",
              "1                         0.0                 1.0            60.0   \n",
              "2                         1.0                 0.0            60.0   \n",
              "4                         1.0                 0.0            60.0   \n",
              "5                         1.0                 0.0            60.0   \n",
              "...                       ...                 ...             ...   \n",
              "6123                      1.0                 0.0            56.0   \n",
              "6125                      1.0                 0.0            72.0   \n",
              "6126                      1.0                 0.0            75.0   \n",
              "6127                      1.0                 0.0            88.0   \n",
              "6128                      1.0                 0.0            73.0   \n",
              "\n",
              "         declared_weight_4  actual_weight_4  win_odds_4  place_odds_4  \\\n",
              "race_id                                                                 \n",
              "0                   1118.0            127.0        39.0          11.0   \n",
              "1                   1168.0            126.0        12.0           3.3   \n",
              "2                   1106.0            127.0         3.5           1.5   \n",
              "4                   1161.0            126.0        28.0           8.1   \n",
              "5                   1120.0            130.0        11.0           2.2   \n",
              "...                    ...              ...         ...           ...   \n",
              "6123                1106.0            127.0         4.2           1.6   \n",
              "6125                1249.0            125.0        67.0          14.0   \n",
              "6126                1073.0            128.0         2.7           1.4   \n",
              "6127                1084.0            125.0        32.0           5.5   \n",
              "6128                1153.0            126.0         1.7           1.2   \n",
              "\n",
              "         horse_country_AUS_4  horse_country_FR_4  horse_country_GB_4  \\\n",
              "race_id                                                                \n",
              "0                        0.0                 0.0                 0.0   \n",
              "1                        1.0                 0.0                 0.0   \n",
              "2                        0.0                 0.0                 0.0   \n",
              "4                        0.0                 0.0                 0.0   \n",
              "5                        0.0                 0.0                 0.0   \n",
              "...                      ...                 ...                 ...   \n",
              "6123                     0.0                 0.0                 0.0   \n",
              "6125                     1.0                 0.0                 0.0   \n",
              "6126                     0.0                 0.0                 0.0   \n",
              "6127                     0.0                 0.0                 0.0   \n",
              "6128                     0.0                 0.0                 0.0   \n",
              "\n",
              "         horse_country_IRE_4  horse_country_NZ_4  horse_country_USA_4  \\\n",
              "race_id                                                                 \n",
              "0                        0.0                 0.0                  0.0   \n",
              "1                        0.0                 0.0                  0.0   \n",
              "2                        0.0                 0.0                  1.0   \n",
              "4                        0.0                 1.0                  0.0   \n",
              "5                        0.0                 1.0                  0.0   \n",
              "...                      ...                 ...                  ...   \n",
              "6123                     0.0                 1.0                  0.0   \n",
              "6125                     0.0                 0.0                  0.0   \n",
              "6126                     1.0                 0.0                  0.0   \n",
              "6127                     0.0                 0.0                  0.0   \n",
              "6128                     0.0                 0.0                  0.0   \n",
              "\n",
              "         horse_country_other_4  horse_type_Brown_4  horse_type_Gelding_4  \\\n",
              "race_id                                                                    \n",
              "0                          1.0                 0.0                   1.0   \n",
              "1                          0.0                 0.0                   1.0   \n",
              "2                          0.0                 1.0                   0.0   \n",
              "4                          0.0                 0.0                   1.0   \n",
              "5                          0.0                 0.0                   0.0   \n",
              "...                        ...                 ...                   ...   \n",
              "6123                       0.0                 0.0                   1.0   \n",
              "6125                       0.0                 0.0                   1.0   \n",
              "6126                       0.0                 0.0                   1.0   \n",
              "6127                       1.0                 0.0                   1.0   \n",
              "6128                       1.0                 0.0                   1.0   \n",
              "\n",
              "         horse_type_other_4  horse_rating_5  declared_weight_5  \\\n",
              "race_id                                                          \n",
              "0                       0.0            60.0              972.0   \n",
              "1                       0.0            60.0             1148.0   \n",
              "2                       0.0            60.0             1031.0   \n",
              "4                       0.0            60.0             1040.0   \n",
              "5                       1.0            60.0             1136.0   \n",
              "...                     ...             ...                ...   \n",
              "6123                    0.0            56.0             1127.0   \n",
              "6125                    0.0            71.0              999.0   \n",
              "6126                    0.0            71.0             1059.0   \n",
              "6127                    0.0            87.0             1192.0   \n",
              "6128                    0.0            73.0             1117.0   \n",
              "\n",
              "         actual_weight_5  win_odds_5  place_odds_5  horse_country_AUS_5  \\\n",
              "race_id                                                                   \n",
              "0                  131.0        50.0          14.0                  0.0   \n",
              "1                  125.0         2.3           1.2                  1.0   \n",
              "2                  126.0         4.4           1.5                  0.0   \n",
              "4                  128.0        33.0          10.0                  0.0   \n",
              "5                  126.0        13.0           2.9                  1.0   \n",
              "...                  ...         ...           ...                  ...   \n",
              "6123               129.0        16.0           4.0                  0.0   \n",
              "6125               122.0        72.0          17.0                  1.0   \n",
              "6126               124.0        99.0          40.0                  0.0   \n",
              "6127               122.0        29.0           6.5                  0.0   \n",
              "6128               124.0        71.0          11.0                  0.0   \n",
              "\n",
              "         horse_country_FR_5  horse_country_GB_5  horse_country_IRE_5  \\\n",
              "race_id                                                                \n",
              "0                       0.0                 1.0                  0.0   \n",
              "1                       0.0                 0.0                  0.0   \n",
              "2                       0.0                 0.0                  0.0   \n",
              "4                       0.0                 0.0                  1.0   \n",
              "5                       0.0                 0.0                  0.0   \n",
              "...                     ...                 ...                  ...   \n",
              "6123                    0.0                 0.0                  0.0   \n",
              "6125                    0.0                 0.0                  0.0   \n",
              "6126                    1.0                 0.0                  0.0   \n",
              "6127                    0.0                 0.0                  0.0   \n",
              "6128                    0.0                 0.0                  0.0   \n",
              "\n",
              "         horse_country_NZ_5  horse_country_USA_5  horse_country_other_5  \\\n",
              "race_id                                                                   \n",
              "0                       0.0                  0.0                    0.0   \n",
              "1                       0.0                  0.0                    0.0   \n",
              "2                       1.0                  0.0                    0.0   \n",
              "4                       0.0                  0.0                    0.0   \n",
              "5                       0.0                  0.0                    0.0   \n",
              "...                     ...                  ...                    ...   \n",
              "6123                    1.0                  0.0                    0.0   \n",
              "6125                    0.0                  0.0                    0.0   \n",
              "6126                    0.0                  0.0                    0.0   \n",
              "6127                    1.0                  0.0                    0.0   \n",
              "6128                    1.0                  0.0                    0.0   \n",
              "\n",
              "         horse_type_Brown_5  horse_type_Gelding_5  horse_type_other_5  \\\n",
              "race_id                                                                 \n",
              "0                       0.0                   1.0                 0.0   \n",
              "1                       0.0                   1.0                 0.0   \n",
              "2                       0.0                   1.0                 0.0   \n",
              "4                       0.0                   1.0                 0.0   \n",
              "5                       0.0                   1.0                 0.0   \n",
              "...                     ...                   ...                 ...   \n",
              "6123                    0.0                   1.0                 0.0   \n",
              "6125                    0.0                   1.0                 0.0   \n",
              "6126                    0.0                   1.0                 0.0   \n",
              "6127                    0.0                   1.0                 0.0   \n",
              "6128                    0.0                   1.0                 0.0   \n",
              "\n",
              "         horse_rating_6  declared_weight_6  actual_weight_6  win_odds_6  \\\n",
              "race_id                                                                   \n",
              "0                  60.0             1114.0            127.0         7.0   \n",
              "1                  60.0             1057.0            121.0        28.0   \n",
              "2                  60.0             1068.0            125.0        65.0   \n",
              "4                  60.0             1106.0            120.0        43.0   \n",
              "5                  60.0             1235.0            125.0        99.0   \n",
              "...                 ...                ...              ...         ...   \n",
              "6123               54.0             1007.0            125.0         7.7   \n",
              "6125               71.0             1146.0            117.0        49.0   \n",
              "6126               71.0             1285.0            124.0        11.0   \n",
              "6127               86.0             1103.0            123.0        10.0   \n",
              "6128               70.0             1166.0            123.0        99.0   \n",
              "\n",
              "         place_odds_6  horse_country_AUS_6  horse_country_FR_6  \\\n",
              "race_id                                                          \n",
              "0                 1.8                  0.0                 0.0   \n",
              "1                 6.3                  0.0                 0.0   \n",
              "2                23.0                  0.0                 0.0   \n",
              "4                18.0                  1.0                 0.0   \n",
              "5                24.0                  0.0                 0.0   \n",
              "...               ...                  ...                 ...   \n",
              "6123              2.2                  1.0                 0.0   \n",
              "6125             11.0                  0.0                 0.0   \n",
              "6126              3.7                  1.0                 0.0   \n",
              "6127              2.6                  1.0                 0.0   \n",
              "6128             23.0                  1.0                 0.0   \n",
              "\n",
              "         horse_country_GB_6  horse_country_IRE_6  horse_country_NZ_6  \\\n",
              "race_id                                                                \n",
              "0                       0.0                  0.0                 1.0   \n",
              "1                       0.0                  0.0                 1.0   \n",
              "2                       0.0                  0.0                 1.0   \n",
              "4                       0.0                  0.0                 0.0   \n",
              "5                       0.0                  0.0                 1.0   \n",
              "...                     ...                  ...                 ...   \n",
              "6123                    0.0                  0.0                 0.0   \n",
              "6125                    0.0                  0.0                 0.0   \n",
              "6126                    0.0                  0.0                 0.0   \n",
              "6127                    0.0                  0.0                 0.0   \n",
              "6128                    0.0                  0.0                 0.0   \n",
              "\n",
              "         horse_country_USA_6  horse_country_other_6  horse_type_Brown_6  \\\n",
              "race_id                                                                   \n",
              "0                        0.0                    0.0                 0.0   \n",
              "1                        0.0                    0.0                 0.0   \n",
              "2                        0.0                    0.0                 0.0   \n",
              "4                        0.0                    0.0                 0.0   \n",
              "5                        0.0                    0.0                 0.0   \n",
              "...                      ...                    ...                 ...   \n",
              "6123                     0.0                    0.0                 0.0   \n",
              "6125                     0.0                    1.0                 0.0   \n",
              "6126                     0.0                    0.0                 0.0   \n",
              "6127                     0.0                    0.0                 0.0   \n",
              "6128                     0.0                    0.0                 0.0   \n",
              "\n",
              "         horse_type_Gelding_6  horse_type_other_6  horse_rating_7  \\\n",
              "race_id                                                             \n",
              "0                         1.0                 0.0            60.0   \n",
              "1                         1.0                 0.0            60.0   \n",
              "2                         1.0                 0.0            60.0   \n",
              "4                         1.0                 0.0            60.0   \n",
              "5                         1.0                 0.0            60.0   \n",
              "...                       ...                 ...             ...   \n",
              "6123                      1.0                 0.0            54.0   \n",
              "6125                      1.0                 0.0            70.0   \n",
              "6126                      1.0                 0.0            69.0   \n",
              "6127                      1.0                 0.0            85.0   \n",
              "6128                      1.0                 0.0            70.0   \n",
              "\n",
              "         declared_weight_7  actual_weight_7  win_odds_7  place_odds_7  \\\n",
              "race_id                                                                 \n",
              "0                    978.0            123.0        99.0          28.0   \n",
              "1                   1064.0            122.0        13.0           3.3   \n",
              "2                   1151.0            123.0        99.0          25.0   \n",
              "4                   1019.0            125.0        12.0           4.0   \n",
              "5                   1015.0            120.0        99.0          23.0   \n",
              "...                    ...              ...         ...           ...   \n",
              "6123                1104.0            127.0        99.0          22.0   \n",
              "6125                1079.0            123.0         2.9           1.5   \n",
              "6126                1270.0            122.0         4.8           1.6   \n",
              "6127                1128.0            122.0         2.8           1.4   \n",
              "6128                1116.0            123.0        17.0           3.0   \n",
              "\n",
              "         horse_country_AUS_7  horse_country_FR_7  horse_country_GB_7  \\\n",
              "race_id                                                                \n",
              "0                        0.0                 0.0                 0.0   \n",
              "1                        0.0                 0.0                 0.0   \n",
              "2                        0.0                 0.0                 0.0   \n",
              "4                        1.0                 0.0                 0.0   \n",
              "5                        0.0                 0.0                 0.0   \n",
              "...                      ...                 ...                 ...   \n",
              "6123                     0.0                 0.0                 1.0   \n",
              "6125                     0.0                 0.0                 0.0   \n",
              "6126                     0.0                 0.0                 1.0   \n",
              "6127                     1.0                 0.0                 0.0   \n",
              "6128                     1.0                 0.0                 0.0   \n",
              "\n",
              "         horse_country_IRE_7  horse_country_NZ_7  horse_country_USA_7  \\\n",
              "race_id                                                                 \n",
              "0                        0.0                 1.0                  0.0   \n",
              "1                        0.0                 1.0                  0.0   \n",
              "2                        0.0                 1.0                  0.0   \n",
              "4                        0.0                 0.0                  0.0   \n",
              "5                        0.0                 1.0                  0.0   \n",
              "...                      ...                 ...                  ...   \n",
              "6123                     0.0                 0.0                  0.0   \n",
              "6125                     0.0                 1.0                  0.0   \n",
              "6126                     0.0                 0.0                  0.0   \n",
              "6127                     0.0                 0.0                  0.0   \n",
              "6128                     0.0                 0.0                  0.0   \n",
              "\n",
              "         horse_country_other_7  horse_type_Brown_7  horse_type_Gelding_7  \\\n",
              "race_id                                                                    \n",
              "0                          0.0                 0.0                   1.0   \n",
              "1                          0.0                 0.0                   1.0   \n",
              "2                          0.0                 0.0                   1.0   \n",
              "4                          0.0                 0.0                   1.0   \n",
              "5                          0.0                 0.0                   1.0   \n",
              "...                        ...                 ...                   ...   \n",
              "6123                       0.0                 0.0                   1.0   \n",
              "6125                       0.0                 0.0                   1.0   \n",
              "6126                       0.0                 0.0                   1.0   \n",
              "6127                       0.0                 0.0                   1.0   \n",
              "6128                       0.0                 0.0                   1.0   \n",
              "\n",
              "         horse_type_other_7  horse_rating_8  declared_weight_8  \\\n",
              "race_id                                                          \n",
              "0                       0.0            60.0             1170.0   \n",
              "1                       0.0            60.0             1132.0   \n",
              "2                       0.0            60.0             1052.0   \n",
              "4                       0.0            60.0             1009.0   \n",
              "5                       0.0            60.0             1114.0   \n",
              "...                     ...             ...                ...   \n",
              "6123                    0.0            52.0             1061.0   \n",
              "6125                    0.0            70.0             1076.0   \n",
              "6126                    0.0            68.0             1098.0   \n",
              "6127                    0.0            85.0             1101.0   \n",
              "6128                    0.0            68.0             1068.0   \n",
              "\n",
              "         actual_weight_8  win_odds_8  place_odds_8  horse_country_AUS_8  \\\n",
              "race_id                                                                   \n",
              "0                  128.0        12.0           3.6                  1.0   \n",
              "1                  117.0        43.0          11.0                  0.0   \n",
              "2                  125.0         6.7           2.0                  0.0   \n",
              "4                  120.0        13.0           4.9                  0.0   \n",
              "5                  124.0        32.0           7.0                  0.0   \n",
              "...                  ...         ...           ...                  ...   \n",
              "6123               125.0        86.0          14.0                  1.0   \n",
              "6125               123.0        14.0           3.2                  0.0   \n",
              "6126               121.0        40.0           8.8                  0.0   \n",
              "6127               122.0        99.0          16.0                  1.0   \n",
              "6128               121.0        55.0           9.0                  0.0   \n",
              "\n",
              "         horse_country_FR_8  horse_country_GB_8  horse_country_IRE_8  \\\n",
              "race_id                                                                \n",
              "0                       0.0                 0.0                  0.0   \n",
              "1                       0.0                 0.0                  0.0   \n",
              "2                       0.0                 0.0                  0.0   \n",
              "4                       0.0                 0.0                  0.0   \n",
              "5                       0.0                 0.0                  0.0   \n",
              "...                     ...                 ...                  ...   \n",
              "6123                    0.0                 0.0                  0.0   \n",
              "6125                    0.0                 0.0                  0.0   \n",
              "6126                    0.0                 0.0                  0.0   \n",
              "6127                    0.0                 0.0                  0.0   \n",
              "6128                    0.0                 0.0                  1.0   \n",
              "\n",
              "         horse_country_NZ_8  horse_country_USA_8  horse_country_other_8  \\\n",
              "race_id                                                                   \n",
              "0                       0.0                  0.0                    0.0   \n",
              "1                       1.0                  0.0                    0.0   \n",
              "2                       1.0                  0.0                    0.0   \n",
              "4                       1.0                  0.0                    0.0   \n",
              "5                       1.0                  0.0                    0.0   \n",
              "...                     ...                  ...                    ...   \n",
              "6123                    0.0                  0.0                    0.0   \n",
              "6125                    1.0                  0.0                    0.0   \n",
              "6126                    1.0                  0.0                    0.0   \n",
              "6127                    0.0                  0.0                    0.0   \n",
              "6128                    0.0                  0.0                    0.0   \n",
              "\n",
              "         horse_type_Brown_8  horse_type_Gelding_8  horse_type_other_8  \\\n",
              "race_id                                                                 \n",
              "0                       0.0                   1.0                 0.0   \n",
              "1                       0.0                   1.0                 0.0   \n",
              "2                       0.0                   1.0                 0.0   \n",
              "4                       1.0                   0.0                 0.0   \n",
              "5                       0.0                   1.0                 0.0   \n",
              "...                     ...                   ...                 ...   \n",
              "6123                    0.0                   1.0                 0.0   \n",
              "6125                    0.0                   1.0                 0.0   \n",
              "6126                    0.0                   1.0                 0.0   \n",
              "6127                    0.0                   1.0                 0.0   \n",
              "6128                    0.0                   1.0                 0.0   \n",
              "\n",
              "         horse_rating_9  declared_weight_9  actual_weight_9  win_odds_9  \\\n",
              "race_id                                                                   \n",
              "0                  60.0             1126.0            123.0        38.0   \n",
              "1                  60.0             1081.0            121.0        14.0   \n",
              "2                  60.0             1122.0            125.0         7.9   \n",
              "4                  60.0             1011.0            120.0         6.1   \n",
              "5                  60.0             1170.0            124.0         6.1   \n",
              "...                 ...                ...              ...         ...   \n",
              "6123               50.0             1150.0            123.0        99.0   \n",
              "6125               68.0             1186.0            121.0        95.0   \n",
              "6126               68.0             1050.0            121.0        93.0   \n",
              "6127               84.0             1105.0            119.0        55.0   \n",
              "6128               67.0             1091.0            118.0         7.9   \n",
              "\n",
              "         place_odds_9  horse_country_AUS_9  horse_country_FR_9  \\\n",
              "race_id                                                          \n",
              "0                13.0                  0.0                 0.0   \n",
              "1                 4.8                  0.0                 0.0   \n",
              "2                 3.0                  1.0                 0.0   \n",
              "4                 1.9                  0.0                 0.0   \n",
              "5                 1.6                  1.0                 0.0   \n",
              "...               ...                  ...                 ...   \n",
              "6123             27.0                  0.0                 0.0   \n",
              "6125             20.0                  0.0                 0.0   \n",
              "6126             16.0                  1.0                 0.0   \n",
              "6127              8.7                  0.0                 0.0   \n",
              "6128              2.0                  1.0                 0.0   \n",
              "\n",
              "         horse_country_GB_9  horse_country_IRE_9  horse_country_NZ_9  \\\n",
              "race_id                                                                \n",
              "0                       0.0                  0.0                 1.0   \n",
              "1                       0.0                  0.0                 1.0   \n",
              "2                       0.0                  0.0                 0.0   \n",
              "4                       0.0                  1.0                 0.0   \n",
              "5                       0.0                  0.0                 0.0   \n",
              "...                     ...                  ...                 ...   \n",
              "6123                    0.0                  1.0                 0.0   \n",
              "6125                    1.0                  0.0                 0.0   \n",
              "6126                    0.0                  0.0                 0.0   \n",
              "6127                    0.0                  0.0                 1.0   \n",
              "6128                    0.0                  0.0                 0.0   \n",
              "\n",
              "         horse_country_USA_9  horse_country_other_9  horse_type_Brown_9  \\\n",
              "race_id                                                                   \n",
              "0                        0.0                    0.0                 0.0   \n",
              "1                        0.0                    0.0                 0.0   \n",
              "2                        0.0                    0.0                 0.0   \n",
              "4                        0.0                    0.0                 0.0   \n",
              "5                        0.0                    0.0                 0.0   \n",
              "...                      ...                    ...                 ...   \n",
              "6123                     0.0                    0.0                 0.0   \n",
              "6125                     0.0                    0.0                 0.0   \n",
              "6126                     0.0                    0.0                 0.0   \n",
              "6127                     0.0                    0.0                 0.0   \n",
              "6128                     0.0                    0.0                 0.0   \n",
              "\n",
              "         horse_type_Gelding_9  horse_type_other_9  horse_rating_10  \\\n",
              "race_id                                                              \n",
              "0                         1.0                 0.0             60.0   \n",
              "1                         1.0                 0.0             60.0   \n",
              "2                         1.0                 0.0             60.0   \n",
              "4                         1.0                 0.0             60.0   \n",
              "5                         0.0                 1.0             60.0   \n",
              "...                       ...                 ...              ...   \n",
              "6123                      1.0                 0.0             48.0   \n",
              "6125                      1.0                 0.0             67.0   \n",
              "6126                      1.0                 0.0             66.0   \n",
              "6127                      1.0                 0.0             84.0   \n",
              "6128                      1.0                 0.0             65.0   \n",
              "\n",
              "         declared_weight_10  actual_weight_10  win_odds_10  place_odds_10  \\\n",
              "race_id                                                                     \n",
              "0                    1072.0             125.0         39.0           12.0   \n",
              "1                    1059.0             121.0         10.0            2.6   \n",
              "2                    1016.0             124.0         16.0            5.2   \n",
              "4                     999.0             114.0         11.0            2.8   \n",
              "5                    1015.0             123.0          8.5            2.8   \n",
              "...                     ...               ...          ...            ...   \n",
              "6123                 1037.0             121.0         17.0            3.9   \n",
              "6125                 1111.0             118.0          3.9            1.4   \n",
              "6126                 1193.0             119.0          7.5            2.1   \n",
              "6127                 1115.0             121.0         99.0           26.0   \n",
              "6128                 1141.0             116.0         33.0            5.7   \n",
              "\n",
              "         horse_country_AUS_10  horse_country_FR_10  horse_country_GB_10  \\\n",
              "race_id                                                                   \n",
              "0                         1.0                  0.0                  0.0   \n",
              "1                         1.0                  0.0                  0.0   \n",
              "2                         1.0                  0.0                  0.0   \n",
              "4                         0.0                  0.0                  0.0   \n",
              "5                         0.0                  0.0                  0.0   \n",
              "...                       ...                  ...                  ...   \n",
              "6123                      0.0                  0.0                  0.0   \n",
              "6125                      1.0                  0.0                  0.0   \n",
              "6126                      1.0                  0.0                  0.0   \n",
              "6127                      0.0                  0.0                  0.0   \n",
              "6128                      0.0                  0.0                  0.0   \n",
              "\n",
              "         horse_country_IRE_10  horse_country_NZ_10  horse_country_USA_10  \\\n",
              "race_id                                                                    \n",
              "0                         0.0                  0.0                   0.0   \n",
              "1                         0.0                  0.0                   0.0   \n",
              "2                         0.0                  0.0                   0.0   \n",
              "4                         1.0                  0.0                   0.0   \n",
              "5                         0.0                  1.0                   0.0   \n",
              "...                       ...                  ...                   ...   \n",
              "6123                      0.0                  1.0                   0.0   \n",
              "6125                      0.0                  0.0                   0.0   \n",
              "6126                      0.0                  0.0                   0.0   \n",
              "6127                      1.0                  0.0                   0.0   \n",
              "6128                      0.0                  1.0                   0.0   \n",
              "\n",
              "         horse_country_other_10  horse_type_Brown_10  horse_type_Gelding_10  \\\n",
              "race_id                                                                       \n",
              "0                           0.0                  0.0                    0.0   \n",
              "1                           0.0                  0.0                    1.0   \n",
              "2                           0.0                  0.0                    1.0   \n",
              "4                           0.0                  0.0                    1.0   \n",
              "5                           0.0                  0.0                    1.0   \n",
              "...                         ...                  ...                    ...   \n",
              "6123                        0.0                  0.0                    1.0   \n",
              "6125                        0.0                  0.0                    1.0   \n",
              "6126                        0.0                  0.0                    1.0   \n",
              "6127                        0.0                  0.0                    1.0   \n",
              "6128                        0.0                  0.0                    1.0   \n",
              "\n",
              "         horse_type_other_10  horse_rating_11  declared_weight_11  \\\n",
              "race_id                                                             \n",
              "0                        1.0             60.0              1135.0   \n",
              "1                        0.0             60.0              1106.0   \n",
              "2                        0.0             60.0              1149.0   \n",
              "4                        0.0             60.0              1011.0   \n",
              "5                        0.0             60.0              1087.0   \n",
              "...                      ...              ...                 ...   \n",
              "6123                     0.0             47.0              1045.0   \n",
              "6125                     0.0             67.0              1110.0   \n",
              "6126                     0.0             65.0               993.0   \n",
              "6127                     0.0             84.0              1167.0   \n",
              "6128                     0.0             64.0              1066.0   \n",
              "\n",
              "         actual_weight_11  win_odds_11  place_odds_11  horse_country_AUS_11  \\\n",
              "race_id                                                                       \n",
              "0                   123.0          8.6            2.5                   0.0   \n",
              "1                   118.0         50.0           15.0                   0.0   \n",
              "2                   122.0          7.0            2.3                   1.0   \n",
              "4                   117.0         19.0            6.4                   0.0   \n",
              "5                   121.0         37.0            5.9                   1.0   \n",
              "...                   ...          ...            ...                   ...   \n",
              "6123                120.0         12.0            3.4                   0.0   \n",
              "6125                120.0         15.0            3.8                   0.0   \n",
              "6126                111.0         33.0            7.3                   0.0   \n",
              "6127                121.0         85.0           13.0                   0.0   \n",
              "6128                110.0         99.0           14.0                   0.0   \n",
              "\n",
              "         horse_country_FR_11  horse_country_GB_11  horse_country_IRE_11  \\\n",
              "race_id                                                                   \n",
              "0                        0.0                  0.0                   0.0   \n",
              "1                        0.0                  0.0                   0.0   \n",
              "2                        0.0                  0.0                   0.0   \n",
              "4                        0.0                  0.0                   0.0   \n",
              "5                        0.0                  0.0                   0.0   \n",
              "...                      ...                  ...                   ...   \n",
              "6123                     0.0                  0.0                   0.0   \n",
              "6125                     0.0                  1.0                   0.0   \n",
              "6126                     0.0                  0.0                   0.0   \n",
              "6127                     0.0                  1.0                   0.0   \n",
              "6128                     0.0                  1.0                   0.0   \n",
              "\n",
              "         horse_country_NZ_11  horse_country_USA_11  horse_country_other_11  \\\n",
              "race_id                                                                      \n",
              "0                        1.0                   0.0                     0.0   \n",
              "1                        0.0                   1.0                     0.0   \n",
              "2                        0.0                   0.0                     0.0   \n",
              "4                        1.0                   0.0                     0.0   \n",
              "5                        0.0                   0.0                     0.0   \n",
              "...                      ...                   ...                     ...   \n",
              "6123                     1.0                   0.0                     0.0   \n",
              "6125                     0.0                   0.0                     0.0   \n",
              "6126                     1.0                   0.0                     0.0   \n",
              "6127                     0.0                   0.0                     0.0   \n",
              "6128                     0.0                   0.0                     0.0   \n",
              "\n",
              "         horse_type_Brown_11  horse_type_Gelding_11  horse_type_other_11  \\\n",
              "race_id                                                                    \n",
              "0                        0.0                    1.0                  0.0   \n",
              "1                        0.0                    1.0                  0.0   \n",
              "2                        0.0                    1.0                  0.0   \n",
              "4                        0.0                    1.0                  0.0   \n",
              "5                        0.0                    1.0                  0.0   \n",
              "...                      ...                    ...                  ...   \n",
              "6123                     0.0                    1.0                  0.0   \n",
              "6125                     0.0                    1.0                  0.0   \n",
              "6126                     0.0                    1.0                  0.0   \n",
              "6127                     0.0                    1.0                  0.0   \n",
              "6128                     0.0                    1.0                  0.0   \n",
              "\n",
              "         horse_rating_12  declared_weight_12  actual_weight_12  win_odds_12  \\\n",
              "race_id                                                                       \n",
              "0                   60.0              1018.0             123.0         23.0   \n",
              "1                   60.0              1095.0             103.0         21.0   \n",
              "2                   60.0              1028.0             116.0         45.0   \n",
              "4                   60.0               988.0             106.0         31.0   \n",
              "5                   60.0              1098.0             119.0         99.0   \n",
              "...                  ...                 ...               ...          ...   \n",
              "6123                46.0              1197.0             119.0          8.4   \n",
              "6125                66.0              1196.0             117.0         26.0   \n",
              "6126                63.0              1100.0             116.0         99.0   \n",
              "6127                83.0              1097.0             113.0         14.0   \n",
              "6128                63.0              1115.0             109.0          8.2   \n",
              "\n",
              "         place_odds_12  horse_country_AUS_12  horse_country_FR_12  \\\n",
              "race_id                                                             \n",
              "0                  8.5                   1.0                  0.0   \n",
              "1                  7.4                   1.0                  0.0   \n",
              "2                 11.0                   1.0                  0.0   \n",
              "4                  9.3                   0.0                  0.0   \n",
              "5                 40.0                   0.0                  0.0   \n",
              "...                ...                   ...                  ...   \n",
              "6123               3.0                   1.0                  0.0   \n",
              "6125               5.8                   1.0                  0.0   \n",
              "6126              36.0                   0.0                  0.0   \n",
              "6127               3.9                   1.0                  0.0   \n",
              "6128               2.0                   1.0                  0.0   \n",
              "\n",
              "         horse_country_GB_12  horse_country_IRE_12  horse_country_NZ_12  \\\n",
              "race_id                                                                   \n",
              "0                        0.0                   0.0                  0.0   \n",
              "1                        0.0                   0.0                  0.0   \n",
              "2                        0.0                   0.0                  0.0   \n",
              "4                        0.0                   0.0                  1.0   \n",
              "5                        0.0                   0.0                  1.0   \n",
              "...                      ...                   ...                  ...   \n",
              "6123                     0.0                   0.0                  0.0   \n",
              "6125                     0.0                   0.0                  0.0   \n",
              "6126                     0.0                   0.0                  1.0   \n",
              "6127                     0.0                   0.0                  0.0   \n",
              "6128                     0.0                   0.0                  0.0   \n",
              "\n",
              "         horse_country_USA_12  horse_country_other_12  horse_type_Brown_12  \\\n",
              "race_id                                                                      \n",
              "0                         0.0                     0.0                  0.0   \n",
              "1                         0.0                     0.0                  0.0   \n",
              "2                         0.0                     0.0                  0.0   \n",
              "4                         0.0                     0.0                  0.0   \n",
              "5                         0.0                     0.0                  0.0   \n",
              "...                       ...                     ...                  ...   \n",
              "6123                      0.0                     0.0                  0.0   \n",
              "6125                      0.0                     0.0                  0.0   \n",
              "6126                      0.0                     0.0                  0.0   \n",
              "6127                      0.0                     0.0                  0.0   \n",
              "6128                      0.0                     0.0                  0.0   \n",
              "\n",
              "         horse_type_Gelding_12  horse_type_other_12  horse_rating_13  \\\n",
              "race_id                                                                \n",
              "0                          1.0                  0.0             60.0   \n",
              "1                          1.0                  0.0             60.0   \n",
              "2                          1.0                  0.0             60.0   \n",
              "4                          1.0                  0.0             60.0   \n",
              "5                          1.0                  0.0             60.0   \n",
              "...                        ...                  ...              ...   \n",
              "6123                       1.0                  0.0             46.0   \n",
              "6125                       1.0                  0.0             63.0   \n",
              "6126                       1.0                  0.0             63.0   \n",
              "6127                       1.0                  0.0             83.0   \n",
              "6128                       1.0                  0.0             63.0   \n",
              "\n",
              "         declared_weight_13  actual_weight_13  win_odds_13  place_odds_13  \\\n",
              "race_id                                                                     \n",
              "0                    1089.0             120.0          5.4            1.7   \n",
              "1                    1060.0             113.0         13.0            4.7   \n",
              "2                    1029.0             110.0         22.0            5.9   \n",
              "4                    1157.0             113.0         99.0           26.0   \n",
              "5                    1064.0             115.0         22.0            5.3   \n",
              "...                     ...               ...          ...            ...   \n",
              "6123                 1133.0             117.0         53.0           12.0   \n",
              "6125                 1196.0             116.0         19.0            4.7   \n",
              "6126                 1168.0             116.0         11.0            3.2   \n",
              "6127                 1092.0             118.0         99.0           24.0   \n",
              "6128                 1180.0             117.0         59.0            9.9   \n",
              "\n",
              "         horse_country_AUS_13  horse_country_FR_13  horse_country_GB_13  \\\n",
              "race_id                                                                   \n",
              "0                         0.0                  0.0                  0.0   \n",
              "1                         1.0                  0.0                  0.0   \n",
              "2                         0.0                  0.0                  0.0   \n",
              "4                         0.0                  0.0                  0.0   \n",
              "5                         0.0                  0.0                  0.0   \n",
              "...                       ...                  ...                  ...   \n",
              "6123                      0.0                  0.0                  0.0   \n",
              "6125                      0.0                  0.0                  0.0   \n",
              "6126                      1.0                  0.0                  0.0   \n",
              "6127                      0.0                  0.0                  0.0   \n",
              "6128                      1.0                  0.0                  0.0   \n",
              "\n",
              "         horse_country_IRE_13  horse_country_NZ_13  horse_country_USA_13  \\\n",
              "race_id                                                                    \n",
              "0                         0.0                  0.0                   1.0   \n",
              "1                         0.0                  0.0                   0.0   \n",
              "2                         0.0                  1.0                   0.0   \n",
              "4                         0.0                  1.0                   0.0   \n",
              "5                         1.0                  0.0                   0.0   \n",
              "...                       ...                  ...                   ...   \n",
              "6123                      1.0                  0.0                   0.0   \n",
              "6125                      0.0                  1.0                   0.0   \n",
              "6126                      0.0                  0.0                   0.0   \n",
              "6127                      1.0                  0.0                   0.0   \n",
              "6128                      0.0                  0.0                   0.0   \n",
              "\n",
              "         horse_country_other_13  horse_type_Brown_13  horse_type_Gelding_13  \\\n",
              "race_id                                                                       \n",
              "0                           0.0                  0.0                    1.0   \n",
              "1                           0.0                  0.0                    1.0   \n",
              "2                           0.0                  0.0                    1.0   \n",
              "4                           0.0                  0.0                    1.0   \n",
              "5                           0.0                  0.0                    1.0   \n",
              "...                         ...                  ...                    ...   \n",
              "6123                        0.0                  0.0                    1.0   \n",
              "6125                        0.0                  0.0                    1.0   \n",
              "6126                        0.0                  0.0                    1.0   \n",
              "6127                        0.0                  0.0                    1.0   \n",
              "6128                        0.0                  0.0                    0.0   \n",
              "\n",
              "         horse_type_other_13  horse_rating_14  declared_weight_14  \\\n",
              "race_id                                                             \n",
              "0                        0.0             60.0              1027.0   \n",
              "1                        0.0             60.0              1110.0   \n",
              "2                        0.0             60.0              1157.0   \n",
              "4                        0.0             60.0              1152.0   \n",
              "5                        0.0             60.0              1155.0   \n",
              "...                      ...              ...                 ...   \n",
              "6123                     0.0             44.0              1113.0   \n",
              "6125                     0.0             62.0              1122.0   \n",
              "6126                     0.0             61.0              1084.0   \n",
              "6127                     0.0             81.0              1098.0   \n",
              "6128                     1.0             63.0              1151.0   \n",
              "\n",
              "         actual_weight_14  win_odds_14  place_odds_14  horse_country_AUS_14  \\\n",
              "race_id                                                                       \n",
              "0                   113.0         11.0            3.9                   1.0   \n",
              "1                   108.0         47.0           16.0                   1.0   \n",
              "2                   110.0         99.0           32.0                   1.0   \n",
              "4                   103.0         25.0            7.7                   0.0   \n",
              "5                   110.0         99.0           40.0                   1.0   \n",
              "...                   ...          ...            ...                   ...   \n",
              "6123                110.0         18.0            5.2                   0.0   \n",
              "6125                115.0          9.6            2.7                   1.0   \n",
              "6126                114.0         14.0            3.4                   0.0   \n",
              "6127                111.0          3.1            1.5                   1.0   \n",
              "6128                116.0         99.0           26.0                   1.0   \n",
              "\n",
              "         horse_country_FR_14  horse_country_GB_14  horse_country_IRE_14  \\\n",
              "race_id                                                                   \n",
              "0                        0.0                  0.0                   0.0   \n",
              "1                        0.0                  0.0                   0.0   \n",
              "2                        0.0                  0.0                   0.0   \n",
              "4                        0.0                  0.0                   0.0   \n",
              "5                        0.0                  0.0                   0.0   \n",
              "...                      ...                  ...                   ...   \n",
              "6123                     0.0                  1.0                   0.0   \n",
              "6125                     0.0                  0.0                   0.0   \n",
              "6126                     0.0                  0.0                   0.0   \n",
              "6127                     0.0                  0.0                   0.0   \n",
              "6128                     0.0                  0.0                   0.0   \n",
              "\n",
              "         horse_country_NZ_14  horse_country_USA_14  horse_country_other_14  \\\n",
              "race_id                                                                      \n",
              "0                        0.0                   0.0                     0.0   \n",
              "1                        0.0                   0.0                     0.0   \n",
              "2                        0.0                   0.0                     0.0   \n",
              "4                        1.0                   0.0                     0.0   \n",
              "5                        0.0                   0.0                     0.0   \n",
              "...                      ...                   ...                     ...   \n",
              "6123                     0.0                   0.0                     0.0   \n",
              "6125                     0.0                   0.0                     0.0   \n",
              "6126                     1.0                   0.0                     0.0   \n",
              "6127                     0.0                   0.0                     0.0   \n",
              "6128                     0.0                   0.0                     0.0   \n",
              "\n",
              "         horse_type_Brown_14  horse_type_Gelding_14  horse_type_other_14  \n",
              "race_id                                                                   \n",
              "0                        0.0                    1.0                  0.0  \n",
              "1                        1.0                    0.0                  0.0  \n",
              "2                        0.0                    1.0                  0.0  \n",
              "4                        0.0                    1.0                  0.0  \n",
              "5                        0.0                    1.0                  0.0  \n",
              "...                      ...                    ...                  ...  \n",
              "6123                     0.0                    1.0                  0.0  \n",
              "6125                     0.0                    1.0                  0.0  \n",
              "6126                     0.0                    1.0                  0.0  \n",
              "6127                     0.0                    1.0                  0.0  \n",
              "6128                     0.0                    1.0                  0.0  \n",
              "\n",
              "[2220 rows x 271 columns]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Xy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "y = Xy['place_combination1'].to_numpy()-1\n",
        "X = Xy.drop(columns = ['place_combination1'])\n",
        "\n",
        "# Split according to time since races are in chronological order\n",
        "\n",
        "train_size = int(len(Xy)*0.8)\n",
        "\n",
        "X_train, X_test, y_train, y_test = [X[0:train_size],X[train_size:],y[0:train_size],y[train_size:]]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Naive classifier based on win_odds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "win_odds_name = [i for i in X.columns if 'win_odds' in i]\n",
        "win_odds_index = [X.columns.to_list().index(i) for i in win_odds_name]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 9.7, 16. ,  3.5, ..., 23. ,  5.4, 11. ],\n",
              "       [14. , 28. ,  7. , ..., 21. , 13. , 47. ],\n",
              "       [ 9.2, 99. , 81. , ..., 45. , 22. , 99. ],\n",
              "       ...,\n",
              "       [26. , 21. ,  9.5, ..., 99. , 11. , 14. ],\n",
              "       [ 6.5, 39. , 14. , ..., 14. , 99. ,  3.1],\n",
              "       [99. , 13. ,  7.9, ...,  8.2, 59. , 99. ]])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.to_numpy()[:,win_odds_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.004954954954954955"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.mean(X.to_numpy()[:,win_odds_index].argmax(axis = 1) == y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# distance function:\n",
        "\n",
        "def most_frequent(List):\n",
        "    return max(set(List), key = List.count)\n",
        "\n",
        "def dist(point1, point2, option = 'manhattan'):\n",
        "    \n",
        "    if(option == 'manhattan'):\n",
        "        return np.sum(np.abs(point1-point2))\n",
        "    if(option == 'euclidean'):\n",
        "        return np.sqrt(np.sum((point1-point2)**2))\n",
        "    \n",
        "# main KNN:\n",
        "    \n",
        "def pred(X_test, X_train, y_train, k = 10):\n",
        "\n",
        "    y_pred = []\n",
        "\n",
        "    for x in tqdm(X_test):\n",
        "\n",
        "        distances = [dist(x, i) for i in X_train]\n",
        "        top_k = np.argsort(distances)[0:k]\n",
        "        labels = [y_train[i] for i in top_k]\n",
        "        y_pred.append(most_frequent(labels))\n",
        "    \n",
        "    return y_pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 191.73it/s]\n",
            "100%|██████████| 100/100 [00:00<00:00, 192.47it/s]\n",
            "100%|██████████| 100/100 [00:00<00:00, 192.93it/s]\n",
            "100%|██████████| 100/100 [00:00<00:00, 191.54it/s]\n",
            "100%|██████████| 100/100 [00:00<00:00, 192.65it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.11, 0.07, 0.09, 0.1, 0.1]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "# KNN hyperparmeter tuning (K)\n",
        "\n",
        "sample = np.random.choice(len(X_test), size = 100, replace=False)\n",
        "X_test_sample = X_test[sample]\n",
        "y_test_sample = y_test[sample]\n",
        "\n",
        "accs = []\n",
        "\n",
        "for k in [4,6,8,10,12]:\n",
        "\n",
        "    y_pred_k = pred(X_test_sample, X_train, y_train, k)\n",
        "    accs.append(np.mean(y_pred_k == y_test_sample))\n",
        "\n",
        "print(accs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/444 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 444/444 [00:02<00:00, 185.09it/s]\n"
          ]
        }
      ],
      "source": [
        "# KNN prediction\n",
        "\n",
        "y_pred_knn = pred(X_test, X_train, y_train, 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "YW4lI2iklGdM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base accuracy: 0.071\n",
            "KNN accuracy: 0.0788\n"
          ]
        }
      ],
      "source": [
        "acc_knn = np.mean(y_test == y_pred_knn)\n",
        "print(\"Base accuracy: 0.071\")\n",
        "print(\"KNN accuracy: {:.4f}\".format(acc_knn))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LibSVM]"
          ]
        }
      ],
      "source": [
        "model_svm = SVC(verbose = True, random_state=42)\n",
        "\n",
        "model_svm.fit(X_train, y_train)\n",
        "\n",
        "y_pred_svm = model_svm.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base accuracy: 0.071\n",
            "svm accuracy: 0.1374\n",
            "svm train accuracy: 0.8074\n"
          ]
        }
      ],
      "source": [
        "acc_svm = np.mean(y_test == y_pred_svm)\n",
        "print(\"Base accuracy: 0.071\")\n",
        "print(\"svm accuracy: {:.4f}\".format(acc_svm))\n",
        "y_train_pred_svm = model_svm.predict(X_train)\n",
        "acc_svm_train = np.mean(y_train_pred_svm == y_train)\n",
        "print(\"svm train accuracy: {:.4f}\".format(acc_svm_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "model_dt.fit(X_train, y_train)\n",
        "\n",
        "y_pred_dt = model_dt.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base accuracy: 0.071\n",
            "Decision Tree accuracy: 0.1509\n",
            "Decision Tree train accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "acc_dt = np.mean(y_test == y_pred_dt)\n",
        "print(\"Base accuracy: 0.071\")\n",
        "print(\"Decision Tree accuracy: {:.4f}\".format(acc_dt))\n",
        "y_train_pred_dt = model_dt.predict(X_train)\n",
        "acc_dt_train = np.mean(y_train_pred_dt == y_train)\n",
        "print(\"Decision Tree train accuracy: {:.4f}\".format(acc_dt_train))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "model_rf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_rf = model_rf.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base accuracy: 0.071\n",
            "Random Forest accuracy: 0.2410\n",
            "Random Forest train accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "acc_rf = np.mean(y_test == y_pred_rf)\n",
        "print(\"Base accuracy: 0.071\")\n",
        "print(\"Random Forest accuracy: {:.4f}\".format(acc_rf))\n",
        "y_train_pred_rf = model_rf.predict(X_train)\n",
        "acc_rf_train = np.mean(y_train_pred_rf == y_train)\n",
        "print(\"Random Forest train accuracy: {:.4f}\".format(acc_rf_train))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0]\teval-mlogloss:2.60217\n",
            "[1]\teval-mlogloss:2.57604\n",
            "[2]\teval-mlogloss:2.56038\n",
            "[3]\teval-mlogloss:2.53301\n",
            "[4]\teval-mlogloss:2.51597\n",
            "[5]\teval-mlogloss:2.50664\n",
            "[6]\teval-mlogloss:2.49898\n",
            "[7]\teval-mlogloss:2.48886\n",
            "[8]\teval-mlogloss:2.47697\n",
            "[9]\teval-mlogloss:2.46991\n",
            "[10]\teval-mlogloss:2.46260\n",
            "[11]\teval-mlogloss:2.45423\n",
            "[12]\teval-mlogloss:2.44510\n",
            "[13]\teval-mlogloss:2.43702\n",
            "[14]\teval-mlogloss:2.43297\n",
            "[15]\teval-mlogloss:2.42642\n",
            "[16]\teval-mlogloss:2.42884\n",
            "[17]\teval-mlogloss:2.42656\n"
          ]
        }
      ],
      "source": [
        "# Using xgboost since it is the most popular graident boosting method\n",
        "\n",
        "model_xgb = RandomForestClassifier(random_state=42)\n",
        "\n",
        "def data_loader():\n",
        "    return (X_train,y_train), (X_test,y_test)\n",
        "\n",
        "train_set = xgb.DMatrix(X_train,y_train)\n",
        "val_set = xgb.DMatrix(X_test,y_test)\n",
        "\n",
        "config = {\n",
        "    'max_depth': 10,\n",
        "    'eta': 0.1,\n",
        "    'objective': 'multi:softprob',  \n",
        "    'num_class': 14\n",
        "}\n",
        "\n",
        "model_xgb = xgb.train(\n",
        "    config,\n",
        "    dtrain = train_set,\n",
        "    num_boost_round = 50,\n",
        "    evals = [(val_set, 'eval')],\n",
        "    early_stopping_rounds=2,\n",
        ")\n",
        "y_pred_xgb = model_xgb.predict(xgb.DMatrix(X_test)).argmax(axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base accuracy: 0.071\n",
            "X Gradient Boost accuracy: 0.1802\n",
            "X Gradient Boost train accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "acc_xgb = np.mean(y_test == y_pred_xgb)\n",
        "print(\"Base accuracy: 0.071\")\n",
        "print(\"X Gradient Boost accuracy: {:.4f}\".format(acc_xgb))\n",
        "y_train_pred_xgb = model_xgb.predict(xgb.DMatrix(X_train)).argmax(axis = 1)\n",
        "acc_xgb_train = np.mean(y_train_pred_xgb == y_train)\n",
        "print(\"X Gradient Boost train accuracy: {:.4f}\".format(acc_xgb_train))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparametre Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\andyw\\anaconda3\\lib\\site-packages\\tune_sklearn\\tune_search.py:389: UserWarning: Ignoring early_stopping value, as BOHB requires HyperBandForBOHB as the EarlyStopping scheduler\n",
            "  warnings.warn(\"Ignoring early_stopping value, \"\n",
            "2023-11-11 19:55:53,480\tINFO worker.py:1538 -- Started a local Ray instance.\n",
            "2023-11-11 19:55:55,680\tWARNING tune.py:674 -- Tune detects GPUs, but no trials are using GPUs. To enable trials to use GPUs, wrap `train_func` with `tune.with_resources(train_func, resources_per_trial={'gpu': 1})` which allows Tune to expose 1 GPU to each trial. For Ray AIR Trainers, you can specify GPU resources through `ScalingConfig(use_gpu=True)`. You can also override `Trainable.default_resource_request` if using the Trainable API.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div class=\"tuneStatus\">\n",
              "  <div style=\"display: flex;flex-direction: row\">\n",
              "    <div style=\"display: flex;flex-direction: column;\">\n",
              "      <h3>Tune Status</h3>\n",
              "      <table>\n",
              "<tbody>\n",
              "<tr><td>Current time:</td><td>2023-11-11 19:56:19</td></tr>\n",
              "<tr><td>Running for: </td><td>00:00:24.32        </td></tr>\n",
              "<tr><td>Memory:      </td><td>15.5/63.8 GiB      </td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "    </div>\n",
              "    <div class=\"vDivider\"></div>\n",
              "    <div class=\"systemInfo\">\n",
              "      <h3>System Info</h3>\n",
              "      Using HyperBand: num_stopped=13 total_brackets=4<br>Round #0:<br>  Bracket(Max Size (n)=1, Milestone (r)=7, completed=100.0%): {TERMINATED: 9} <br>  Bracket(Max Size (n)=2, Milestone (r)=7, completed=100.0%): {TERMINATED: 5} <br>  Bracket(Max Size (n)=3, Milestone (r)=10, completed=100.0%): {TERMINATED: 3} <br>Round #1:<br>  Bracket(Max Size (n)=1, Milestone (r)=7, completed=100.0%): {TERMINATED: 3} <br>Resources requested: 0/24 CPUs, 0/1 GPUs, 0.0/29.55 GiB heap, 0.0/14.78 GiB objects (0.0/1.0 accelerator_type:G)\n",
              "    </div>\n",
              "    \n",
              "  </div>\n",
              "  <div class=\"hDivider\"></div>\n",
              "  <div class=\"trialStatus\">\n",
              "    <h3>Trial Status</h3>\n",
              "    <table>\n",
              "<thead>\n",
              "<tr><th>Trial name         </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  max_depth</th><th style=\"text-align: right;\">  max_features</th><th style=\"text-align: right;\">  min_samples_leaf</th><th style=\"text-align: right;\">  min_samples_split</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  split0_test_score</th><th style=\"text-align: right;\">  split1_test_score</th><th style=\"text-align: right;\">  split2_test_score</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>_Trainable_67f630e5</td><td>TERMINATED</td><td>127.0.0.1:38324</td><td style=\"text-align: right;\">         30</td><td style=\"text-align: right;\">           100</td><td style=\"text-align: right;\">                10</td><td style=\"text-align: right;\">                 30</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.771686</td><td style=\"text-align: right;\">           0.255618</td><td style=\"text-align: right;\">           0.270423</td><td style=\"text-align: right;\">           0.256338</td></tr>\n",
              "<tr><td>_Trainable_90d9c4b2</td><td>TERMINATED</td><td>127.0.0.1:41776</td><td style=\"text-align: right;\">         40</td><td style=\"text-align: right;\">           140</td><td style=\"text-align: right;\">                30</td><td style=\"text-align: right;\">                 30</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">        2.23906 </td><td style=\"text-align: right;\">           0.280899</td><td style=\"text-align: right;\">           0.304225</td><td style=\"text-align: right;\">           0.253521</td></tr>\n",
              "<tr><td>_Trainable_52193e05</td><td>TERMINATED</td><td>127.0.0.1:39092</td><td style=\"text-align: right;\">         20</td><td style=\"text-align: right;\">            60</td><td style=\"text-align: right;\">                20</td><td style=\"text-align: right;\">                 30</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.472615</td><td style=\"text-align: right;\">           0.269663</td><td style=\"text-align: right;\">           0.276056</td><td style=\"text-align: right;\">           0.233803</td></tr>\n",
              "<tr><td>_Trainable_4363807b</td><td>TERMINATED</td><td>127.0.0.1:41776</td><td style=\"text-align: right;\">         40</td><td style=\"text-align: right;\">            60</td><td style=\"text-align: right;\">                20</td><td style=\"text-align: right;\">                 20</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.511613</td><td style=\"text-align: right;\">           0.269663</td><td style=\"text-align: right;\">           0.276056</td><td style=\"text-align: right;\">           0.233803</td></tr>\n",
              "<tr><td>_Trainable_0f4a375f</td><td>TERMINATED</td><td>127.0.0.1:42032</td><td style=\"text-align: right;\">         20</td><td style=\"text-align: right;\">           140</td><td style=\"text-align: right;\">                10</td><td style=\"text-align: right;\">                 20</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.12829 </td><td style=\"text-align: right;\">           0.27809 </td><td style=\"text-align: right;\">           0.230986</td><td style=\"text-align: right;\">           0.239437</td></tr>\n",
              "<tr><td>_Trainable_d25cce99</td><td>TERMINATED</td><td>127.0.0.1:28984</td><td style=\"text-align: right;\">         40</td><td style=\"text-align: right;\">           100</td><td style=\"text-align: right;\">                30</td><td style=\"text-align: right;\">                 10</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">        1.48732 </td><td style=\"text-align: right;\">           0.272472</td><td style=\"text-align: right;\">           0.290141</td><td style=\"text-align: right;\">           0.256338</td></tr>\n",
              "<tr><td>_Trainable_970eefb2</td><td>TERMINATED</td><td>127.0.0.1:14868</td><td style=\"text-align: right;\">         30</td><td style=\"text-align: right;\">            60</td><td style=\"text-align: right;\">                30</td><td style=\"text-align: right;\">                 30</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.417099</td><td style=\"text-align: right;\">           0.269663</td><td style=\"text-align: right;\">           0.270423</td><td style=\"text-align: right;\">           0.247887</td></tr>\n",
              "<tr><td>_Trainable_be3240a8</td><td>TERMINATED</td><td>127.0.0.1:15540</td><td style=\"text-align: right;\">         40</td><td style=\"text-align: right;\">           140</td><td style=\"text-align: right;\">                10</td><td style=\"text-align: right;\">                 20</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.26658 </td><td style=\"text-align: right;\">           0.27809 </td><td style=\"text-align: right;\">           0.230986</td><td style=\"text-align: right;\">           0.239437</td></tr>\n",
              "<tr><td>_Trainable_36d96126</td><td>TERMINATED</td><td>127.0.0.1:39332</td><td style=\"text-align: right;\">         40</td><td style=\"text-align: right;\">            60</td><td style=\"text-align: right;\">                30</td><td style=\"text-align: right;\">                 10</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">        2.14894 </td><td style=\"text-align: right;\">           0.280899</td><td style=\"text-align: right;\">           0.292958</td><td style=\"text-align: right;\">           0.264789</td></tr>\n",
              "<tr><td>_Trainable_06a299dd</td><td>TERMINATED</td><td>127.0.0.1:38488</td><td style=\"text-align: right;\">         20</td><td style=\"text-align: right;\">            60</td><td style=\"text-align: right;\">                20</td><td style=\"text-align: right;\">                 20</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">        1.29571 </td><td style=\"text-align: right;\">           0.272472</td><td style=\"text-align: right;\">           0.292958</td><td style=\"text-align: right;\">           0.250704</td></tr>\n",
              "<tr><td>_Trainable_44ebbe5c</td><td>TERMINATED</td><td>127.0.0.1:38476</td><td style=\"text-align: right;\">         40</td><td style=\"text-align: right;\">           100</td><td style=\"text-align: right;\">                10</td><td style=\"text-align: right;\">                 30</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">        4.58856 </td><td style=\"text-align: right;\">           0.27809 </td><td style=\"text-align: right;\">           0.307042</td><td style=\"text-align: right;\">           0.28169 </td></tr>\n",
              "<tr><td>_Trainable_dc674057</td><td>TERMINATED</td><td>127.0.0.1:26904</td><td style=\"text-align: right;\">         40</td><td style=\"text-align: right;\">           140</td><td style=\"text-align: right;\">                10</td><td style=\"text-align: right;\">                 20</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">        2.79284 </td><td style=\"text-align: right;\">           0.27809 </td><td style=\"text-align: right;\">           0.304225</td><td style=\"text-align: right;\">           0.264789</td></tr>\n",
              "<tr><td>_Trainable_996c6aaf</td><td>TERMINATED</td><td>127.0.0.1:15640</td><td style=\"text-align: right;\">         40</td><td style=\"text-align: right;\">           100</td><td style=\"text-align: right;\">                10</td><td style=\"text-align: right;\">                 20</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">        2.36121 </td><td style=\"text-align: right;\">           0.258427</td><td style=\"text-align: right;\">           0.276056</td><td style=\"text-align: right;\">           0.273239</td></tr>\n",
              "<tr><td>_Trainable_b3e18fb1</td><td>TERMINATED</td><td>127.0.0.1:29864</td><td style=\"text-align: right;\">         20</td><td style=\"text-align: right;\">            60</td><td style=\"text-align: right;\">                30</td><td style=\"text-align: right;\">                 20</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">        2.31868 </td><td style=\"text-align: right;\">           0.280899</td><td style=\"text-align: right;\">           0.292958</td><td style=\"text-align: right;\">           0.264789</td></tr>\n",
              "<tr><td>_Trainable_9bed8d31</td><td>TERMINATED</td><td>127.0.0.1:15728</td><td style=\"text-align: right;\">         40</td><td style=\"text-align: right;\">           140</td><td style=\"text-align: right;\">                10</td><td style=\"text-align: right;\">                 20</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        8.85988 </td><td style=\"text-align: right;\">           0.27809 </td><td style=\"text-align: right;\">           0.301408</td><td style=\"text-align: right;\">           0.264789</td></tr>\n",
              "<tr><td>_Trainable_1368811e</td><td>TERMINATED</td><td>127.0.0.1:37952</td><td style=\"text-align: right;\">         20</td><td style=\"text-align: right;\">           140</td><td style=\"text-align: right;\">                30</td><td style=\"text-align: right;\">                 30</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        6.337   </td><td style=\"text-align: right;\">           0.275281</td><td style=\"text-align: right;\">           0.323944</td><td style=\"text-align: right;\">           0.261972</td></tr>\n",
              "<tr><td>_Trainable_11e12abd</td><td>TERMINATED</td><td>127.0.0.1:34716</td><td style=\"text-align: right;\">         20</td><td style=\"text-align: right;\">           100</td><td style=\"text-align: right;\">                20</td><td style=\"text-align: right;\">                 10</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        5.43897 </td><td style=\"text-align: right;\">           0.280899</td><td style=\"text-align: right;\">           0.315493</td><td style=\"text-align: right;\">           0.267606</td></tr>\n",
              "<tr><td>_Trainable_b6ed9155</td><td>TERMINATED</td><td>127.0.0.1:38476</td><td style=\"text-align: right;\">         30</td><td style=\"text-align: right;\">            60</td><td style=\"text-align: right;\">                20</td><td style=\"text-align: right;\">                 10</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">        1.10473 </td><td style=\"text-align: right;\">           0.272472</td><td style=\"text-align: right;\">           0.292958</td><td style=\"text-align: right;\">           0.250704</td></tr>\n",
              "<tr><td>_Trainable_3d87978f</td><td>TERMINATED</td><td>127.0.0.1:31912</td><td style=\"text-align: right;\">         20</td><td style=\"text-align: right;\">           100</td><td style=\"text-align: right;\">                10</td><td style=\"text-align: right;\">                 20</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">        1.99652 </td><td style=\"text-align: right;\">           0.258427</td><td style=\"text-align: right;\">           0.276056</td><td style=\"text-align: right;\">           0.273239</td></tr>\n",
              "<tr><td>_Trainable_82c8fe81</td><td>TERMINATED</td><td>127.0.0.1:11108</td><td style=\"text-align: right;\">         20</td><td style=\"text-align: right;\">            60</td><td style=\"text-align: right;\">                30</td><td style=\"text-align: right;\">                 20</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">        2.06955 </td><td style=\"text-align: right;\">           0.280899</td><td style=\"text-align: right;\">           0.292958</td><td style=\"text-align: right;\">           0.264789</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "  </div>\n",
              "</div>\n",
              "<style>\n",
              ".tuneStatus {\n",
              "  color: var(--jp-ui-font-color1);\n",
              "}\n",
              ".tuneStatus .systemInfo {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              ".tuneStatus td {\n",
              "  white-space: nowrap;\n",
              "}\n",
              ".tuneStatus .trialStatus {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              ".tuneStatus h3 {\n",
              "  font-weight: bold;\n",
              "}\n",
              ".tuneStatus .hDivider {\n",
              "  border-bottom-width: var(--jp-border-width);\n",
              "  border-bottom-color: var(--jp-border-color0);\n",
              "  border-bottom-style: solid;\n",
              "}\n",
              ".tuneStatus .vDivider {\n",
              "  border-left-width: var(--jp-border-width);\n",
              "  border-left-color: var(--jp-border-color0);\n",
              "  border-left-style: solid;\n",
              "  margin: 0.5em 1em 0.5em 1em;\n",
              "}\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div class=\"trialProgress\">\n",
              "  <h3>Trial Progress</h3>\n",
              "  <table>\n",
              "<thead>\n",
              "<tr><th>Trial name         </th><th style=\"text-align: right;\">  average_test_score</th><th style=\"text-align: right;\">  objective</th><th style=\"text-align: right;\">  split0_test_score</th><th style=\"text-align: right;\">  split1_test_score</th><th style=\"text-align: right;\">  split2_test_score</th><th style=\"text-align: right;\">  split3_test_score</th><th style=\"text-align: right;\">  split4_test_score</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>_Trainable_06a299dd</td><td style=\"text-align: right;\">            0.260693</td><td style=\"text-align: right;\">   0.260693</td><td style=\"text-align: right;\">           0.269663</td><td style=\"text-align: right;\">           0.276056</td><td style=\"text-align: right;\">           0.233803</td><td style=\"text-align: right;\">           0.256338</td><td style=\"text-align: right;\">           0.267606</td></tr>\n",
              "<tr><td>_Trainable_0f4a375f</td><td style=\"text-align: right;\">            0.251111</td><td style=\"text-align: right;\">   0.251111</td><td style=\"text-align: right;\">           0.27809 </td><td style=\"text-align: right;\">           0.230986</td><td style=\"text-align: right;\">           0.239437</td><td style=\"text-align: right;\">           0.239437</td><td style=\"text-align: right;\">           0.267606</td></tr>\n",
              "<tr><td>_Trainable_11e12abd</td><td style=\"text-align: right;\">            0.285476</td><td style=\"text-align: right;\">   0.285476</td><td style=\"text-align: right;\">           0.280899</td><td style=\"text-align: right;\">           0.315493</td><td style=\"text-align: right;\">           0.267606</td><td style=\"text-align: right;\">           0.253521</td><td style=\"text-align: right;\">           0.309859</td></tr>\n",
              "<tr><td>_Trainable_1368811e</td><td style=\"text-align: right;\">            0.282662</td><td style=\"text-align: right;\">   0.282662</td><td style=\"text-align: right;\">           0.275281</td><td style=\"text-align: right;\">           0.323944</td><td style=\"text-align: right;\">           0.261972</td><td style=\"text-align: right;\">           0.24507 </td><td style=\"text-align: right;\">           0.307042</td></tr>\n",
              "<tr><td>_Trainable_36d96126</td><td style=\"text-align: right;\">            0.279278</td><td style=\"text-align: right;\">   0.279278</td><td style=\"text-align: right;\">           0.280899</td><td style=\"text-align: right;\">           0.292958</td><td style=\"text-align: right;\">           0.264789</td><td style=\"text-align: right;\">           0.253521</td><td style=\"text-align: right;\">           0.304225</td></tr>\n",
              "<tr><td>_Trainable_3d87978f</td><td style=\"text-align: right;\">            0.26746 </td><td style=\"text-align: right;\">   0.26746 </td><td style=\"text-align: right;\">           0.258427</td><td style=\"text-align: right;\">           0.276056</td><td style=\"text-align: right;\">           0.273239</td><td style=\"text-align: right;\">           0.24507 </td><td style=\"text-align: right;\">           0.284507</td></tr>\n",
              "<tr><td>_Trainable_4363807b</td><td style=\"text-align: right;\">            0.260693</td><td style=\"text-align: right;\">   0.260693</td><td style=\"text-align: right;\">           0.269663</td><td style=\"text-align: right;\">           0.276056</td><td style=\"text-align: right;\">           0.233803</td><td style=\"text-align: right;\">           0.256338</td><td style=\"text-align: right;\">           0.267606</td></tr>\n",
              "<tr><td>_Trainable_44ebbe5c</td><td style=\"text-align: right;\">            0.281533</td><td style=\"text-align: right;\">   0.281533</td><td style=\"text-align: right;\">           0.27809 </td><td style=\"text-align: right;\">           0.307042</td><td style=\"text-align: right;\">           0.28169 </td><td style=\"text-align: right;\">           0.24507 </td><td style=\"text-align: right;\">           0.295775</td></tr>\n",
              "<tr><td>_Trainable_52193e05</td><td style=\"text-align: right;\">            0.260693</td><td style=\"text-align: right;\">   0.260693</td><td style=\"text-align: right;\">           0.269663</td><td style=\"text-align: right;\">           0.276056</td><td style=\"text-align: right;\">           0.233803</td><td style=\"text-align: right;\">           0.256338</td><td style=\"text-align: right;\">           0.267606</td></tr>\n",
              "<tr><td>_Trainable_67f630e5</td><td style=\"text-align: right;\">            0.260701</td><td style=\"text-align: right;\">   0.260701</td><td style=\"text-align: right;\">           0.255618</td><td style=\"text-align: right;\">           0.270423</td><td style=\"text-align: right;\">           0.256338</td><td style=\"text-align: right;\">           0.216901</td><td style=\"text-align: right;\">           0.304225</td></tr>\n",
              "<tr><td>_Trainable_82c8fe81</td><td style=\"text-align: right;\">            0.279278</td><td style=\"text-align: right;\">   0.279278</td><td style=\"text-align: right;\">           0.280899</td><td style=\"text-align: right;\">           0.292958</td><td style=\"text-align: right;\">           0.264789</td><td style=\"text-align: right;\">           0.253521</td><td style=\"text-align: right;\">           0.304225</td></tr>\n",
              "<tr><td>_Trainable_90d9c4b2</td><td style=\"text-align: right;\">            0.27253 </td><td style=\"text-align: right;\">   0.27253 </td><td style=\"text-align: right;\">           0.258427</td><td style=\"text-align: right;\">           0.298592</td><td style=\"text-align: right;\">           0.247887</td><td style=\"text-align: right;\">           0.242254</td><td style=\"text-align: right;\">           0.315493</td></tr>\n",
              "<tr><td>_Trainable_970eefb2</td><td style=\"text-align: right;\">            0.264073</td><td style=\"text-align: right;\">   0.264073</td><td style=\"text-align: right;\">           0.269663</td><td style=\"text-align: right;\">           0.270423</td><td style=\"text-align: right;\">           0.247887</td><td style=\"text-align: right;\">           0.230986</td><td style=\"text-align: right;\">           0.301408</td></tr>\n",
              "<tr><td>_Trainable_996c6aaf</td><td style=\"text-align: right;\">            0.254509</td><td style=\"text-align: right;\">   0.254509</td><td style=\"text-align: right;\">           0.247191</td><td style=\"text-align: right;\">           0.253521</td><td style=\"text-align: right;\">           0.256338</td><td style=\"text-align: right;\">           0.242254</td><td style=\"text-align: right;\">           0.273239</td></tr>\n",
              "<tr><td>_Trainable_9bed8d31</td><td style=\"text-align: right;\">            0.28435 </td><td style=\"text-align: right;\">   0.28435 </td><td style=\"text-align: right;\">           0.27809 </td><td style=\"text-align: right;\">           0.301408</td><td style=\"text-align: right;\">           0.264789</td><td style=\"text-align: right;\">           0.253521</td><td style=\"text-align: right;\">           0.323944</td></tr>\n",
              "<tr><td>_Trainable_b3e18fb1</td><td style=\"text-align: right;\">            0.279278</td><td style=\"text-align: right;\">   0.279278</td><td style=\"text-align: right;\">           0.280899</td><td style=\"text-align: right;\">           0.292958</td><td style=\"text-align: right;\">           0.264789</td><td style=\"text-align: right;\">           0.253521</td><td style=\"text-align: right;\">           0.304225</td></tr>\n",
              "<tr><td>_Trainable_b6ed9155</td><td style=\"text-align: right;\">            0.272526</td><td style=\"text-align: right;\">   0.272526</td><td style=\"text-align: right;\">           0.266854</td><td style=\"text-align: right;\">           0.292958</td><td style=\"text-align: right;\">           0.250704</td><td style=\"text-align: right;\">           0.256338</td><td style=\"text-align: right;\">           0.295775</td></tr>\n",
              "<tr><td>_Trainable_be3240a8</td><td style=\"text-align: right;\">            0.251111</td><td style=\"text-align: right;\">   0.251111</td><td style=\"text-align: right;\">           0.27809 </td><td style=\"text-align: right;\">           0.230986</td><td style=\"text-align: right;\">           0.239437</td><td style=\"text-align: right;\">           0.239437</td><td style=\"text-align: right;\">           0.267606</td></tr>\n",
              "<tr><td>_Trainable_d25cce99</td><td style=\"text-align: right;\">            0.269706</td><td style=\"text-align: right;\">   0.269706</td><td style=\"text-align: right;\">           0.272472</td><td style=\"text-align: right;\">           0.290141</td><td style=\"text-align: right;\">           0.256338</td><td style=\"text-align: right;\">           0.228169</td><td style=\"text-align: right;\">           0.301408</td></tr>\n",
              "<tr><td>_Trainable_dc674057</td><td style=\"text-align: right;\">            0.27759 </td><td style=\"text-align: right;\">   0.27759 </td><td style=\"text-align: right;\">           0.27809 </td><td style=\"text-align: right;\">           0.304225</td><td style=\"text-align: right;\">           0.264789</td><td style=\"text-align: right;\">           0.250704</td><td style=\"text-align: right;\">           0.290141</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</div>\n",
              "<style>\n",
              ".trialProgress {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  color: var(--jp-ui-font-color1);\n",
              "}\n",
              ".trialProgress h3 {\n",
              "  font-weight: bold;\n",
              "}\n",
              ".trialProgress td {\n",
              "  white-space: nowrap;\n",
              "}\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(_Trainable pid=41776)\u001b[0m 2023-11-11 19:56:07,138\tINFO trainable.py:790 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\andyw\\AppData\\Local\\Temp\\checkpoint_tmp_2a68334353a042c6a7abb220df8e5ef7\n",
            "\u001b[2m\u001b[36m(_Trainable pid=41776)\u001b[0m 2023-11-11 19:56:07,138\tINFO trainable.py:799 -- Current state after restoring: {'_iteration': 1, '_timesteps_total': None, '_time_total': 0.8771958351135254, '_episodes_total': None}\n",
            "\u001b[2m\u001b[36m(_Trainable pid=38476)\u001b[0m 2023-11-11 19:56:08,980\tINFO trainable.py:790 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\andyw\\AppData\\Local\\Temp\\checkpoint_tmp_e022942bd2f7410f81decfd55598faf3\n",
            "\u001b[2m\u001b[36m(_Trainable pid=38476)\u001b[0m 2023-11-11 19:56:08,980\tINFO trainable.py:799 -- Current state after restoring: {'_iteration': 3, '_timesteps_total': None, '_time_total': 2.0835771560668945, '_episodes_total': None}\n",
            "\u001b[2m\u001b[36m(_Trainable pid=39332)\u001b[0m 2023-11-11 19:56:10,524\tINFO trainable.py:790 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\andyw\\AppData\\Local\\Temp\\checkpoint_tmp_daabb8b94a184e0282e5cbcd5245a662\n",
            "\u001b[2m\u001b[36m(_Trainable pid=39332)\u001b[0m 2023-11-11 19:56:10,524\tINFO trainable.py:799 -- Current state after restoring: {'_iteration': 1, '_timesteps_total': None, '_time_total': 0.4080979824066162, '_episodes_total': None}\n",
            "\u001b[2m\u001b[36m(_Trainable pid=28984)\u001b[0m 2023-11-11 19:56:10,526\tINFO trainable.py:790 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\andyw\\AppData\\Local\\Temp\\checkpoint_tmp_7fb29f0cf85f46f38d5d396527af0fe4\n",
            "\u001b[2m\u001b[36m(_Trainable pid=28984)\u001b[0m 2023-11-11 19:56:10,526\tINFO trainable.py:799 -- Current state after restoring: {'_iteration': 1, '_timesteps_total': None, '_time_total': 0.6361420154571533, '_episodes_total': None}\n",
            "\u001b[2m\u001b[36m(_Trainable pid=39332)\u001b[0m 2023-11-11 19:56:11,408\tINFO trainable.py:790 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\andyw\\AppData\\Local\\Temp\\checkpoint_tmp_b47c756ba81343f6a447796d6d68de7f\n",
            "\u001b[2m\u001b[36m(_Trainable pid=39332)\u001b[0m 2023-11-11 19:56:11,408\tINFO trainable.py:799 -- Current state after restoring: {'_iteration': 3, '_timesteps_total': None, '_time_total': 0.9877192974090576, '_episodes_total': None}\n",
            "\u001b[2m\u001b[36m(_Trainable pid=29864)\u001b[0m 2023-11-11 19:56:12,043\tINFO trainable.py:790 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\andyw\\AppData\\Local\\Temp\\checkpoint_tmp_9c830f5c35a6460192853ef7eb42fccd\n",
            "\u001b[2m\u001b[36m(_Trainable pid=29864)\u001b[0m 2023-11-11 19:56:12,043\tINFO trainable.py:799 -- Current state after restoring: {'_iteration': 3, '_timesteps_total': None, '_time_total': 1.2063729763031006, '_episodes_total': None}\n",
            "\u001b[2m\u001b[36m(_Trainable pid=38476)\u001b[0m 2023-11-11 19:56:14,770\tINFO trainable.py:790 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\andyw\\AppData\\Local\\Temp\\checkpoint_tmp_35ad00d1cc434056b8d2b80dc21e1104\n",
            "\u001b[2m\u001b[36m(_Trainable pid=38476)\u001b[0m 2023-11-11 19:56:14,770\tINFO trainable.py:799 -- Current state after restoring: {'_iteration': 1, '_timesteps_total': None, '_time_total': 0.48261475563049316, '_episodes_total': None}\n",
            "\u001b[2m\u001b[36m(_Trainable pid=31912)\u001b[0m 2023-11-11 19:56:17,663\tINFO trainable.py:790 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\andyw\\AppData\\Local\\Temp\\checkpoint_tmp_8c5a83eeecd94f6d912b3d679e23a2fd\n",
            "\u001b[2m\u001b[36m(_Trainable pid=31912)\u001b[0m 2023-11-11 19:56:17,663\tINFO trainable.py:799 -- Current state after restoring: {'_iteration': 1, '_timesteps_total': None, '_time_total': 0.8342454433441162, '_episodes_total': None}\n",
            "\u001b[2m\u001b[36m(_Trainable pid=11108)\u001b[0m 2023-11-11 19:56:17,663\tINFO trainable.py:790 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\andyw\\AppData\\Local\\Temp\\checkpoint_tmp_f0a2443fd67048b5a7a743bdfef26fe1\n",
            "\u001b[2m\u001b[36m(_Trainable pid=11108)\u001b[0m 2023-11-11 19:56:17,663\tINFO trainable.py:799 -- Current state after restoring: {'_iteration': 1, '_timesteps_total': None, '_time_total': 0.44609928131103516, '_episodes_total': None}\n",
            "\u001b[2m\u001b[36m(_Trainable pid=11108)\u001b[0m 2023-11-11 19:56:18,884\tINFO trainable.py:790 -- Restored on 127.0.0.1 from checkpoint: C:\\Users\\andyw\\AppData\\Local\\Temp\\checkpoint_tmp_87f89b9424874e8c98e2efaa602f51c3\n",
            "\u001b[2m\u001b[36m(_Trainable pid=11108)\u001b[0m 2023-11-11 19:56:18,884\tINFO trainable.py:799 -- Current state after restoring: {'_iteration': 3, '_timesteps_total': None, '_time_total': 0.9886155128479004, '_episodes_total': None}\n",
            "2023-11-11 19:56:20,221\tINFO tune.py:762 -- Total run time: 25.30 seconds (24.31 seconds for the tuning loop).\n"
          ]
        }
      ],
      "source": [
        "# random search for hyperparameters\n",
        "\n",
        "from tune_sklearn import TuneSearchCV\n",
        "\n",
        "params = {\n",
        "    'max_depth': [20,30,40],\n",
        "    'min_samples_leaf' : [10,20,30],\n",
        "    'min_samples_split': [10,20,30],\n",
        "    'max_features' : [60,100,140],\n",
        "}\n",
        "\n",
        "tune_search = TuneSearchCV(\n",
        "    RandomForestClassifier(\n",
        "        n_estimators = 100,\n",
        "        random_state = 42, \n",
        "        criterion = 'gini'\n",
        "        ),\n",
        "    params,\n",
        "    scoring = 'accuracy',\n",
        "    verbose=2,\n",
        "    n_jobs = -1,\n",
        "    early_stopping=\"MedianStoppingRule\",\n",
        "    n_trials=20,\n",
        "    max_iters=10,\n",
        "    search_optimization=\"bohb\"\n",
        ")\n",
        "\n",
        "result = tune_search.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tune_search.best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and evaluate RF using the best set of hyperparameters\n",
        "\n",
        "model_rf_tuned = RandomForestClassifier(\n",
        "    max_depth=30,\n",
        "    max_features=80, \n",
        "    min_samples_leaf=15,\n",
        "    min_samples_split=20,\n",
        "    n_estimators=100,\n",
        "    random_state=42)\n",
        "\n",
        "model_rf_tuned.fit(X_train, y_train)\n",
        "\n",
        "y_pred_rf_tuned = model_rf_tuned.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base accuracy: 0.071\n",
            "Random Forest accuracy: 0.2883\n",
            "Random Forest train accuracy: 0.4724\n"
          ]
        }
      ],
      "source": [
        "acc_rf_tuned = np.mean(y_test == y_pred_rf_tuned)\n",
        "print(\"Base accuracy: 0.071\")\n",
        "print(\"Random Forest accuracy: {:.4f}\".format(acc_rf_tuned))\n",
        "y_train_pred_rf_tuned = model_rf_tuned.predict(X_train)\n",
        "acc_rf_train_tuned = np.mean(y_train_pred_rf_tuned == y_train)\n",
        "print(\"Random Forest train accuracy: {:.4f}\".format(acc_rf_train_tuned))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div class=\"tuneStatus\">\n",
              "  <div style=\"display: flex;flex-direction: row\">\n",
              "    <div style=\"display: flex;flex-direction: column;\">\n",
              "      <h3>Tune Status</h3>\n",
              "      <table>\n",
              "<tbody>\n",
              "<tr><td>Current time:</td><td>2023-11-11 19:58:38</td></tr>\n",
              "<tr><td>Running for: </td><td>00:02:16.55        </td></tr>\n",
              "<tr><td>Memory:      </td><td>15.7/63.8 GiB      </td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "    </div>\n",
              "    <div class=\"vDivider\"></div>\n",
              "    <div class=\"systemInfo\">\n",
              "      <h3>System Info</h3>\n",
              "      Using FIFO scheduling algorithm.<br>Resources requested: 1.0/24 CPUs, 1.0/1 GPUs, 0.0/29.55 GiB heap, 0.0/14.78 GiB objects (0.0/1.0 accelerator_type:G)\n",
              "    </div>\n",
              "    \n",
              "  </div>\n",
              "  <div class=\"hDivider\"></div>\n",
              "  <div class=\"trialStatus\">\n",
              "    <h3>Trial Status</h3>\n",
              "    <table>\n",
              "<thead>\n",
              "<tr><th>Trial name            </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  colsample_bytree</th><th style=\"text-align: right;\">       eta</th><th style=\"text-align: right;\">  max_depth</th><th style=\"text-align: right;\">  min_child_weight</th><th style=\"text-align: right;\">  reg_alpha</th><th style=\"text-align: right;\">  reg_lambda</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  mlogloss-mlogloss</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_data_4bb4b_00019</td><td>RUNNING   </td><td>127.0.0.1:39076</td><td style=\"text-align: right;\">          0.571496</td><td style=\"text-align: right;\">0.0333443 </td><td style=\"text-align: right;\">          6</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">   0.595501</td><td style=\"text-align: right;\">    3.12927 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                   </td></tr>\n",
              "<tr><td>train_data_4bb4b_00020</td><td>PENDING   </td><td>               </td><td style=\"text-align: right;\">          0.850485</td><td style=\"text-align: right;\">0.00139806</td><td style=\"text-align: right;\">         11</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">   3.56059 </td><td style=\"text-align: right;\">    4.12711 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                   </td></tr>\n",
              "<tr><td>train_data_4bb4b_00021</td><td>PENDING   </td><td>               </td><td style=\"text-align: right;\">          0.542419</td><td style=\"text-align: right;\">0.0940328 </td><td style=\"text-align: right;\">          9</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">   1.91615 </td><td style=\"text-align: right;\">    1.93393 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                   </td></tr>\n",
              "<tr><td>train_data_4bb4b_00022</td><td>PENDING   </td><td>               </td><td style=\"text-align: right;\">          0.973624</td><td style=\"text-align: right;\">0.0937567 </td><td style=\"text-align: right;\">          8</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">   1.94367 </td><td style=\"text-align: right;\">    3.79155 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                   </td></tr>\n",
              "<tr><td>train_data_4bb4b_00023</td><td>PENDING   </td><td>               </td><td style=\"text-align: right;\">          0.688994</td><td style=\"text-align: right;\">0.0257139 </td><td style=\"text-align: right;\">          7</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">   1.71824 </td><td style=\"text-align: right;\">    1.31875 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                   </td></tr>\n",
              "<tr><td>train_data_4bb4b_00024</td><td>PENDING   </td><td>               </td><td style=\"text-align: right;\">          0.505677</td><td style=\"text-align: right;\">0.00865608</td><td style=\"text-align: right;\">          7</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">   0.682208</td><td style=\"text-align: right;\">    0.375886</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                   </td></tr>\n",
              "<tr><td>train_data_4bb4b_00025</td><td>PENDING   </td><td>               </td><td style=\"text-align: right;\">          0.873022</td><td style=\"text-align: right;\">0.0146804 </td><td style=\"text-align: right;\">         13</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">   1.93687 </td><td style=\"text-align: right;\">    4.81465 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                   </td></tr>\n",
              "<tr><td>train_data_4bb4b_00026</td><td>PENDING   </td><td>               </td><td style=\"text-align: right;\">          0.9343  </td><td style=\"text-align: right;\">0.00280022</td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">   0.159557</td><td style=\"text-align: right;\">    4.81979 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                   </td></tr>\n",
              "<tr><td>train_data_4bb4b_00027</td><td>PENDING   </td><td>               </td><td style=\"text-align: right;\">          0.763851</td><td style=\"text-align: right;\">0.0968121 </td><td style=\"text-align: right;\">          9</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">   2.81389 </td><td style=\"text-align: right;\">    0.461603</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                   </td></tr>\n",
              "<tr><td>train_data_4bb4b_00028</td><td>PENDING   </td><td>               </td><td style=\"text-align: right;\">          0.761549</td><td style=\"text-align: right;\">0.0181467 </td><td style=\"text-align: right;\">          8</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">   2.32725 </td><td style=\"text-align: right;\">    3.50917 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                   </td></tr>\n",
              "<tr><td>train_data_4bb4b_00029</td><td>PENDING   </td><td>               </td><td style=\"text-align: right;\">          0.792157</td><td style=\"text-align: right;\">0.0634331 </td><td style=\"text-align: right;\">          9</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">   1.47672 </td><td style=\"text-align: right;\">    0.322687</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                   </td></tr>\n",
              "<tr><td>train_data_4bb4b_00030</td><td>PENDING   </td><td>               </td><td style=\"text-align: right;\">          0.727828</td><td style=\"text-align: right;\">0.0173886 </td><td style=\"text-align: right;\">         12</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">   1.02179 </td><td style=\"text-align: right;\">    1.45917 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                   </td></tr>\n",
              "<tr><td>train_data_4bb4b_00031</td><td>PENDING   </td><td>               </td><td style=\"text-align: right;\">          0.676676</td><td style=\"text-align: right;\">0.0146998 </td><td style=\"text-align: right;\">         14</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">   4.87453 </td><td style=\"text-align: right;\">    0.4809  </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                   </td></tr>\n",
              "<tr><td>train_data_4bb4b_00032</td><td>PENDING   </td><td>               </td><td style=\"text-align: right;\">          0.768048</td><td style=\"text-align: right;\">0.00415964</td><td style=\"text-align: right;\">         10</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">   3.45518 </td><td style=\"text-align: right;\">    4.0876  </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                   </td></tr>\n",
              "<tr><td>train_data_4bb4b_00033</td><td>PENDING   </td><td>               </td><td style=\"text-align: right;\">          0.970266</td><td style=\"text-align: right;\">0.00681205</td><td style=\"text-align: right;\">          7</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">   4.60397 </td><td style=\"text-align: right;\">    2.9477  </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                   </td></tr>\n",
              "<tr><td>train_data_4bb4b_00034</td><td>PENDING   </td><td>               </td><td style=\"text-align: right;\">          0.938331</td><td style=\"text-align: right;\">0.0126816 </td><td style=\"text-align: right;\">         14</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">   2.11515 </td><td style=\"text-align: right;\">    0.907688</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                   </td></tr>\n",
              "<tr><td>train_data_4bb4b_00035</td><td>PENDING   </td><td>               </td><td style=\"text-align: right;\">          0.740185</td><td style=\"text-align: right;\">0.0934484 </td><td style=\"text-align: right;\">         10</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">   3.77293 </td><td style=\"text-align: right;\">    1.94602 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                   </td></tr>\n",
              "<tr><td>train_data_4bb4b_00036</td><td>PENDING   </td><td>               </td><td style=\"text-align: right;\">          0.914582</td><td style=\"text-align: right;\">0.0137456 </td><td style=\"text-align: right;\">         13</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">   0.280427</td><td style=\"text-align: right;\">    0.411208</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                   </td></tr>\n",
              "<tr><td>train_data_4bb4b_00037</td><td>PENDING   </td><td>               </td><td style=\"text-align: right;\">          0.528188</td><td style=\"text-align: right;\">0.0536346 </td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">   4.99862 </td><td style=\"text-align: right;\">    4.08321 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                   </td></tr>\n",
              "<tr><td>train_data_4bb4b_00038</td><td>PENDING   </td><td>               </td><td style=\"text-align: right;\">          0.777716</td><td style=\"text-align: right;\">0.0345124 </td><td style=\"text-align: right;\">         10</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">   4.26327 </td><td style=\"text-align: right;\">    4.72935 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                   </td></tr>\n",
              "<tr><td>train_data_4bb4b_00039</td><td>PENDING   </td><td>               </td><td style=\"text-align: right;\">          0.725272</td><td style=\"text-align: right;\">0.00181267</td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">   3.07026 </td><td style=\"text-align: right;\">    4.77485 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                   </td></tr>\n",
              "<tr><td>train_data_4bb4b_00040</td><td>PENDING   </td><td>               </td><td style=\"text-align: right;\">          0.8785  </td><td style=\"text-align: right;\">0.00124106</td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">   0.208705</td><td style=\"text-align: right;\">    1.4165  </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                   </td></tr>\n",
              "<tr><td>train_data_4bb4b_00041</td><td>PENDING   </td><td>               </td><td style=\"text-align: right;\">          0.738105</td><td style=\"text-align: right;\">0.0459984 </td><td style=\"text-align: right;\">         14</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">   4.10029 </td><td style=\"text-align: right;\">    1.60811 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                   </td></tr>\n",
              "<tr><td>train_data_4bb4b_00042</td><td>PENDING   </td><td>               </td><td style=\"text-align: right;\">          0.938327</td><td style=\"text-align: right;\">0.00641159</td><td style=\"text-align: right;\">          9</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">   0.241035</td><td style=\"text-align: right;\">    0.756675</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                   </td></tr>\n",
              "<tr><td>train_data_4bb4b_00043</td><td>PENDING   </td><td>               </td><td style=\"text-align: right;\">          0.819681</td><td style=\"text-align: right;\">0.0407464 </td><td style=\"text-align: right;\">          9</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">   3.12459 </td><td style=\"text-align: right;\">    4.52544 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                   </td></tr>\n",
              "<tr><td>train_data_4bb4b_00044</td><td>PENDING   </td><td>               </td><td style=\"text-align: right;\">          0.804044</td><td style=\"text-align: right;\">0.0187624 </td><td style=\"text-align: right;\">          6</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">   0.54591 </td><td style=\"text-align: right;\">    2.8186  </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                   </td></tr>\n",
              "<tr><td>train_data_4bb4b_00000</td><td>TERMINATED</td><td>127.0.0.1:38292</td><td style=\"text-align: right;\">          0.86502 </td><td style=\"text-align: right;\">0.00124575</td><td style=\"text-align: right;\">          9</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">   0.877368</td><td style=\"text-align: right;\">    2.87358 </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         4.96598</td><td style=\"text-align: right;\">            2.59101</td></tr>\n",
              "<tr><td>train_data_4bb4b_00001</td><td>TERMINATED</td><td>127.0.0.1:29628</td><td style=\"text-align: right;\">          0.67094 </td><td style=\"text-align: right;\">0.00152615</td><td style=\"text-align: right;\">         10</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">   1.62593 </td><td style=\"text-align: right;\">    0.561369</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         5.49375</td><td style=\"text-align: right;\">            2.58419</td></tr>\n",
              "<tr><td>train_data_4bb4b_00002</td><td>TERMINATED</td><td>127.0.0.1:13696</td><td style=\"text-align: right;\">          0.737087</td><td style=\"text-align: right;\">0.00156916</td><td style=\"text-align: right;\">          8</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">   2.42001 </td><td style=\"text-align: right;\">    2.50892 </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         4.54388</td><td style=\"text-align: right;\">            2.58413</td></tr>\n",
              "<tr><td>train_data_4bb4b_00003</td><td>TERMINATED</td><td>127.0.0.1:21268</td><td style=\"text-align: right;\">          0.716926</td><td style=\"text-align: right;\">0.00626628</td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">   3.21196 </td><td style=\"text-align: right;\">    3.11767 </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         2.85425</td><td style=\"text-align: right;\">            2.4568 </td></tr>\n",
              "<tr><td>train_data_4bb4b_00004</td><td>TERMINATED</td><td>127.0.0.1:20488</td><td style=\"text-align: right;\">          0.687306</td><td style=\"text-align: right;\">0.0178534 </td><td style=\"text-align: right;\">          6</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">   4.2968  </td><td style=\"text-align: right;\">    2.56537 </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         3.36206</td><td style=\"text-align: right;\">            2.32995</td></tr>\n",
              "<tr><td>train_data_4bb4b_00005</td><td>TERMINATED</td><td>127.0.0.1:36208</td><td style=\"text-align: right;\">          0.581467</td><td style=\"text-align: right;\">0.001384  </td><td style=\"text-align: right;\">          9</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">   0.229905</td><td style=\"text-align: right;\">    3.24785 </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         5.05052</td><td style=\"text-align: right;\">            2.59469</td></tr>\n",
              "<tr><td>train_data_4bb4b_00006</td><td>TERMINATED</td><td>127.0.0.1:17340</td><td style=\"text-align: right;\">          0.838584</td><td style=\"text-align: right;\">0.0140196 </td><td style=\"text-align: right;\">          8</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">   4.0749  </td><td style=\"text-align: right;\">    0.729652</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         4.16337</td><td style=\"text-align: right;\">            2.35352</td></tr>\n",
              "<tr><td>train_data_4bb4b_00007</td><td>TERMINATED</td><td>127.0.0.1:39772</td><td style=\"text-align: right;\">          0.693051</td><td style=\"text-align: right;\">0.0836337 </td><td style=\"text-align: right;\">          9</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">   1.05938 </td><td style=\"text-align: right;\">    4.53622 </td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         2.38452</td><td style=\"text-align: right;\">            2.27743</td></tr>\n",
              "<tr><td>train_data_4bb4b_00008</td><td>TERMINATED</td><td>127.0.0.1:23276</td><td style=\"text-align: right;\">          0.547221</td><td style=\"text-align: right;\">0.0232281 </td><td style=\"text-align: right;\">          6</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">   1.66298 </td><td style=\"text-align: right;\">    0.448824</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         3.51616</td><td style=\"text-align: right;\">            2.30197</td></tr>\n",
              "<tr><td>train_data_4bb4b_00009</td><td>TERMINATED</td><td>127.0.0.1:32028</td><td style=\"text-align: right;\">          0.511636</td><td style=\"text-align: right;\">0.0425537 </td><td style=\"text-align: right;\">         14</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">   0.679008</td><td style=\"text-align: right;\">    1.48109 </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         6.05185</td><td style=\"text-align: right;\">            2.29272</td></tr>\n",
              "<tr><td>train_data_4bb4b_00010</td><td>TERMINATED</td><td>127.0.0.1:9220 </td><td style=\"text-align: right;\">          0.814471</td><td style=\"text-align: right;\">0.056878  </td><td style=\"text-align: right;\">          9</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">   4.03706 </td><td style=\"text-align: right;\">    3.70185 </td><td style=\"text-align: right;\">    91</td><td style=\"text-align: right;\">         4.37687</td><td style=\"text-align: right;\">            2.28928</td></tr>\n",
              "<tr><td>train_data_4bb4b_00011</td><td>TERMINATED</td><td>127.0.0.1:16668</td><td style=\"text-align: right;\">          0.58872 </td><td style=\"text-align: right;\">0.0317124 </td><td style=\"text-align: right;\">         13</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">   4.95348 </td><td style=\"text-align: right;\">    4.05349 </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         4.54739</td><td style=\"text-align: right;\">            2.29153</td></tr>\n",
              "<tr><td>train_data_4bb4b_00012</td><td>TERMINATED</td><td>127.0.0.1:39720</td><td style=\"text-align: right;\">          0.625008</td><td style=\"text-align: right;\">0.0144907 </td><td style=\"text-align: right;\">          8</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">   2.85315 </td><td style=\"text-align: right;\">    4.34887 </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         4.37435</td><td style=\"text-align: right;\">            2.38328</td></tr>\n",
              "<tr><td>train_data_4bb4b_00013</td><td>TERMINATED</td><td>127.0.0.1:9940 </td><td style=\"text-align: right;\">          0.875436</td><td style=\"text-align: right;\">0.0322913 </td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">   4.52251 </td><td style=\"text-align: right;\">    0.605307</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         2.90003</td><td style=\"text-align: right;\">            2.26942</td></tr>\n",
              "<tr><td>train_data_4bb4b_00014</td><td>TERMINATED</td><td>127.0.0.1:8544 </td><td style=\"text-align: right;\">          0.694809</td><td style=\"text-align: right;\">0.00393795</td><td style=\"text-align: right;\">          8</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">   0.362078</td><td style=\"text-align: right;\">    0.589926</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         4.14305</td><td style=\"text-align: right;\">            2.49219</td></tr>\n",
              "<tr><td>train_data_4bb4b_00015</td><td>TERMINATED</td><td>127.0.0.1:37812</td><td style=\"text-align: right;\">          0.545643</td><td style=\"text-align: right;\">0.00435138</td><td style=\"text-align: right;\">         13</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">   4.75798 </td><td style=\"text-align: right;\">    4.7553  </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         4.65777</td><td style=\"text-align: right;\">            2.52553</td></tr>\n",
              "<tr><td>train_data_4bb4b_00016</td><td>TERMINATED</td><td>127.0.0.1:38068</td><td style=\"text-align: right;\">          0.815919</td><td style=\"text-align: right;\">0.00788662</td><td style=\"text-align: right;\">          7</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">   1.71046 </td><td style=\"text-align: right;\">    1.53673 </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         4.0676 </td><td style=\"text-align: right;\">            2.42998</td></tr>\n",
              "<tr><td>train_data_4bb4b_00017</td><td>TERMINATED</td><td>127.0.0.1:37088</td><td style=\"text-align: right;\">          0.973097</td><td style=\"text-align: right;\">0.0365252 </td><td style=\"text-align: right;\">          9</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">   4.66155 </td><td style=\"text-align: right;\">    0.655977</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         4.53368</td><td style=\"text-align: right;\">            2.27553</td></tr>\n",
              "<tr><td>train_data_4bb4b_00018</td><td>TERMINATED</td><td>127.0.0.1:32428</td><td style=\"text-align: right;\">          0.997966</td><td style=\"text-align: right;\">0.00129343</td><td style=\"text-align: right;\">         14</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">   2.77499 </td><td style=\"text-align: right;\">    3.71147 </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         5.05123</td><td style=\"text-align: right;\">            2.59137</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "  </div>\n",
              "</div>\n",
              "<style>\n",
              ".tuneStatus {\n",
              "  color: var(--jp-ui-font-color1);\n",
              "}\n",
              ".tuneStatus .systemInfo {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              ".tuneStatus td {\n",
              "  white-space: nowrap;\n",
              "}\n",
              ".tuneStatus .trialStatus {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              ".tuneStatus h3 {\n",
              "  font-weight: bold;\n",
              "}\n",
              ".tuneStatus .hDivider {\n",
              "  border-bottom-width: var(--jp-border-width);\n",
              "  border-bottom-color: var(--jp-border-color0);\n",
              "  border-bottom-style: solid;\n",
              "}\n",
              ".tuneStatus .vDivider {\n",
              "  border-left-width: var(--jp-border-width);\n",
              "  border-left-color: var(--jp-border-color0);\n",
              "  border-left-style: solid;\n",
              "  margin: 0.5em 1em 0.5em 1em;\n",
              "}\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div class=\"trialProgress\">\n",
              "  <h3>Trial Progress</h3>\n",
              "  <table>\n",
              "<thead>\n",
              "<tr><th>Trial name            </th><th>date               </th><th>done  </th><th>episodes_total  </th><th>experiment_id                   </th><th>experiment_tag                                                                                          </th><th>hostname  </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">  mlogloss-mlogloss</th><th>node_ip  </th><th style=\"text-align: right;\">  pid</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th>timesteps_total  </th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th><th style=\"text-align: right;\">  warmup_time</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_data_4bb4b_00000</td><td>2023-11-11_19-56-28</td><td>True  </td><td>                </td><td>40b3bb3ff34f456090b92488e608e34d</td><td>0_colsample_bytree=0.8650,eta=0.0012,max_depth=9,min_child_weight=2,reg_alpha=0.8774,reg_lambda=2.8736  </td><td>WDesktop  </td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">            2.59101</td><td>127.0.0.1</td><td style=\"text-align: right;\">38292</td><td style=\"text-align: right;\">             4.96598</td><td style=\"text-align: right;\">         0.0472159</td><td style=\"text-align: right;\">       4.96598</td><td style=\"text-align: right;\"> 1699750588</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                 100</td><td>4bb4b_00000</td><td style=\"text-align: right;\">   0.00299978</td></tr>\n",
              "<tr><td>train_data_4bb4b_00001</td><td>2023-11-11_19-56-37</td><td>True  </td><td>                </td><td>7655aeed66e348928569fedbd354dea0</td><td>1_colsample_bytree=0.6709,eta=0.0015,max_depth=10,min_child_weight=1,reg_alpha=1.6259,reg_lambda=0.5614 </td><td>WDesktop  </td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">            2.58419</td><td>127.0.0.1</td><td style=\"text-align: right;\">29628</td><td style=\"text-align: right;\">             5.49375</td><td style=\"text-align: right;\">         0.05     </td><td style=\"text-align: right;\">       5.49375</td><td style=\"text-align: right;\"> 1699750597</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                 100</td><td>4bb4b_00001</td><td style=\"text-align: right;\">   0.00199986</td></tr>\n",
              "<tr><td>train_data_4bb4b_00002</td><td>2023-11-11_19-56-44</td><td>True  </td><td>                </td><td>bc63d37c3ae94b8e84c0a07b07a85c30</td><td>2_colsample_bytree=0.7371,eta=0.0016,max_depth=8,min_child_weight=1,reg_alpha=2.4200,reg_lambda=2.5089  </td><td>WDesktop  </td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">            2.58413</td><td>127.0.0.1</td><td style=\"text-align: right;\">13696</td><td style=\"text-align: right;\">             4.54388</td><td style=\"text-align: right;\">         0.0440032</td><td style=\"text-align: right;\">       4.54388</td><td style=\"text-align: right;\"> 1699750604</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                 100</td><td>4bb4b_00002</td><td style=\"text-align: right;\">   0.00300002</td></tr>\n",
              "<tr><td>train_data_4bb4b_00003</td><td>2023-11-11_19-56-49</td><td>True  </td><td>                </td><td>c66053bc703d4bc2a7421e9fbdb3b7f1</td><td>3_colsample_bytree=0.7169,eta=0.0063,max_depth=5,min_child_weight=2,reg_alpha=3.2120,reg_lambda=3.1177  </td><td>WDesktop  </td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">            2.4568 </td><td>127.0.0.1</td><td style=\"text-align: right;\">21268</td><td style=\"text-align: right;\">             2.85425</td><td style=\"text-align: right;\">         0.0250032</td><td style=\"text-align: right;\">       2.85425</td><td style=\"text-align: right;\"> 1699750609</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                 100</td><td>4bb4b_00003</td><td style=\"text-align: right;\">   0.00299954</td></tr>\n",
              "<tr><td>train_data_4bb4b_00004</td><td>2023-11-11_19-56-56</td><td>True  </td><td>                </td><td>bdb5952d8d884af08e875ee140ed7cc3</td><td>4_colsample_bytree=0.6873,eta=0.0179,max_depth=6,min_child_weight=4,reg_alpha=4.2968,reg_lambda=2.5654  </td><td>WDesktop  </td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">            2.32995</td><td>127.0.0.1</td><td style=\"text-align: right;\">20488</td><td style=\"text-align: right;\">             3.36206</td><td style=\"text-align: right;\">         0.0311198</td><td style=\"text-align: right;\">       3.36206</td><td style=\"text-align: right;\"> 1699750616</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                 100</td><td>4bb4b_00004</td><td style=\"text-align: right;\">   0.00200081</td></tr>\n",
              "<tr><td>train_data_4bb4b_00005</td><td>2023-11-11_19-57-04</td><td>True  </td><td>                </td><td>bdbcc73da5f940ff9d64fcd393e69da7</td><td>5_colsample_bytree=0.5815,eta=0.0014,max_depth=9,min_child_weight=1,reg_alpha=0.2299,reg_lambda=3.2479  </td><td>WDesktop  </td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">            2.59469</td><td>127.0.0.1</td><td style=\"text-align: right;\">36208</td><td style=\"text-align: right;\">             5.05052</td><td style=\"text-align: right;\">         0.053    </td><td style=\"text-align: right;\">       5.05052</td><td style=\"text-align: right;\"> 1699750624</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                 100</td><td>4bb4b_00005</td><td style=\"text-align: right;\">   0.00300002</td></tr>\n",
              "<tr><td>train_data_4bb4b_00006</td><td>2023-11-11_19-57-11</td><td>True  </td><td>                </td><td>4d1fb40333a44100b87b9d5d6ef0d9c9</td><td>6_colsample_bytree=0.8386,eta=0.0140,max_depth=8,min_child_weight=4,reg_alpha=4.0749,reg_lambda=0.7297  </td><td>WDesktop  </td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">            2.35352</td><td>127.0.0.1</td><td style=\"text-align: right;\">17340</td><td style=\"text-align: right;\">             4.16337</td><td style=\"text-align: right;\">         0.0390017</td><td style=\"text-align: right;\">       4.16337</td><td style=\"text-align: right;\"> 1699750631</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                 100</td><td>4bb4b_00006</td><td style=\"text-align: right;\">   0.00199962</td></tr>\n",
              "<tr><td>train_data_4bb4b_00007</td><td>2023-11-11_19-57-16</td><td>True  </td><td>                </td><td>999fb960e89e41cdba842993f6b0e291</td><td>7_colsample_bytree=0.6931,eta=0.0836,max_depth=9,min_child_weight=4,reg_alpha=1.0594,reg_lambda=4.5362  </td><td>WDesktop  </td><td style=\"text-align: right;\">                        49</td><td style=\"text-align: right;\">            2.27743</td><td>127.0.0.1</td><td style=\"text-align: right;\">39772</td><td style=\"text-align: right;\">             2.38452</td><td style=\"text-align: right;\">         0.0440099</td><td style=\"text-align: right;\">       2.38452</td><td style=\"text-align: right;\"> 1699750636</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  49</td><td>4bb4b_00007</td><td style=\"text-align: right;\">   0.00199986</td></tr>\n",
              "<tr><td>train_data_4bb4b_00008</td><td>2023-11-11_19-57-22</td><td>True  </td><td>                </td><td>23e58d6910a7401ba7e27f43ca4376a3</td><td>8_colsample_bytree=0.5472,eta=0.0232,max_depth=6,min_child_weight=1,reg_alpha=1.6630,reg_lambda=0.4488  </td><td>WDesktop  </td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">            2.30197</td><td>127.0.0.1</td><td style=\"text-align: right;\">23276</td><td style=\"text-align: right;\">             3.51616</td><td style=\"text-align: right;\">         0.033    </td><td style=\"text-align: right;\">       3.51616</td><td style=\"text-align: right;\"> 1699750642</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                 100</td><td>4bb4b_00008</td><td style=\"text-align: right;\">   0.00299954</td></tr>\n",
              "<tr><td>train_data_4bb4b_00009</td><td>2023-11-11_19-57-31</td><td>True  </td><td>                </td><td>59d2cfe99c744214844999d461ef4598</td><td>9_colsample_bytree=0.5116,eta=0.0426,max_depth=14,min_child_weight=2,reg_alpha=0.6790,reg_lambda=1.4811 </td><td>WDesktop  </td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">            2.29272</td><td>127.0.0.1</td><td style=\"text-align: right;\">32028</td><td style=\"text-align: right;\">             6.05185</td><td style=\"text-align: right;\">         0.0579998</td><td style=\"text-align: right;\">       6.05185</td><td style=\"text-align: right;\"> 1699750651</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                 100</td><td>4bb4b_00009</td><td style=\"text-align: right;\">   0.00300121</td></tr>\n",
              "<tr><td>train_data_4bb4b_00010</td><td>2023-11-11_19-57-38</td><td>True  </td><td>                </td><td>fd384bf172334d4eaab727a9bb75a40a</td><td>10_colsample_bytree=0.8145,eta=0.0569,max_depth=9,min_child_weight=2,reg_alpha=4.0371,reg_lambda=3.7018 </td><td>WDesktop  </td><td style=\"text-align: right;\">                        91</td><td style=\"text-align: right;\">            2.28928</td><td>127.0.0.1</td><td style=\"text-align: right;\"> 9220</td><td style=\"text-align: right;\">             4.37687</td><td style=\"text-align: right;\">         0.0450096</td><td style=\"text-align: right;\">       4.37687</td><td style=\"text-align: right;\"> 1699750658</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  91</td><td>4bb4b_00010</td><td style=\"text-align: right;\">   0.00200129</td></tr>\n",
              "<tr><td>train_data_4bb4b_00011</td><td>2023-11-11_19-57-46</td><td>True  </td><td>                </td><td>0947ada7ea2f4d949c60b8d23782ff42</td><td>11_colsample_bytree=0.5887,eta=0.0317,max_depth=13,min_child_weight=4,reg_alpha=4.9535,reg_lambda=4.0535</td><td>WDesktop  </td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">            2.29153</td><td>127.0.0.1</td><td style=\"text-align: right;\">16668</td><td style=\"text-align: right;\">             4.54739</td><td style=\"text-align: right;\">         0.0430002</td><td style=\"text-align: right;\">       4.54739</td><td style=\"text-align: right;\"> 1699750666</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                 100</td><td>4bb4b_00011</td><td style=\"text-align: right;\">   0.00200033</td></tr>\n",
              "<tr><td>train_data_4bb4b_00012</td><td>2023-11-11_19-57-52</td><td>True  </td><td>                </td><td>832e14d762f54f19a07c82021797a7e7</td><td>12_colsample_bytree=0.6250,eta=0.0145,max_depth=8,min_child_weight=2,reg_alpha=2.8531,reg_lambda=4.3489 </td><td>WDesktop  </td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">            2.38328</td><td>127.0.0.1</td><td style=\"text-align: right;\">39720</td><td style=\"text-align: right;\">             4.37435</td><td style=\"text-align: right;\">         0.0409398</td><td style=\"text-align: right;\">       4.37435</td><td style=\"text-align: right;\"> 1699750672</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                 100</td><td>4bb4b_00012</td><td style=\"text-align: right;\">   0.00200033</td></tr>\n",
              "<tr><td>train_data_4bb4b_00013</td><td>2023-11-11_19-57-58</td><td>True  </td><td>                </td><td>e7fc2280c10349ff9c1e51cc3cdf2fac</td><td>13_colsample_bytree=0.8754,eta=0.0323,max_depth=5,min_child_weight=3,reg_alpha=4.5225,reg_lambda=0.6053 </td><td>WDesktop  </td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">            2.26942</td><td>127.0.0.1</td><td style=\"text-align: right;\"> 9940</td><td style=\"text-align: right;\">             2.90003</td><td style=\"text-align: right;\">         0.0255089</td><td style=\"text-align: right;\">       2.90003</td><td style=\"text-align: right;\"> 1699750678</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                 100</td><td>4bb4b_00013</td><td style=\"text-align: right;\">   0.00300145</td></tr>\n",
              "<tr><td>train_data_4bb4b_00014</td><td>2023-11-11_19-58-05</td><td>True  </td><td>                </td><td>cdef28325a4e4f87b8d7dea8a7b4226e</td><td>14_colsample_bytree=0.6948,eta=0.0039,max_depth=8,min_child_weight=4,reg_alpha=0.3621,reg_lambda=0.5899 </td><td>WDesktop  </td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">            2.49219</td><td>127.0.0.1</td><td style=\"text-align: right;\"> 8544</td><td style=\"text-align: right;\">             4.14305</td><td style=\"text-align: right;\">         0.0385127</td><td style=\"text-align: right;\">       4.14305</td><td style=\"text-align: right;\"> 1699750685</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                 100</td><td>4bb4b_00014</td><td style=\"text-align: right;\">   0.0030005 </td></tr>\n",
              "<tr><td>train_data_4bb4b_00015</td><td>2023-11-11_19-58-13</td><td>True  </td><td>                </td><td>580891dcc99d40bfa23e85de76d97469</td><td>15_colsample_bytree=0.5456,eta=0.0044,max_depth=13,min_child_weight=3,reg_alpha=4.7580,reg_lambda=4.7553</td><td>WDesktop  </td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">            2.52553</td><td>127.0.0.1</td><td style=\"text-align: right;\">37812</td><td style=\"text-align: right;\">             4.65777</td><td style=\"text-align: right;\">         0.0470004</td><td style=\"text-align: right;\">       4.65777</td><td style=\"text-align: right;\"> 1699750693</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                 100</td><td>4bb4b_00015</td><td style=\"text-align: right;\">   0.00200033</td></tr>\n",
              "<tr><td>train_data_4bb4b_00016</td><td>2023-11-11_19-58-19</td><td>True  </td><td>                </td><td>31126e95ff4f4bd8a7d0fbfe3fc84d19</td><td>16_colsample_bytree=0.8159,eta=0.0079,max_depth=7,min_child_weight=2,reg_alpha=1.7105,reg_lambda=1.5367 </td><td>WDesktop  </td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">            2.42998</td><td>127.0.0.1</td><td style=\"text-align: right;\">38068</td><td style=\"text-align: right;\">             4.0676 </td><td style=\"text-align: right;\">         0.039    </td><td style=\"text-align: right;\">       4.0676 </td><td style=\"text-align: right;\"> 1699750699</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                 100</td><td>4bb4b_00016</td><td style=\"text-align: right;\">   0.00300431</td></tr>\n",
              "<tr><td>train_data_4bb4b_00017</td><td>2023-11-11_19-58-27</td><td>True  </td><td>                </td><td>472e77a5621e4255b773e17d64872ff2</td><td>17_colsample_bytree=0.9731,eta=0.0365,max_depth=9,min_child_weight=4,reg_alpha=4.6616,reg_lambda=0.6560 </td><td>WDesktop  </td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">            2.27553</td><td>127.0.0.1</td><td style=\"text-align: right;\">37088</td><td style=\"text-align: right;\">             4.53368</td><td style=\"text-align: right;\">         0.0405123</td><td style=\"text-align: right;\">       4.53368</td><td style=\"text-align: right;\"> 1699750707</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                 100</td><td>4bb4b_00017</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_4bb4b_00018</td><td>2023-11-11_19-58-34</td><td>True  </td><td>                </td><td>20d812bef7c24515b47aa1f56e1d7d92</td><td>18_colsample_bytree=0.9980,eta=0.0013,max_depth=14,min_child_weight=3,reg_alpha=2.7750,reg_lambda=3.7115</td><td>WDesktop  </td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">            2.59137</td><td>127.0.0.1</td><td style=\"text-align: right;\">32428</td><td style=\"text-align: right;\">             5.05123</td><td style=\"text-align: right;\">         0.0495129</td><td style=\"text-align: right;\">       5.05123</td><td style=\"text-align: right;\"> 1699750714</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                 100</td><td>4bb4b_00018</td><td style=\"text-align: right;\">   0.00300097</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</div>\n",
              "<style>\n",
              ".trialProgress {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  color: var(--jp-ui-font-color1);\n",
              "}\n",
              ".trialProgress h3 {\n",
              "  font-weight: bold;\n",
              "}\n",
              ".trialProgress td {\n",
              "  white-space: nowrap;\n",
              "}\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-11-11 19:58:36,788\tWARNING tune.py:690 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
            "2023-11-11 19:58:38,552\tERROR tune.py:758 -- Trials did not complete: [train_data_4bb4b_00019, train_data_4bb4b_00020, train_data_4bb4b_00021, train_data_4bb4b_00022, train_data_4bb4b_00023, train_data_4bb4b_00024, train_data_4bb4b_00025, train_data_4bb4b_00026, train_data_4bb4b_00027, train_data_4bb4b_00028, train_data_4bb4b_00029, train_data_4bb4b_00030, train_data_4bb4b_00031, train_data_4bb4b_00032, train_data_4bb4b_00033, train_data_4bb4b_00034, train_data_4bb4b_00035, train_data_4bb4b_00036, train_data_4bb4b_00037, train_data_4bb4b_00038, train_data_4bb4b_00039, train_data_4bb4b_00040, train_data_4bb4b_00041, train_data_4bb4b_00042, train_data_4bb4b_00043, train_data_4bb4b_00044]\n",
            "2023-11-11 19:58:38,552\tINFO tune.py:762 -- Total run time: 137.00 seconds (136.54 seconds for the tuning loop).\n",
            "2023-11-11 19:58:38,553\tWARNING tune.py:768 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n"
          ]
        }
      ],
      "source": [
        "# random search for hyperparameters\n",
        "# additional function to support xgboost \n",
        "\n",
        "from ray import tune\n",
        "from ray.tune.integration.xgboost import TuneReportCheckpointCallback\n",
        "\n",
        "def data_loader():\n",
        "    return (X_train,y_train), (X_test,y_test)\n",
        "\n",
        "def train_data(config,data):\n",
        "    t1, t2 = data\n",
        "    train_set = xgb.DMatrix(t1[0], label = t1[1])\n",
        "    val_set = xgb.DMatrix(t2[0], label = t2[1])\n",
        "    results = {}\n",
        "    bst = xgb.train(\n",
        "        config,\n",
        "        train_set,\n",
        "        num_boost_round = 100,\n",
        "        evals = [(val_set, 'mlogloss')],\n",
        "        evals_result = results,\n",
        "        verbose_eval = False,\n",
        "        callbacks=[TuneReportCheckpointCallback(filename=\"model.xgb\")],\n",
        "        early_stopping_rounds=5,\n",
        "    )\n",
        "\n",
        "config = {\n",
        "          \"objective\": \"multi:softprob\",\n",
        "          \"tree_method\": \"gpu_hist\",\n",
        "          \"eval_metric\": [\"mlogloss\"],\n",
        "          \"max_depth\": tune.randint(5,15),\n",
        "          \"min_child_weight\": tune.randint(1,5),\n",
        "          \"colsample_bytree\": tune.uniform(0.5, 1.0),\n",
        "          \"eta\": tune.loguniform(1e-3, 1e-1),\n",
        "          \"reg_lambda\": tune.uniform(0.1, 5),\n",
        "          \"reg_alpha\": tune.uniform(0.1, 5),\n",
        "          \"num_class\": 14,\n",
        "          \"seed\": 42\n",
        "}\n",
        "t1,t2 = data_loader()\n",
        "\n",
        "analysis = tune.run(\n",
        "    tune.with_parameters(train_data, data = (t1,t2)),\n",
        "    resources_per_trial = {\"gpu\":1},\n",
        "    config = config,\n",
        "    num_samples = 50,\n",
        "    metric='mlogloss-mlogloss',\n",
        "    mode=\"max\",\n",
        "    stop={\n",
        "        \"training_iteration\": 500\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'mlogloss-mlogloss': 2.257413484760233,\n",
              " 'time_this_iter_s': 0.02700018882751465,\n",
              " 'done': True,\n",
              " 'timesteps_total': None,\n",
              " 'episodes_total': None,\n",
              " 'training_iteration': 100,\n",
              " 'trial_id': '53c4e_00037',\n",
              " 'experiment_id': 'e36a056782d14317ac83031dbc0f4ef0',\n",
              " 'date': '2023-11-11_17-59-07',\n",
              " 'timestamp': 1699743547,\n",
              " 'time_total_s': 2.8465235233306885,\n",
              " 'pid': 29084,\n",
              " 'hostname': 'WDesktop',\n",
              " 'node_ip': '127.0.0.1',\n",
              " 'config': {'objective': 'multi:softprob',\n",
              "  'tree_method': 'gpu_hist',\n",
              "  'eval_metric': ['mlogloss'],\n",
              "  'max_depth': 5,\n",
              "  'min_child_weight': 1,\n",
              "  'colsample_bytree': 0.5281877483254636,\n",
              "  'eta': 0.05363456374740404,\n",
              "  'reg_lambda': 4.083214944737381,\n",
              "  'reg_alpha': 4.99861659910204,\n",
              "  'num_class': 14,\n",
              "  'seed': 42},\n",
              " 'time_since_restore': 2.8465235233306885,\n",
              " 'timesteps_since_restore': 0,\n",
              " 'iterations_since_restore': 100,\n",
              " 'warmup_time': 0.0019998550415039062,\n",
              " 'experiment_tag': '37_colsample_bytree=0.5282,eta=0.0536,max_depth=5,min_child_weight=1,reg_alpha=4.9986,reg_lambda=4.0832'}"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "analysis.get_best_trial(metric = 'mlogloss-mlogloss', mode = 'min').last_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0]\teval-mlogloss:2.61659\n",
            "[1]\teval-mlogloss:2.59550\n",
            "[2]\teval-mlogloss:2.57767\n",
            "[3]\teval-mlogloss:2.56522\n",
            "[4]\teval-mlogloss:2.54924\n",
            "[5]\teval-mlogloss:2.53673\n",
            "[6]\teval-mlogloss:2.52453\n",
            "[7]\teval-mlogloss:2.51401\n",
            "[8]\teval-mlogloss:2.50283\n",
            "[9]\teval-mlogloss:2.49195\n",
            "[10]\teval-mlogloss:2.47925\n",
            "[11]\teval-mlogloss:2.46801\n",
            "[12]\teval-mlogloss:2.45908\n",
            "[13]\teval-mlogloss:2.44953\n",
            "[14]\teval-mlogloss:2.44293\n",
            "[15]\teval-mlogloss:2.43521\n",
            "[16]\teval-mlogloss:2.42767\n",
            "[17]\teval-mlogloss:2.42079\n",
            "[18]\teval-mlogloss:2.41502\n",
            "[19]\teval-mlogloss:2.40690\n",
            "[20]\teval-mlogloss:2.39817\n",
            "[21]\teval-mlogloss:2.39146\n",
            "[22]\teval-mlogloss:2.38713\n",
            "[23]\teval-mlogloss:2.38085\n",
            "[24]\teval-mlogloss:2.37612\n",
            "[25]\teval-mlogloss:2.37111\n",
            "[26]\teval-mlogloss:2.36523\n",
            "[27]\teval-mlogloss:2.36168\n",
            "[28]\teval-mlogloss:2.35825\n",
            "[29]\teval-mlogloss:2.35295\n",
            "[30]\teval-mlogloss:2.34763\n",
            "[31]\teval-mlogloss:2.34398\n",
            "[32]\teval-mlogloss:2.34148\n",
            "[33]\teval-mlogloss:2.33760\n",
            "[34]\teval-mlogloss:2.33563\n",
            "[35]\teval-mlogloss:2.33214\n",
            "[36]\teval-mlogloss:2.32851\n",
            "[37]\teval-mlogloss:2.32641\n",
            "[38]\teval-mlogloss:2.32199\n",
            "[39]\teval-mlogloss:2.32024\n",
            "[40]\teval-mlogloss:2.31660\n",
            "[41]\teval-mlogloss:2.31393\n",
            "[42]\teval-mlogloss:2.31156\n",
            "[43]\teval-mlogloss:2.30910\n",
            "[44]\teval-mlogloss:2.30790\n",
            "[45]\teval-mlogloss:2.30485\n",
            "[46]\teval-mlogloss:2.30210\n",
            "[47]\teval-mlogloss:2.30079\n",
            "[48]\teval-mlogloss:2.29984\n",
            "[49]\teval-mlogloss:2.29747\n",
            "[50]\teval-mlogloss:2.29512\n",
            "[51]\teval-mlogloss:2.29446\n",
            "[52]\teval-mlogloss:2.29211\n",
            "[53]\teval-mlogloss:2.29060\n",
            "[54]\teval-mlogloss:2.28890\n",
            "[55]\teval-mlogloss:2.28759\n",
            "[56]\teval-mlogloss:2.28576\n",
            "[57]\teval-mlogloss:2.28414\n",
            "[58]\teval-mlogloss:2.28316\n",
            "[59]\teval-mlogloss:2.28133\n",
            "[60]\teval-mlogloss:2.28016\n",
            "[61]\teval-mlogloss:2.27968\n",
            "[62]\teval-mlogloss:2.27883\n",
            "[63]\teval-mlogloss:2.27726\n",
            "[64]\teval-mlogloss:2.27532\n",
            "[65]\teval-mlogloss:2.27355\n",
            "[66]\teval-mlogloss:2.27294\n",
            "[67]\teval-mlogloss:2.27180\n",
            "[68]\teval-mlogloss:2.27099\n",
            "[69]\teval-mlogloss:2.27117\n",
            "[70]\teval-mlogloss:2.26998\n",
            "[71]\teval-mlogloss:2.26845\n",
            "[72]\teval-mlogloss:2.26702\n",
            "[73]\teval-mlogloss:2.26675\n",
            "[74]\teval-mlogloss:2.26605\n",
            "[75]\teval-mlogloss:2.26573\n",
            "[76]\teval-mlogloss:2.26565\n",
            "[77]\teval-mlogloss:2.26534\n",
            "[78]\teval-mlogloss:2.26482\n",
            "[79]\teval-mlogloss:2.26444\n",
            "[80]\teval-mlogloss:2.26372\n",
            "[81]\teval-mlogloss:2.26259\n",
            "[82]\teval-mlogloss:2.26195\n",
            "[83]\teval-mlogloss:2.26148\n",
            "[84]\teval-mlogloss:2.26030\n",
            "[85]\teval-mlogloss:2.26078\n",
            "[86]\teval-mlogloss:2.26003\n",
            "[87]\teval-mlogloss:2.25915\n",
            "[88]\teval-mlogloss:2.25891\n",
            "[89]\teval-mlogloss:2.25886\n",
            "[90]\teval-mlogloss:2.25920\n",
            "[91]\teval-mlogloss:2.25886\n",
            "[92]\teval-mlogloss:2.25940\n",
            "[93]\teval-mlogloss:2.25921\n",
            "[94]\teval-mlogloss:2.25941\n",
            "[95]\teval-mlogloss:2.25866\n",
            "[96]\teval-mlogloss:2.25772\n",
            "[97]\teval-mlogloss:2.25791\n",
            "[98]\teval-mlogloss:2.25745\n",
            "[99]\teval-mlogloss:2.25741\n",
            "[100]\teval-mlogloss:2.25735\n",
            "[101]\teval-mlogloss:2.25728\n",
            "[102]\teval-mlogloss:2.25669\n",
            "[103]\teval-mlogloss:2.25634\n",
            "[104]\teval-mlogloss:2.25693\n",
            "[105]\teval-mlogloss:2.25661\n",
            "[106]\teval-mlogloss:2.25608\n",
            "[107]\teval-mlogloss:2.25645\n",
            "[108]\teval-mlogloss:2.25750\n",
            "[109]\teval-mlogloss:2.25752\n",
            "[110]\teval-mlogloss:2.25821\n",
            "[111]\teval-mlogloss:2.25915\n"
          ]
        }
      ],
      "source": [
        "# Train and evaluate xgboost using the best set of hyperparameters\n",
        "\n",
        "config = {'objective': 'multi:softprob',\n",
        "  'tree_method': 'gpu_hist',\n",
        "  'eval_metric': ['mlogloss'],\n",
        "  'max_depth': 5,\n",
        "  'min_child_weight': 1,\n",
        "  'colsample_bytree': 0.5281877483254636,\n",
        "  'eta': 0.05363456374740404,\n",
        "  'reg_lambda': 4.083214944737381,\n",
        "  'reg_alpha': 4.99861659910204,\n",
        "  'num_class': 14,\n",
        "  'seed': 42}\n",
        "\n",
        "train_set = xgb.DMatrix(X_train,y_train)\n",
        "val_set = xgb.DMatrix(X_test,y_test)\n",
        "results = {}\n",
        "\n",
        "model_xgb = xgb.train(\n",
        "    config,\n",
        "    dtrain = train_set,\n",
        "    num_boost_round = 500,\n",
        "    evals = [(val_set, 'eval')],\n",
        "    early_stopping_rounds=5,\n",
        ")\n",
        "y_pred_xgb = model_xgb.predict(xgb.DMatrix(X_test)).argmax(axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base accuracy: 0.071\n",
            "X Gradient Boost accuracy: 0.2455\n",
            "X Gradient Boost train accuracy: 0.7523\n"
          ]
        }
      ],
      "source": [
        "acc_xgb = np.mean(y_test == y_pred_xgb)\n",
        "print(\"Base accuracy: 0.071\")\n",
        "print(\"X Gradient Boost accuracy: {:.4f}\".format(acc_xgb))\n",
        "y_train_pred_xgb = model_xgb.predict(xgb.DMatrix(X_train)).argmax(axis = 1)\n",
        "acc_xgb_train = np.mean(y_train_pred_xgb == y_train)\n",
        "print(\"X Gradient Boost train accuracy: {:.4f}\".format(acc_xgb_train))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Dense Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparamter tuning for DNN;\n",
        "\n",
        "import tensorflow as tf\n",
        "import math\n",
        "from ray import tune\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras import initializers\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "from ray.tune.integration.keras import TuneReportCallback\n",
        "from ray.tune.schedulers import HyperBandScheduler\n",
        "\n",
        "def data_loader():\n",
        "    return (X_train,pd.get_dummies(y_train)), (X_test,pd.get_dummies(y_test))\n",
        "\n",
        "def lr_step_decay(epoch, lr):\n",
        "    drop_rate = 0.5\n",
        "    epochs_drop = 5.0\n",
        "    return 0.01 * math.pow(drop_rate, math.floor(epoch/epochs_drop))\n",
        "\n",
        "def train_data(config,data):\n",
        "\n",
        "    t1, t2 = data\n",
        "\n",
        "    X_train = t1[0]\n",
        "    y_train = t1[1]\n",
        "    X_test = t2[0]\n",
        "    y_test = t2[1]\n",
        "\n",
        "    n_units = config['units']\n",
        "    n_layers = config['layers']\n",
        "    activation = config['activation']\n",
        "    if(activation == 'tanh' or activation == 'sigmoid'):\n",
        "        initializer = 'glorot_uniform'\n",
        "    else:\n",
        "        initializer = 'he_normal'\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(units = n_units, \n",
        "                    input_dim=X_train.shape[1], \n",
        "                    activation= activation,\n",
        "                    kernel_initializer= initializer, \n",
        "                    name='h1'))\n",
        "    \n",
        "    for i in range(2, n_layers + 1):\n",
        "        model.add(Dense(units= n_units, \n",
        "                        activation= activation,\n",
        "                        kernel_initializer= initializer,  \n",
        "                        name='h{}'.format(i)))\n",
        "        \n",
        "    model.add(Dense(units=14, activation='softmax', kernel_initializer=initializer, name='o'))\n",
        "\n",
        "    model.compile(\n",
        "        loss=\"categorical_crossentropy\", \n",
        "        optimizer=tf.keras.optimizers.Adam(\n",
        "            learning_rate=0.01), \n",
        "        metrics=['accuracy'])\n",
        "\n",
        "    model.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        batch_size=128,\n",
        "        epochs=50,\n",
        "        verbose=0,\n",
        "        validation_data=(X_test, y_test),\n",
        "        callbacks=[TuneReportCallback({\n",
        "            \"val_accuracy\": \"val_accuracy\"\n",
        "            }),\n",
        "            LearningRateScheduler(\n",
        "                lr_step_decay, verbose=0\n",
        "            ),\n",
        "            EarlyStopping(\n",
        "                monitor='val_accuracy', \n",
        "                patience=6\n",
        "            )\n",
        "        ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(train_data pid=39400)\u001b[0m 2023-11-11 20:42:52.816687: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found\n",
            "\u001b[2m\u001b[36m(train_data pid=39400)\u001b[0m 2023-11-11 20:42:52.817559: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "\u001b[2m\u001b[36m(train_data pid=39400)\u001b[0m Skipping registering GPU devices...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div class=\"trialProgress\">\n",
              "  <h3>Trial Progress</h3>\n",
              "  <table>\n",
              "<thead>\n",
              "<tr><th>Trial name            </th><th>date               </th><th>done  </th><th>episodes_total  </th><th>experiment_id                   </th><th>hostname  </th><th style=\"text-align: right;\">  iterations_since_restore</th><th>node_ip  </th><th style=\"text-align: right;\">  pid</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th>timesteps_total  </th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th><th style=\"text-align: right;\">  val_accuracy</th><th style=\"text-align: right;\">  warmup_time</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_data_c8fc6_00000</td><td>2023-11-11_20-42-53</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                         8</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            0.978253</td><td style=\"text-align: right;\">         0.0280032</td><td style=\"text-align: right;\">      0.978253</td><td style=\"text-align: right;\"> 1699753373</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   8</td><td>c8fc6_00000</td><td style=\"text-align: right;\">     0.103604 </td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00001</td><td>2023-11-11_20-42-54</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                        10</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            0.651096</td><td style=\"text-align: right;\">         0.0279999</td><td style=\"text-align: right;\">      0.651096</td><td style=\"text-align: right;\"> 1699753374</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  10</td><td>c8fc6_00001</td><td style=\"text-align: right;\">     0.103604 </td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00002</td><td>2023-11-11_20-42-54</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                        11</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            0.678026</td><td style=\"text-align: right;\">         0.0310972</td><td style=\"text-align: right;\">      0.678026</td><td style=\"text-align: right;\"> 1699753374</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  11</td><td>c8fc6_00002</td><td style=\"text-align: right;\">     0.108108 </td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00003</td><td>2023-11-11_20-42-55</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                         8</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            0.700553</td><td style=\"text-align: right;\">         0.03     </td><td style=\"text-align: right;\">      0.700553</td><td style=\"text-align: right;\"> 1699753375</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   8</td><td>c8fc6_00003</td><td style=\"text-align: right;\">     0.0923423</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00004</td><td>2023-11-11_20-42-56</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                        12</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            0.862578</td><td style=\"text-align: right;\">         0.0305054</td><td style=\"text-align: right;\">      0.862578</td><td style=\"text-align: right;\"> 1699753376</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  12</td><td>c8fc6_00004</td><td style=\"text-align: right;\">     0.0990991</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00005</td><td>2023-11-11_20-42-57</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                         8</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            0.666596</td><td style=\"text-align: right;\">         0.0290024</td><td style=\"text-align: right;\">      0.666596</td><td style=\"text-align: right;\"> 1699753377</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   8</td><td>c8fc6_00005</td><td style=\"text-align: right;\">     0.0990991</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00006</td><td>2023-11-11_20-42-58</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                         9</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            0.915334</td><td style=\"text-align: right;\">         0.0343461</td><td style=\"text-align: right;\">      0.915334</td><td style=\"text-align: right;\"> 1699753378</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   9</td><td>c8fc6_00006</td><td style=\"text-align: right;\">     0.0833333</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00007</td><td>2023-11-11_20-42-59</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                        10</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            0.971732</td><td style=\"text-align: right;\">         0.0330026</td><td style=\"text-align: right;\">      0.971732</td><td style=\"text-align: right;\"> 1699753379</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  10</td><td>c8fc6_00007</td><td style=\"text-align: right;\">     0.0878378</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00008</td><td>2023-11-11_20-43-00</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                         8</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            0.8883  </td><td style=\"text-align: right;\">         0.037003 </td><td style=\"text-align: right;\">      0.8883  </td><td style=\"text-align: right;\"> 1699753380</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   8</td><td>c8fc6_00008</td><td style=\"text-align: right;\">     0.0990991</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00009</td><td>2023-11-11_20-43-01</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                        12</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            1.54378 </td><td style=\"text-align: right;\">         0.0379992</td><td style=\"text-align: right;\">      1.54378 </td><td style=\"text-align: right;\"> 1699753381</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  12</td><td>c8fc6_00009</td><td style=\"text-align: right;\">     0.0878378</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00010</td><td>2023-11-11_20-43-02</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                         7</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            1.2196  </td><td style=\"text-align: right;\">         0.0379992</td><td style=\"text-align: right;\">      1.2196  </td><td style=\"text-align: right;\"> 1699753382</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   7</td><td>c8fc6_00010</td><td style=\"text-align: right;\">     0.0990991</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00011</td><td>2023-11-11_20-43-04</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                         7</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            1.28339 </td><td style=\"text-align: right;\">         0.0409994</td><td style=\"text-align: right;\">      1.28339 </td><td style=\"text-align: right;\"> 1699753384</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   7</td><td>c8fc6_00011</td><td style=\"text-align: right;\">     0.0990991</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00012</td><td>2023-11-11_20-43-06</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                         9</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            2.29116 </td><td style=\"text-align: right;\">         0.0480001</td><td style=\"text-align: right;\">      2.29116 </td><td style=\"text-align: right;\"> 1699753386</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   9</td><td>c8fc6_00012</td><td style=\"text-align: right;\">     0.0990991</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00013</td><td>2023-11-11_20-43-08</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                        12</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            2.29572 </td><td style=\"text-align: right;\">         0.0560002</td><td style=\"text-align: right;\">      2.29572 </td><td style=\"text-align: right;\"> 1699753388</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  12</td><td>c8fc6_00013</td><td style=\"text-align: right;\">     0.103604 </td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00014</td><td>2023-11-11_20-43-10</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                         8</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            2.26206 </td><td style=\"text-align: right;\">         0.0595074</td><td style=\"text-align: right;\">      2.26206 </td><td style=\"text-align: right;\"> 1699753390</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   8</td><td>c8fc6_00014</td><td style=\"text-align: right;\">     0.0990991</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00015</td><td>2023-11-11_20-43-11</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                         8</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            0.573447</td><td style=\"text-align: right;\">         0.0290005</td><td style=\"text-align: right;\">      0.573447</td><td style=\"text-align: right;\"> 1699753391</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   8</td><td>c8fc6_00015</td><td style=\"text-align: right;\">     0.101351 </td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00016</td><td>2023-11-11_20-43-12</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                         8</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            0.564866</td><td style=\"text-align: right;\">         0.0295115</td><td style=\"text-align: right;\">      0.564866</td><td style=\"text-align: right;\"> 1699753392</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   8</td><td>c8fc6_00016</td><td style=\"text-align: right;\">     0.112613 </td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00017</td><td>2023-11-11_20-43-12</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                        16</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            0.808461</td><td style=\"text-align: right;\">         0.0292664</td><td style=\"text-align: right;\">      0.808461</td><td style=\"text-align: right;\"> 1699753392</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  16</td><td>c8fc6_00017</td><td style=\"text-align: right;\">     0.105856 </td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00018</td><td>2023-11-11_20-43-13</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                        11</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            0.783543</td><td style=\"text-align: right;\">         0.0315056</td><td style=\"text-align: right;\">      0.783543</td><td style=\"text-align: right;\"> 1699753393</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  11</td><td>c8fc6_00018</td><td style=\"text-align: right;\">     0.0923423</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00019</td><td>2023-11-11_20-43-14</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                        15</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            0.906557</td><td style=\"text-align: right;\">         0.0340004</td><td style=\"text-align: right;\">      0.906557</td><td style=\"text-align: right;\"> 1699753394</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  15</td><td>c8fc6_00019</td><td style=\"text-align: right;\">     0.0855856</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00020</td><td>2023-11-11_20-43-15</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                         8</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            0.681472</td><td style=\"text-align: right;\">         0.0329638</td><td style=\"text-align: right;\">      0.681472</td><td style=\"text-align: right;\"> 1699753395</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   8</td><td>c8fc6_00020</td><td style=\"text-align: right;\">     0.0990991</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00021</td><td>2023-11-11_20-43-16</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                         7</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            0.869574</td><td style=\"text-align: right;\">         0.0365109</td><td style=\"text-align: right;\">      0.869574</td><td style=\"text-align: right;\"> 1699753396</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   7</td><td>c8fc6_00021</td><td style=\"text-align: right;\">     0.0788288</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00022</td><td>2023-11-11_20-43-17</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                         7</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            0.998646</td><td style=\"text-align: right;\">         0.0385063</td><td style=\"text-align: right;\">      0.998646</td><td style=\"text-align: right;\"> 1699753397</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   7</td><td>c8fc6_00022</td><td style=\"text-align: right;\">     0.0878378</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00023</td><td>2023-11-11_20-43-18</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                         9</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            0.953449</td><td style=\"text-align: right;\">         0.0400038</td><td style=\"text-align: right;\">      0.953449</td><td style=\"text-align: right;\"> 1699753398</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   9</td><td>c8fc6_00023</td><td style=\"text-align: right;\">     0.0990991</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00024</td><td>2023-11-11_20-43-19</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                         9</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            1.40008 </td><td style=\"text-align: right;\">         0.0475099</td><td style=\"text-align: right;\">      1.40008 </td><td style=\"text-align: right;\"> 1699753399</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   9</td><td>c8fc6_00024</td><td style=\"text-align: right;\">     0.0990991</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00025</td><td>2023-11-11_20-43-21</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                         9</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            1.42828 </td><td style=\"text-align: right;\">         0.0505066</td><td style=\"text-align: right;\">      1.42828 </td><td style=\"text-align: right;\"> 1699753401</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   9</td><td>c8fc6_00025</td><td style=\"text-align: right;\">     0.0990991</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00026</td><td>2023-11-11_20-43-22</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                         8</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            1.52408 </td><td style=\"text-align: right;\">         0.0520003</td><td style=\"text-align: right;\">      1.52408 </td><td style=\"text-align: right;\"> 1699753402</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   8</td><td>c8fc6_00026</td><td style=\"text-align: right;\">     0.0990991</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00027</td><td>2023-11-11_20-43-24</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                         7</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            2.25913 </td><td style=\"text-align: right;\">         0.0745101</td><td style=\"text-align: right;\">      2.25913 </td><td style=\"text-align: right;\"> 1699753404</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   7</td><td>c8fc6_00027</td><td style=\"text-align: right;\">     0.0990991</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00028</td><td>2023-11-11_20-43-27</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                         7</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            2.43932 </td><td style=\"text-align: right;\">         0.0825155</td><td style=\"text-align: right;\">      2.43932 </td><td style=\"text-align: right;\"> 1699753407</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   7</td><td>c8fc6_00028</td><td style=\"text-align: right;\">     0.0990991</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00029</td><td>2023-11-11_20-43-29</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                         7</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            2.35478 </td><td style=\"text-align: right;\">         0.0915112</td><td style=\"text-align: right;\">      2.35478 </td><td style=\"text-align: right;\"> 1699753409</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   7</td><td>c8fc6_00029</td><td style=\"text-align: right;\">     0.0990991</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00030</td><td>2023-11-11_20-43-30</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                         8</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            0.616213</td><td style=\"text-align: right;\">         0.0340037</td><td style=\"text-align: right;\">      0.616213</td><td style=\"text-align: right;\"> 1699753410</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   8</td><td>c8fc6_00030</td><td style=\"text-align: right;\">     0.108108 </td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00031</td><td>2023-11-11_20-43-31</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                        12</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            0.780503</td><td style=\"text-align: right;\">         0.0340054</td><td style=\"text-align: right;\">      0.780503</td><td style=\"text-align: right;\"> 1699753411</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  12</td><td>c8fc6_00031</td><td style=\"text-align: right;\">     0.126126 </td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00032</td><td>2023-11-11_20-43-32</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                        12</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            1.03371 </td><td style=\"text-align: right;\">         0.0530419</td><td style=\"text-align: right;\">      1.03371 </td><td style=\"text-align: right;\"> 1699753412</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  12</td><td>c8fc6_00032</td><td style=\"text-align: right;\">     0.123874 </td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00033</td><td>2023-11-11_20-43-33</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                        10</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            1.04282 </td><td style=\"text-align: right;\">         0.0580049</td><td style=\"text-align: right;\">      1.04282 </td><td style=\"text-align: right;\"> 1699753413</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  10</td><td>c8fc6_00033</td><td style=\"text-align: right;\">     0.0720721</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00034</td><td>2023-11-11_20-43-34</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                        10</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            0.991825</td><td style=\"text-align: right;\">         0.0560071</td><td style=\"text-align: right;\">      0.991825</td><td style=\"text-align: right;\"> 1699753414</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  10</td><td>c8fc6_00034</td><td style=\"text-align: right;\">     0.103604 </td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00035</td><td>2023-11-11_20-43-35</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                        11</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            1.0577  </td><td style=\"text-align: right;\">         0.0560005</td><td style=\"text-align: right;\">      1.0577  </td><td style=\"text-align: right;\"> 1699753415</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  11</td><td>c8fc6_00035</td><td style=\"text-align: right;\">     0.0990991</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00036</td><td>2023-11-11_20-43-36</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                        12</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            1.41483 </td><td style=\"text-align: right;\">         0.0680034</td><td style=\"text-align: right;\">      1.41483 </td><td style=\"text-align: right;\"> 1699753416</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  12</td><td>c8fc6_00036</td><td style=\"text-align: right;\">     0.0788288</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00037</td><td>2023-11-11_20-43-38</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                        12</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            1.42654 </td><td style=\"text-align: right;\">         0.0681212</td><td style=\"text-align: right;\">      1.42654 </td><td style=\"text-align: right;\"> 1699753418</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  12</td><td>c8fc6_00037</td><td style=\"text-align: right;\">     0.0945946</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00038</td><td>2023-11-11_20-43-39</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                         7</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            1.35344 </td><td style=\"text-align: right;\">         0.0905073</td><td style=\"text-align: right;\">      1.35344 </td><td style=\"text-align: right;\"> 1699753419</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   7</td><td>c8fc6_00038</td><td style=\"text-align: right;\">     0.0990991</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00039</td><td>2023-11-11_20-43-41</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                         7</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            1.81036 </td><td style=\"text-align: right;\">         0.10104  </td><td style=\"text-align: right;\">      1.81036 </td><td style=\"text-align: right;\"> 1699753421</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   7</td><td>c8fc6_00039</td><td style=\"text-align: right;\">     0.0990991</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00040</td><td>2023-11-11_20-43-43</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                        12</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            2.47785 </td><td style=\"text-align: right;\">         0.131021 </td><td style=\"text-align: right;\">      2.47785 </td><td style=\"text-align: right;\"> 1699753423</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  12</td><td>c8fc6_00040</td><td style=\"text-align: right;\">     0.0923423</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00041</td><td>2023-11-11_20-43-45</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                         8</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            1.97655 </td><td style=\"text-align: right;\">         0.114125 </td><td style=\"text-align: right;\">      1.97655 </td><td style=\"text-align: right;\"> 1699753425</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   8</td><td>c8fc6_00041</td><td style=\"text-align: right;\">     0.0990991</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00042</td><td>2023-11-11_20-43-49</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                         9</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            3.9075  </td><td style=\"text-align: right;\">         0.226531 </td><td style=\"text-align: right;\">      3.9075  </td><td style=\"text-align: right;\"> 1699753429</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   9</td><td>c8fc6_00042</td><td style=\"text-align: right;\">     0.0990991</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00043</td><td>2023-11-11_20-43-54</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                        16</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            4.8898  </td><td style=\"text-align: right;\">         0.193915 </td><td style=\"text-align: right;\">      4.8898  </td><td style=\"text-align: right;\"> 1699753434</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  16</td><td>c8fc6_00043</td><td style=\"text-align: right;\">     0.0855856</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00044</td><td>2023-11-11_20-43-58</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                         7</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            3.66814 </td><td style=\"text-align: right;\">         0.253133 </td><td style=\"text-align: right;\">      3.66814 </td><td style=\"text-align: right;\"> 1699753438</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   7</td><td>c8fc6_00044</td><td style=\"text-align: right;\">     0.0990991</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00045</td><td>2023-11-11_20-43-59</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                         8</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            0.812963</td><td style=\"text-align: right;\">         0.0620291</td><td style=\"text-align: right;\">      0.812963</td><td style=\"text-align: right;\"> 1699753439</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   8</td><td>c8fc6_00045</td><td style=\"text-align: right;\">     0.103604 </td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00046</td><td>2023-11-11_20-44-00</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                         8</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            0.854824</td><td style=\"text-align: right;\">         0.0635157</td><td style=\"text-align: right;\">      0.854824</td><td style=\"text-align: right;\"> 1699753440</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   8</td><td>c8fc6_00046</td><td style=\"text-align: right;\">     0.0968468</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00047</td><td>2023-11-11_20-44-01</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                        15</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            1.28487 </td><td style=\"text-align: right;\">         0.0660577</td><td style=\"text-align: right;\">      1.28487 </td><td style=\"text-align: right;\"> 1699753441</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  15</td><td>c8fc6_00047</td><td style=\"text-align: right;\">     0.0923423</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00048</td><td>2023-11-11_20-44-02</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                         9</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            1.23844 </td><td style=\"text-align: right;\">         0.0870495</td><td style=\"text-align: right;\">      1.23844 </td><td style=\"text-align: right;\"> 1699753442</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   9</td><td>c8fc6_00048</td><td style=\"text-align: right;\">     0.0720721</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00049</td><td>2023-11-11_20-44-04</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                        12</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            2.0026  </td><td style=\"text-align: right;\">         0.129538 </td><td style=\"text-align: right;\">      2.0026  </td><td style=\"text-align: right;\"> 1699753444</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  12</td><td>c8fc6_00049</td><td style=\"text-align: right;\">     0.0810811</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00050</td><td>2023-11-11_20-44-05</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                         7</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            1.27999 </td><td style=\"text-align: right;\">         0.122092 </td><td style=\"text-align: right;\">      1.27999 </td><td style=\"text-align: right;\"> 1699753445</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   7</td><td>c8fc6_00050</td><td style=\"text-align: right;\">     0.0990991</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00051</td><td>2023-11-11_20-44-07</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                         7</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            1.85019 </td><td style=\"text-align: right;\">         0.180106 </td><td style=\"text-align: right;\">      1.85019 </td><td style=\"text-align: right;\"> 1699753447</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   7</td><td>c8fc6_00051</td><td style=\"text-align: right;\">     0.0990991</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00052</td><td>2023-11-11_20-44-09</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                         7</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            1.878   </td><td style=\"text-align: right;\">         0.191602 </td><td style=\"text-align: right;\">      1.878   </td><td style=\"text-align: right;\"> 1699753449</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   7</td><td>c8fc6_00052</td><td style=\"text-align: right;\">     0.0878378</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00053</td><td>2023-11-11_20-44-11</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                         9</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            2.10867 </td><td style=\"text-align: right;\">         0.169111 </td><td style=\"text-align: right;\">      2.10867 </td><td style=\"text-align: right;\"> 1699753451</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   9</td><td>c8fc6_00053</td><td style=\"text-align: right;\">     0.0990991</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00054</td><td>2023-11-11_20-44-16</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                         9</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            5.1423  </td><td style=\"text-align: right;\">         0.464249 </td><td style=\"text-align: right;\">      5.1423  </td><td style=\"text-align: right;\"> 1699753456</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   9</td><td>c8fc6_00054</td><td style=\"text-align: right;\">     0.0990991</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00055</td><td>2023-11-11_20-44-23</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                        16</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            6.27679 </td><td style=\"text-align: right;\">         0.334063 </td><td style=\"text-align: right;\">      6.27679 </td><td style=\"text-align: right;\"> 1699753463</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  16</td><td>c8fc6_00055</td><td style=\"text-align: right;\">     0.0900901</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00056</td><td>2023-11-11_20-44-27</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                         9</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            3.84094 </td><td style=\"text-align: right;\">         0.328726 </td><td style=\"text-align: right;\">      3.84094 </td><td style=\"text-align: right;\"> 1699753467</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   9</td><td>c8fc6_00056</td><td style=\"text-align: right;\">     0.0990991</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00057</td><td>2023-11-11_20-44-34</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                         7</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            7.62996 </td><td style=\"text-align: right;\">         0.820379 </td><td style=\"text-align: right;\">      7.62996 </td><td style=\"text-align: right;\"> 1699753474</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   7</td><td>c8fc6_00057</td><td style=\"text-align: right;\">     0.0833333</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00058</td><td>2023-11-11_20-44-45</td><td>True  </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                        10</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">           10.4086  </td><td style=\"text-align: right;\">         0.83884  </td><td style=\"text-align: right;\">     10.4086  </td><td style=\"text-align: right;\"> 1699753485</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  10</td><td>c8fc6_00058</td><td style=\"text-align: right;\">     0.0653153</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "<tr><td>train_data_c8fc6_00059</td><td>2023-11-11_20-44-48</td><td>False </td><td>                </td><td>acc5a5c233f745a4ab10547268e0918f</td><td>WDesktop  </td><td style=\"text-align: right;\">                         1</td><td>127.0.0.1</td><td style=\"text-align: right;\">39400</td><td style=\"text-align: right;\">            2.9976  </td><td style=\"text-align: right;\">         2.9976   </td><td style=\"text-align: right;\">      2.9976  </td><td style=\"text-align: right;\"> 1699753488</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>c8fc6_00059</td><td style=\"text-align: right;\">     0.0698198</td><td style=\"text-align: right;\">   0.00300026</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</div>\n",
              "<style>\n",
              ".trialProgress {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  color: var(--jp-ui-font-color1);\n",
              "}\n",
              ".trialProgress h3 {\n",
              "  font-weight: bold;\n",
              "}\n",
              ".trialProgress td {\n",
              "  white-space: nowrap;\n",
              "}\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-11-11 20:44:50,161\tWARNING tune.py:690 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
            "2023-11-11 20:44:50,413\tERROR tune.py:758 -- Trials did not complete: [train_data_c8fc6_00059, train_data_c8fc6_00060, train_data_c8fc6_00061, train_data_c8fc6_00062, train_data_c8fc6_00063, train_data_c8fc6_00064, train_data_c8fc6_00065, train_data_c8fc6_00066, train_data_c8fc6_00067, train_data_c8fc6_00068, train_data_c8fc6_00069, train_data_c8fc6_00070, train_data_c8fc6_00071, train_data_c8fc6_00072, train_data_c8fc6_00073, train_data_c8fc6_00074]\n",
            "2023-11-11 20:44:50,413\tINFO tune.py:762 -- Total run time: 121.70 seconds (121.45 seconds for the tuning loop).\n",
            "2023-11-11 20:44:50,414\tWARNING tune.py:768 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n"
          ]
        }
      ],
      "source": [
        "t1,t2 = data_loader()\n",
        "\n",
        "config = {\n",
        "          'units': tune.grid_search([32,64,128,256,512]),\n",
        "          'layers': tune.grid_search([4,8,16,32,64]),\n",
        "          'activation': tune.grid_search(['ReLU','tanh','sigmoid'])\n",
        "}\n",
        "\n",
        "analysis = tune.run(\n",
        "    tune.with_parameters(train_data, data = (t1,t2)),\n",
        "    resources_per_trial = {\"cpu\":24},\n",
        "    config = config,\n",
        "    metric='val_accuracy',\n",
        "    mode=\"max\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 3.0453 - accuracy: 0.0698 - val_loss: 2.6777 - val_accuracy: 0.0563\n",
            "Epoch 2/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 2.6163 - accuracy: 0.1008 - val_loss: 2.6428 - val_accuracy: 0.0766\n",
            "Epoch 3/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 2.5771 - accuracy: 0.1408 - val_loss: 2.6567 - val_accuracy: 0.0901\n",
            "Epoch 4/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 2.5161 - accuracy: 0.1684 - val_loss: 2.7393 - val_accuracy: 0.1014\n",
            "Epoch 5/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 2.4492 - accuracy: 0.1751 - val_loss: 2.8347 - val_accuracy: 0.0946\n",
            "Epoch 6/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 2.3726 - accuracy: 0.2134 - val_loss: 3.0013 - val_accuracy: 0.0811\n",
            "Epoch 7/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 2.2947 - accuracy: 0.2337 - val_loss: 3.0879 - val_accuracy: 0.0833\n",
            "Epoch 8/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 2.2108 - accuracy: 0.2601 - val_loss: 3.3391 - val_accuracy: 0.0811\n",
            "Epoch 9/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 2.1268 - accuracy: 0.2894 - val_loss: 3.5076 - val_accuracy: 0.0856\n",
            "Epoch 10/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 2.0456 - accuracy: 0.3142 - val_loss: 3.5776 - val_accuracy: 0.0991\n",
            "Epoch 11/1000\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 1.9949 - accuracy: 0.3131 - val_loss: 3.6498 - val_accuracy: 0.0901\n",
            "Epoch 12/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.9286 - accuracy: 0.3525 - val_loss: 3.9712 - val_accuracy: 0.0901\n",
            "Epoch 13/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.8759 - accuracy: 0.3592 - val_loss: 4.0735 - val_accuracy: 0.0946\n",
            "Epoch 14/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.8108 - accuracy: 0.3761 - val_loss: 4.0972 - val_accuracy: 0.1036\n",
            "Epoch 15/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.7499 - accuracy: 0.3902 - val_loss: 4.4115 - val_accuracy: 0.0923\n",
            "Epoch 16/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.6855 - accuracy: 0.4285 - val_loss: 4.7583 - val_accuracy: 0.0946\n",
            "Epoch 17/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.6462 - accuracy: 0.4443 - val_loss: 4.8804 - val_accuracy: 0.1014\n",
            "Epoch 18/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.5776 - accuracy: 0.4572 - val_loss: 5.1981 - val_accuracy: 0.0991\n",
            "Epoch 19/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.5300 - accuracy: 0.4657 - val_loss: 5.1733 - val_accuracy: 0.0923\n",
            "Epoch 20/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.4642 - accuracy: 0.4854 - val_loss: 5.9542 - val_accuracy: 0.0901\n",
            "Epoch 21/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.4390 - accuracy: 0.4916 - val_loss: 6.0135 - val_accuracy: 0.0788\n",
            "Epoch 22/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.4609 - accuracy: 0.4966 - val_loss: 6.2178 - val_accuracy: 0.0631\n",
            "Epoch 23/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.4122 - accuracy: 0.5208 - val_loss: 6.1797 - val_accuracy: 0.0811\n",
            "Epoch 24/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.4022 - accuracy: 0.5208 - val_loss: 6.5094 - val_accuracy: 0.0721\n",
            "Epoch 25/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.3662 - accuracy: 0.5146 - val_loss: 6.5916 - val_accuracy: 0.0946\n",
            "Epoch 26/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.3587 - accuracy: 0.5175 - val_loss: 6.5938 - val_accuracy: 0.0833\n",
            "Epoch 27/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.2793 - accuracy: 0.5434 - val_loss: 6.8099 - val_accuracy: 0.0766\n",
            "Epoch 28/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.2428 - accuracy: 0.5636 - val_loss: 7.1392 - val_accuracy: 0.0833\n",
            "Epoch 29/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.1835 - accuracy: 0.5687 - val_loss: 7.1512 - val_accuracy: 0.0923\n",
            "Epoch 30/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.1946 - accuracy: 0.5811 - val_loss: 7.9049 - val_accuracy: 0.0991\n",
            "Epoch 31/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.1899 - accuracy: 0.5850 - val_loss: 7.6031 - val_accuracy: 0.0878\n",
            "Epoch 32/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.1703 - accuracy: 0.5873 - val_loss: 8.0537 - val_accuracy: 0.0878\n",
            "Epoch 33/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.1177 - accuracy: 0.6059 - val_loss: 7.9599 - val_accuracy: 0.0946\n",
            "Epoch 34/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.1239 - accuracy: 0.6109 - val_loss: 8.4325 - val_accuracy: 0.0901\n",
            "Epoch 35/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.1001 - accuracy: 0.6092 - val_loss: 8.3658 - val_accuracy: 0.0901\n",
            "Epoch 36/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.0567 - accuracy: 0.6301 - val_loss: 8.3726 - val_accuracy: 0.0856\n",
            "Epoch 37/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.0891 - accuracy: 0.6250 - val_loss: 8.8139 - val_accuracy: 0.0923\n",
            "Epoch 38/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.1359 - accuracy: 0.6030 - val_loss: 8.4476 - val_accuracy: 0.0833\n",
            "Epoch 39/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.1047 - accuracy: 0.6075 - val_loss: 8.6101 - val_accuracy: 0.1104\n",
            "Epoch 40/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.0679 - accuracy: 0.6239 - val_loss: 9.2979 - val_accuracy: 0.0766\n",
            "Epoch 41/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.0606 - accuracy: 0.6267 - val_loss: 8.9167 - val_accuracy: 0.0923\n",
            "Epoch 42/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.0071 - accuracy: 0.6447 - val_loss: 9.3947 - val_accuracy: 0.0946\n",
            "Epoch 43/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.9554 - accuracy: 0.6537 - val_loss: 9.5270 - val_accuracy: 0.0991\n",
            "Epoch 44/1000\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.9108 - accuracy: 0.6830 - val_loss: 9.7246 - val_accuracy: 0.1014\n",
            "Epoch 45/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.8521 - accuracy: 0.7005 - val_loss: 9.8503 - val_accuracy: 0.0968\n",
            "Epoch 46/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.8725 - accuracy: 0.6858 - val_loss: 10.2523 - val_accuracy: 0.1059\n",
            "Epoch 47/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.8326 - accuracy: 0.7083 - val_loss: 10.7043 - val_accuracy: 0.1059\n",
            "Epoch 48/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.7953 - accuracy: 0.7196 - val_loss: 11.2437 - val_accuracy: 0.1059\n",
            "Epoch 49/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.8168 - accuracy: 0.7145 - val_loss: 11.5213 - val_accuracy: 0.0968\n",
            "Epoch 50/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.8372 - accuracy: 0.7078 - val_loss: 10.9610 - val_accuracy: 0.0991\n",
            "Epoch 51/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.9470 - accuracy: 0.6689 - val_loss: 11.0960 - val_accuracy: 0.0968\n",
            "Epoch 52/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.0069 - accuracy: 0.6543 - val_loss: 11.5396 - val_accuracy: 0.0946\n",
            "Epoch 53/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.0039 - accuracy: 0.6627 - val_loss: 11.0048 - val_accuracy: 0.0923\n",
            "Epoch 54/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.9519 - accuracy: 0.6633 - val_loss: 11.2074 - val_accuracy: 0.0788\n",
            "Epoch 55/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.9512 - accuracy: 0.6689 - val_loss: 11.2923 - val_accuracy: 0.0901\n",
            "Epoch 56/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.8931 - accuracy: 0.6931 - val_loss: 11.1679 - val_accuracy: 0.1014\n",
            "Epoch 57/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.7987 - accuracy: 0.7140 - val_loss: 11.6058 - val_accuracy: 0.0946\n",
            "Epoch 58/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.8066 - accuracy: 0.7292 - val_loss: 11.6165 - val_accuracy: 0.0878\n",
            "Epoch 59/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.7374 - accuracy: 0.7438 - val_loss: 11.9222 - val_accuracy: 0.0856\n",
            "Epoch 60/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.7387 - accuracy: 0.7523 - val_loss: 12.5147 - val_accuracy: 0.0856\n",
            "Epoch 61/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.7309 - accuracy: 0.7545 - val_loss: 12.6236 - val_accuracy: 0.0901\n",
            "Epoch 62/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.7184 - accuracy: 0.7584 - val_loss: 12.7569 - val_accuracy: 0.0743\n",
            "Epoch 63/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.7532 - accuracy: 0.7331 - val_loss: 13.1693 - val_accuracy: 0.0856\n",
            "Epoch 64/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.6994 - accuracy: 0.7506 - val_loss: 13.3708 - val_accuracy: 0.0698\n",
            "Epoch 65/1000\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.6965 - accuracy: 0.7601 - val_loss: 13.5236 - val_accuracy: 0.0878\n",
            "Epoch 66/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.7154 - accuracy: 0.7466 - val_loss: 13.1794 - val_accuracy: 0.0946\n",
            "Epoch 67/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.7002 - accuracy: 0.7568 - val_loss: 13.5343 - val_accuracy: 0.1014\n",
            "Epoch 68/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.7028 - accuracy: 0.7556 - val_loss: 13.3791 - val_accuracy: 0.0833\n",
            "Epoch 69/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.7102 - accuracy: 0.7551 - val_loss: 14.0276 - val_accuracy: 0.0946\n",
            "Epoch 70/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.6968 - accuracy: 0.7506 - val_loss: 13.9631 - val_accuracy: 0.0878\n",
            "Epoch 71/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.6846 - accuracy: 0.7607 - val_loss: 14.6171 - val_accuracy: 0.0923\n",
            "Epoch 72/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.7131 - accuracy: 0.7545 - val_loss: 13.9075 - val_accuracy: 0.1104\n",
            "Epoch 73/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.7434 - accuracy: 0.7427 - val_loss: 14.3011 - val_accuracy: 0.0856\n",
            "Epoch 74/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.7568 - accuracy: 0.7359 - val_loss: 14.1858 - val_accuracy: 0.0721\n",
            "Epoch 75/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.7593 - accuracy: 0.7421 - val_loss: 14.0471 - val_accuracy: 0.0991\n",
            "Epoch 76/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.7510 - accuracy: 0.7477 - val_loss: 14.1211 - val_accuracy: 0.0878\n",
            "Epoch 77/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.6731 - accuracy: 0.7630 - val_loss: 14.3463 - val_accuracy: 0.0811\n",
            "Epoch 78/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.6569 - accuracy: 0.7748 - val_loss: 14.7954 - val_accuracy: 0.0856\n",
            "Epoch 79/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.6262 - accuracy: 0.7843 - val_loss: 15.5113 - val_accuracy: 0.0856\n",
            "Epoch 80/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.6104 - accuracy: 0.7843 - val_loss: 15.3523 - val_accuracy: 0.0811\n",
            "Epoch 81/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.6020 - accuracy: 0.7922 - val_loss: 14.7854 - val_accuracy: 0.0901\n",
            "Epoch 82/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.6234 - accuracy: 0.7905 - val_loss: 15.1470 - val_accuracy: 0.0901\n",
            "Epoch 83/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.5541 - accuracy: 0.8148 - val_loss: 15.8008 - val_accuracy: 0.0968\n",
            "Epoch 84/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.5377 - accuracy: 0.8221 - val_loss: 16.4842 - val_accuracy: 0.0878\n",
            "Epoch 85/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.5165 - accuracy: 0.8345 - val_loss: 16.6404 - val_accuracy: 0.1014\n",
            "Epoch 86/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.5189 - accuracy: 0.8316 - val_loss: 17.0068 - val_accuracy: 0.0878\n",
            "Epoch 87/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4825 - accuracy: 0.8339 - val_loss: 17.5324 - val_accuracy: 0.0743\n",
            "Epoch 88/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.5106 - accuracy: 0.8322 - val_loss: 17.4488 - val_accuracy: 0.0856\n",
            "Epoch 89/1000\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.5096 - accuracy: 0.8356 - val_loss: 17.5714 - val_accuracy: 0.0878\n",
            "Epoch 90/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.5124 - accuracy: 0.8288 - val_loss: 18.0105 - val_accuracy: 0.0856\n",
            "Epoch 91/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.6398 - accuracy: 0.7759 - val_loss: 18.1358 - val_accuracy: 0.0878\n",
            "Epoch 92/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.7171 - accuracy: 0.7652 - val_loss: 17.7098 - val_accuracy: 0.0856\n",
            "Epoch 93/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.8022 - accuracy: 0.7297 - val_loss: 16.5958 - val_accuracy: 0.0946\n",
            "Epoch 94/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.7533 - accuracy: 0.7466 - val_loss: 17.4934 - val_accuracy: 0.0901\n",
            "Epoch 95/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.7513 - accuracy: 0.7680 - val_loss: 16.5216 - val_accuracy: 0.0878\n",
            "Epoch 96/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.7431 - accuracy: 0.7534 - val_loss: 16.9975 - val_accuracy: 0.0743\n",
            "Epoch 97/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.7609 - accuracy: 0.7337 - val_loss: 16.8713 - val_accuracy: 0.0923\n",
            "Epoch 98/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.7132 - accuracy: 0.7528 - val_loss: 15.6452 - val_accuracy: 0.0901\n",
            "Epoch 99/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.6779 - accuracy: 0.7652 - val_loss: 15.9993 - val_accuracy: 0.0721\n",
            "Epoch 100/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.5990 - accuracy: 0.7962 - val_loss: 16.7565 - val_accuracy: 0.0676\n",
            "Epoch 101/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.5768 - accuracy: 0.8069 - val_loss: 16.6161 - val_accuracy: 0.0766\n",
            "Epoch 102/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.5704 - accuracy: 0.7956 - val_loss: 17.3450 - val_accuracy: 0.0833\n",
            "Epoch 103/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.5228 - accuracy: 0.8283 - val_loss: 17.5818 - val_accuracy: 0.0721\n",
            "Epoch 104/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4731 - accuracy: 0.8446 - val_loss: 17.9616 - val_accuracy: 0.0856\n",
            "Epoch 105/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4140 - accuracy: 0.8632 - val_loss: 17.9012 - val_accuracy: 0.0878\n",
            "Epoch 106/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4515 - accuracy: 0.8485 - val_loss: 18.3266 - val_accuracy: 0.0901\n",
            "Epoch 107/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4533 - accuracy: 0.8525 - val_loss: 18.7545 - val_accuracy: 0.0743\n",
            "Epoch 108/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4409 - accuracy: 0.8559 - val_loss: 18.7967 - val_accuracy: 0.0968\n",
            "Epoch 109/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4003 - accuracy: 0.8705 - val_loss: 19.2661 - val_accuracy: 0.0878\n",
            "Epoch 110/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3826 - accuracy: 0.8739 - val_loss: 19.4742 - val_accuracy: 0.0946\n",
            "Epoch 111/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3017 - accuracy: 0.9048 - val_loss: 19.6642 - val_accuracy: 0.0901\n",
            "Epoch 112/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2957 - accuracy: 0.9043 - val_loss: 20.1991 - val_accuracy: 0.0721\n",
            "Epoch 113/1000\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2643 - accuracy: 0.9144 - val_loss: 20.6478 - val_accuracy: 0.0833\n",
            "Epoch 114/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2871 - accuracy: 0.9161 - val_loss: 21.2505 - val_accuracy: 0.0856\n",
            "Epoch 115/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3197 - accuracy: 0.9009 - val_loss: 22.0173 - val_accuracy: 0.0788\n",
            "Epoch 116/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3308 - accuracy: 0.9015 - val_loss: 21.6956 - val_accuracy: 0.0901\n",
            "Epoch 117/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3444 - accuracy: 0.8874 - val_loss: 21.7418 - val_accuracy: 0.0788\n",
            "Epoch 118/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3640 - accuracy: 0.8834 - val_loss: 22.2141 - val_accuracy: 0.0766\n",
            "Epoch 119/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4316 - accuracy: 0.8587 - val_loss: 22.3155 - val_accuracy: 0.0833\n",
            "Epoch 120/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.5658 - accuracy: 0.8232 - val_loss: 21.9078 - val_accuracy: 0.0856\n",
            "Epoch 121/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.6206 - accuracy: 0.8125 - val_loss: 21.2085 - val_accuracy: 0.0788\n",
            "Epoch 122/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.7037 - accuracy: 0.7714 - val_loss: 20.9527 - val_accuracy: 0.0698\n",
            "Epoch 123/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.7659 - accuracy: 0.7568 - val_loss: 19.4295 - val_accuracy: 0.0653\n",
            "Epoch 124/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.9322 - accuracy: 0.7314 - val_loss: 20.7755 - val_accuracy: 0.0833\n",
            "Epoch 125/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.9605 - accuracy: 0.7157 - val_loss: 19.3052 - val_accuracy: 0.0923\n",
            "Epoch 126/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.8499 - accuracy: 0.7297 - val_loss: 17.7595 - val_accuracy: 0.0698\n",
            "Epoch 127/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.7691 - accuracy: 0.7523 - val_loss: 17.6124 - val_accuracy: 0.0743\n",
            "Epoch 128/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.7731 - val_loss: 17.3689 - val_accuracy: 0.0856\n",
            "Epoch 129/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.6806 - accuracy: 0.7815 - val_loss: 17.4975 - val_accuracy: 0.0743\n",
            "Epoch 130/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.6056 - accuracy: 0.8018 - val_loss: 17.4076 - val_accuracy: 0.0811\n",
            "Epoch 131/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4909 - accuracy: 0.8384 - val_loss: 18.0955 - val_accuracy: 0.0743\n",
            "Epoch 132/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4713 - accuracy: 0.8440 - val_loss: 18.1416 - val_accuracy: 0.0766\n",
            "Epoch 133/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4099 - accuracy: 0.8637 - val_loss: 18.7390 - val_accuracy: 0.0721\n",
            "Epoch 134/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3807 - accuracy: 0.8778 - val_loss: 19.0360 - val_accuracy: 0.0743\n",
            "Epoch 135/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3463 - accuracy: 0.8919 - val_loss: 19.6080 - val_accuracy: 0.0743\n",
            "Epoch 136/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3286 - accuracy: 0.8964 - val_loss: 19.4068 - val_accuracy: 0.0833\n",
            "Epoch 137/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3046 - accuracy: 0.9077 - val_loss: 20.2766 - val_accuracy: 0.0766\n",
            "Epoch 138/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2851 - accuracy: 0.9122 - val_loss: 21.1155 - val_accuracy: 0.0698\n",
            "Epoch 139/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2706 - accuracy: 0.9178 - val_loss: 21.1554 - val_accuracy: 0.0811\n",
            "Epoch 140/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2664 - accuracy: 0.9217 - val_loss: 22.0245 - val_accuracy: 0.0698\n",
            "Epoch 141/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2404 - accuracy: 0.9291 - val_loss: 21.7202 - val_accuracy: 0.0676\n",
            "Epoch 142/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2466 - accuracy: 0.9296 - val_loss: 22.3112 - val_accuracy: 0.0811\n",
            "Epoch 143/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2366 - accuracy: 0.9212 - val_loss: 22.0517 - val_accuracy: 0.0766\n",
            "Epoch 144/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2616 - accuracy: 0.9200 - val_loss: 23.4820 - val_accuracy: 0.0856\n",
            "Epoch 145/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2723 - accuracy: 0.9195 - val_loss: 23.2499 - val_accuracy: 0.0811\n",
            "Epoch 146/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2754 - accuracy: 0.9212 - val_loss: 23.1119 - val_accuracy: 0.0676\n",
            "Epoch 147/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2778 - accuracy: 0.9133 - val_loss: 24.1702 - val_accuracy: 0.0743\n",
            "Epoch 148/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3701 - accuracy: 0.8789 - val_loss: 24.0650 - val_accuracy: 0.0743\n",
            "Epoch 149/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4332 - accuracy: 0.8620 - val_loss: 24.3340 - val_accuracy: 0.0788\n",
            "Epoch 150/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.5004 - accuracy: 0.8395 - val_loss: 23.9495 - val_accuracy: 0.0856\n",
            "Epoch 151/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.5673 - accuracy: 0.8361 - val_loss: 23.7235 - val_accuracy: 0.0878\n",
            "Epoch 152/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.5854 - accuracy: 0.8311 - val_loss: 23.1306 - val_accuracy: 0.0833\n",
            "Epoch 153/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.6079 - accuracy: 0.8170 - val_loss: 22.6957 - val_accuracy: 0.0946\n",
            "Epoch 154/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.6891 - accuracy: 0.7973 - val_loss: 22.5498 - val_accuracy: 0.1036\n",
            "Epoch 155/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.7139 - accuracy: 0.7731 - val_loss: 21.4921 - val_accuracy: 0.0901\n",
            "Epoch 156/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.8014 - accuracy: 0.7596 - val_loss: 21.8019 - val_accuracy: 0.0811\n",
            "Epoch 157/1000\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.7443 - accuracy: 0.7646 - val_loss: 20.7112 - val_accuracy: 0.0676\n",
            "Epoch 158/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.7085 - accuracy: 0.7832 - val_loss: 20.2424 - val_accuracy: 0.0608\n",
            "Epoch 159/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.7588 - accuracy: 0.7776 - val_loss: 20.1529 - val_accuracy: 0.0788\n",
            "Epoch 160/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.7075 - accuracy: 0.7720 - val_loss: 19.7787 - val_accuracy: 0.0833\n",
            "Epoch 161/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.5793 - accuracy: 0.8164 - val_loss: 19.8659 - val_accuracy: 0.0811\n",
            "Epoch 162/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.5176 - accuracy: 0.8249 - val_loss: 20.3197 - val_accuracy: 0.0766\n",
            "Epoch 163/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4794 - accuracy: 0.8418 - val_loss: 20.2991 - val_accuracy: 0.0833\n",
            "Epoch 164/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4272 - accuracy: 0.8598 - val_loss: 20.5055 - val_accuracy: 0.0743\n",
            "Epoch 165/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3374 - accuracy: 0.8908 - val_loss: 21.4500 - val_accuracy: 0.0833\n",
            "Epoch 166/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2963 - accuracy: 0.9065 - val_loss: 21.5618 - val_accuracy: 0.0856\n",
            "Epoch 167/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2642 - accuracy: 0.9217 - val_loss: 21.5579 - val_accuracy: 0.0788\n",
            "Epoch 168/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2891 - accuracy: 0.9161 - val_loss: 22.1082 - val_accuracy: 0.0811\n",
            "Epoch 169/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2911 - accuracy: 0.9268 - val_loss: 21.8767 - val_accuracy: 0.0856\n",
            "Epoch 170/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2222 - accuracy: 0.9262 - val_loss: 22.8089 - val_accuracy: 0.0833\n",
            "Epoch 171/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2094 - accuracy: 0.9420 - val_loss: 22.8880 - val_accuracy: 0.0878\n",
            "Epoch 172/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2159 - accuracy: 0.9443 - val_loss: 23.6636 - val_accuracy: 0.0811\n",
            "Epoch 173/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1924 - accuracy: 0.9448 - val_loss: 24.1262 - val_accuracy: 0.0946\n",
            "Epoch 174/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2156 - accuracy: 0.9431 - val_loss: 24.2885 - val_accuracy: 0.0878\n",
            "Epoch 175/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2107 - accuracy: 0.9403 - val_loss: 24.5999 - val_accuracy: 0.0811\n",
            "Epoch 176/1000\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2091 - accuracy: 0.9443 - val_loss: 25.1860 - val_accuracy: 0.0721\n",
            "Epoch 177/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1858 - accuracy: 0.9454 - val_loss: 25.1938 - val_accuracy: 0.0698\n",
            "Epoch 178/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1927 - accuracy: 0.9459 - val_loss: 25.4036 - val_accuracy: 0.0743\n",
            "Epoch 179/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1658 - accuracy: 0.9578 - val_loss: 25.4706 - val_accuracy: 0.0698\n",
            "Epoch 180/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1554 - accuracy: 0.9583 - val_loss: 27.1034 - val_accuracy: 0.0766\n",
            "Epoch 181/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1487 - accuracy: 0.9595 - val_loss: 26.9395 - val_accuracy: 0.0766\n",
            "Epoch 182/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1594 - accuracy: 0.9566 - val_loss: 27.1720 - val_accuracy: 0.0766\n",
            "Epoch 183/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1658 - accuracy: 0.9527 - val_loss: 27.0327 - val_accuracy: 0.0653\n",
            "Epoch 184/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1947 - accuracy: 0.9476 - val_loss: 27.9039 - val_accuracy: 0.0901\n",
            "Epoch 185/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2186 - accuracy: 0.9381 - val_loss: 27.4293 - val_accuracy: 0.0518\n",
            "Epoch 186/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2671 - accuracy: 0.9206 - val_loss: 28.4550 - val_accuracy: 0.0946\n",
            "Epoch 187/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3926 - accuracy: 0.8784 - val_loss: 28.6999 - val_accuracy: 0.0721\n",
            "Epoch 188/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.5779 - accuracy: 0.8333 - val_loss: 26.9738 - val_accuracy: 0.0833\n",
            "Epoch 189/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.7084 - accuracy: 0.8012 - val_loss: 25.4367 - val_accuracy: 0.0698\n",
            "Epoch 190/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.9347 - accuracy: 0.7466 - val_loss: 24.3386 - val_accuracy: 0.0788\n",
            "Epoch 191/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.0940 - accuracy: 0.7213 - val_loss: 23.9661 - val_accuracy: 0.0833\n",
            "Epoch 192/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.0093 - accuracy: 0.7224 - val_loss: 22.3626 - val_accuracy: 0.0833\n",
            "Epoch 193/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.8972 - accuracy: 0.7303 - val_loss: 22.0499 - val_accuracy: 0.0743\n",
            "Epoch 194/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.9209 - accuracy: 0.7320 - val_loss: 21.9889 - val_accuracy: 0.0766\n",
            "Epoch 195/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.7463 - accuracy: 0.7708 - val_loss: 20.1771 - val_accuracy: 0.0743\n",
            "Epoch 196/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.6530 - accuracy: 0.7877 - val_loss: 20.7175 - val_accuracy: 0.0721\n",
            "Epoch 197/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.5243 - accuracy: 0.8390 - val_loss: 20.5303 - val_accuracy: 0.0721\n",
            "Epoch 198/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4515 - accuracy: 0.8423 - val_loss: 20.7864 - val_accuracy: 0.0766\n",
            "Epoch 199/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3682 - accuracy: 0.8806 - val_loss: 21.1860 - val_accuracy: 0.0788\n",
            "Epoch 200/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3115 - accuracy: 0.9020 - val_loss: 21.6103 - val_accuracy: 0.0788\n",
            "Epoch 201/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2676 - accuracy: 0.9155 - val_loss: 22.5556 - val_accuracy: 0.0856\n",
            "Epoch 202/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2270 - accuracy: 0.9268 - val_loss: 23.0523 - val_accuracy: 0.0743\n",
            "Epoch 203/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2159 - accuracy: 0.9381 - val_loss: 23.6159 - val_accuracy: 0.0856\n",
            "Epoch 204/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1984 - accuracy: 0.9426 - val_loss: 23.5809 - val_accuracy: 0.0901\n",
            "Epoch 205/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1765 - accuracy: 0.9516 - val_loss: 23.9956 - val_accuracy: 0.0856\n",
            "Epoch 206/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1606 - accuracy: 0.9606 - val_loss: 24.5556 - val_accuracy: 0.0833\n",
            "Epoch 207/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1515 - accuracy: 0.9566 - val_loss: 25.0532 - val_accuracy: 0.0788\n",
            "Epoch 208/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1382 - accuracy: 0.9645 - val_loss: 25.2616 - val_accuracy: 0.0878\n",
            "Epoch 209/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1259 - accuracy: 0.9696 - val_loss: 25.6048 - val_accuracy: 0.0788\n",
            "Epoch 210/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1287 - accuracy: 0.9707 - val_loss: 26.4176 - val_accuracy: 0.0766\n",
            "Epoch 211/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1234 - accuracy: 0.9713 - val_loss: 26.2559 - val_accuracy: 0.0788\n",
            "Epoch 212/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1152 - accuracy: 0.9735 - val_loss: 27.2225 - val_accuracy: 0.0676\n",
            "Epoch 213/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1133 - accuracy: 0.9718 - val_loss: 27.7274 - val_accuracy: 0.0698\n",
            "Epoch 214/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1303 - accuracy: 0.9640 - val_loss: 27.2662 - val_accuracy: 0.0676\n",
            "Epoch 215/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1413 - accuracy: 0.9561 - val_loss: 27.7469 - val_accuracy: 0.0766\n",
            "Epoch 216/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1445 - accuracy: 0.9634 - val_loss: 28.1761 - val_accuracy: 0.0811\n",
            "Epoch 217/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1485 - accuracy: 0.9578 - val_loss: 28.4077 - val_accuracy: 0.0721\n",
            "Epoch 218/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1707 - accuracy: 0.9606 - val_loss: 29.4626 - val_accuracy: 0.0631\n",
            "Epoch 219/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2340 - accuracy: 0.9336 - val_loss: 29.0847 - val_accuracy: 0.0766\n",
            "Epoch 220/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2835 - accuracy: 0.9127 - val_loss: 28.8387 - val_accuracy: 0.0788\n",
            "Epoch 221/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3887 - accuracy: 0.8919 - val_loss: 29.3034 - val_accuracy: 0.0766\n",
            "Epoch 222/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.5296 - accuracy: 0.8457 - val_loss: 29.8349 - val_accuracy: 0.0721\n",
            "Epoch 223/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.7330 - accuracy: 0.8080 - val_loss: 27.0664 - val_accuracy: 0.0923\n",
            "Epoch 224/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.9099 - accuracy: 0.7607 - val_loss: 25.0895 - val_accuracy: 0.0833\n",
            "Epoch 225/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.9285 - accuracy: 0.7432 - val_loss: 24.9261 - val_accuracy: 0.0698\n",
            "Epoch 226/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.9530 - accuracy: 0.7421 - val_loss: 22.7484 - val_accuracy: 0.0653\n",
            "Epoch 227/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.8058 - accuracy: 0.7596 - val_loss: 22.9888 - val_accuracy: 0.0676\n",
            "Epoch 228/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.7764 - accuracy: 0.7742 - val_loss: 21.4312 - val_accuracy: 0.0788\n",
            "Epoch 229/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.6940 - accuracy: 0.7883 - val_loss: 21.0893 - val_accuracy: 0.1014\n",
            "Epoch 230/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.5878 - accuracy: 0.8119 - val_loss: 20.9988 - val_accuracy: 0.0788\n",
            "Epoch 231/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4674 - accuracy: 0.8463 - val_loss: 21.0003 - val_accuracy: 0.0946\n",
            "Epoch 232/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4003 - accuracy: 0.8671 - val_loss: 21.6760 - val_accuracy: 0.0923\n",
            "Epoch 233/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4009 - accuracy: 0.8688 - val_loss: 22.0768 - val_accuracy: 0.0901\n",
            "Epoch 234/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3568 - accuracy: 0.8756 - val_loss: 22.2929 - val_accuracy: 0.0856\n",
            "Epoch 235/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2973 - accuracy: 0.9009 - val_loss: 22.4857 - val_accuracy: 0.0833\n",
            "Epoch 236/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2619 - accuracy: 0.9257 - val_loss: 23.4642 - val_accuracy: 0.0878\n",
            "Epoch 237/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2297 - accuracy: 0.9217 - val_loss: 23.5728 - val_accuracy: 0.0743\n",
            "Epoch 238/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1963 - accuracy: 0.9448 - val_loss: 23.8659 - val_accuracy: 0.0833\n",
            "Epoch 239/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1767 - accuracy: 0.9488 - val_loss: 24.5589 - val_accuracy: 0.0811\n",
            "Epoch 240/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1655 - accuracy: 0.9493 - val_loss: 24.9013 - val_accuracy: 0.0878\n",
            "Epoch 241/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1674 - accuracy: 0.9561 - val_loss: 25.4395 - val_accuracy: 0.0968\n",
            "Epoch 242/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1598 - accuracy: 0.9533 - val_loss: 24.8631 - val_accuracy: 0.0788\n",
            "Epoch 243/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1659 - accuracy: 0.9589 - val_loss: 25.7461 - val_accuracy: 0.0766\n",
            "Epoch 244/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1389 - accuracy: 0.9690 - val_loss: 26.1376 - val_accuracy: 0.0788\n",
            "Epoch 245/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1817 - accuracy: 0.9533 - val_loss: 26.4659 - val_accuracy: 0.0946\n",
            "Epoch 246/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2028 - accuracy: 0.9516 - val_loss: 27.4975 - val_accuracy: 0.0788\n",
            "Epoch 247/1000\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2569 - accuracy: 0.9251 - val_loss: 26.7672 - val_accuracy: 0.0811\n",
            "Epoch 248/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2470 - accuracy: 0.9217 - val_loss: 26.7843 - val_accuracy: 0.0923\n",
            "Epoch 249/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2665 - accuracy: 0.9195 - val_loss: 27.3398 - val_accuracy: 0.0833\n",
            "Epoch 250/1000\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3562 - accuracy: 0.8941 - val_loss: 27.1310 - val_accuracy: 0.0766\n",
            "Epoch 251/1000\n",
            " 1/14 [=>............................] - ETA: 0s - loss: 0.2605 - accuracy: 0.9141"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28888\\2126229166.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;36m0.001\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mepochs_drop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m model.fit(\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\andyw\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\andyw\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1604\u001b[0m                             \u001b[0msteps_per_execution\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1605\u001b[0m                         )\n\u001b[1;32m-> 1606\u001b[1;33m                     val_logs = self.evaluate(\n\u001b[0m\u001b[0;32m   1607\u001b[0m                         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1608\u001b[0m                         \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\andyw\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\andyw\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1937\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_test_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1938\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1939\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Single epoch.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1940\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1941\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\andyw\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0menumerate_epochs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1305\u001b[0m         \u001b[1;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1307\u001b[0m             \u001b[0mdata_iterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\andyw\\anaconda3\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"generator didn't yield\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\andyw\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m_truncate_execution_to_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1325\u001b[0m         should_truncate = (\n\u001b[0;32m   1326\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inferred_steps\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inferred_steps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m         )\n\u001b[0;32m   1329\u001b[0m         \u001b[0moriginal_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\andyw\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    635\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 637\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    638\u001b[0m     raise NotImplementedError(\n\u001b[0;32m    639\u001b[0m         \"numpy() is only available when eager execution is enabled.\")\n",
            "\u001b[1;32mc:\\Users\\andyw\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36mread_value\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    726\u001b[0m     \u001b[1;31m# Return an identity so it can get placed on whatever device the context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m     \u001b[1;31m# specifies instead of the device where the variable is.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mread_value_no_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\andyw\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\andyw\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1176\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1177\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1178\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\andyw\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[1;34m(input, name)\u001b[0m\n\u001b[0;32m    292\u001b[0m     \u001b[1;31m# variables. Variables have correct handle data when graph building.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m   \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m   \u001b[1;31m# Propagate handle data for happier shape inference for resource variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_handle_data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\andyw\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[1;34m(input, name)\u001b[0m\n\u001b[0;32m   4066\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4067\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4068\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   4069\u001b[0m         _ctx, \"Identity\", name, input)\n\u001b[0;32m   4070\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Neural network\n",
        "\n",
        "config ={'units': 16, 'layers': 4, 'activation': 'ReLU'}\n",
        "\n",
        "n_units = config['units']\n",
        "n_layers = config['layers']\n",
        "activation = config['activation']\n",
        "if(activation == 'tanh' or activation == 'sigmoid'):\n",
        "    initializer = 'glorot_uniform'\n",
        "else:\n",
        "    initializer = 'he_normal'\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(units = n_units, \n",
        "                input_dim=X_train.shape[1], \n",
        "                activation= activation,\n",
        "                kernel_initializer= initializer, \n",
        "                name='h1'))\n",
        "\n",
        "for i in range(2, n_layers + 1):\n",
        "    model.add(Dense(units= n_units, \n",
        "                    activation= activation,\n",
        "                    kernel_initializer= initializer,  \n",
        "                    name='h{}'.format(i)))\n",
        "        \n",
        "model.add(Dense(units=14, activation='softmax', kernel_initializer=initializer, name='o'))\n",
        "\n",
        "model.compile(\n",
        "    loss=\"categorical_crossentropy\", \n",
        "    optimizer=tf.keras.optimizers.Adam(\n",
        "        learning_rate=0.01), \n",
        "    metrics=['accuracy'])\n",
        "\n",
        "def lr_step_decay(epoch, lr):\n",
        "    drop_rate = 0.5\n",
        "    epochs_drop = 10\n",
        "    return 0.001 * math.pow(drop_rate, math.floor(epoch/epochs_drop))\n",
        "\n",
        "model.fit(\n",
        "    X_train,\n",
        "    pd.get_dummies(y_train),\n",
        "    batch_size=128,\n",
        "    epochs=1000,\n",
        "    verbose=1,\n",
        "    validation_data=(X_test, pd.get_dummies(y_test)),\n",
        "    callbacks=[\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "|                | KNN    | SVM    | Decision Tree | Random Forest | Gradient Boosting |\n",
        "|----------------|--------|--------|---------------|---------------|-------------------|\n",
        "|       Accuracy | 0.0788 | 0.1374 |        0.1509 | 0.2410        | 0.1802            |\n",
        "| Tuned Accuracy |        |        |               | 0.2883        | 0.2455            |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature win_odds_1 importance: 0.04369203281101722\n",
            "Feature win_odds_7 importance: 0.04029833396998381\n",
            "Feature win_odds_5 importance: 0.03811723473875967\n",
            "Feature win_odds_6 importance: 0.03783301617170432\n",
            "Feature win_odds_2 importance: 0.037018343962721356\n",
            "Feature win_odds_10 importance: 0.03421853575262265\n",
            "Feature place_odds_7 importance: 0.034173789055530285\n",
            "Feature place_odds_1 importance: 0.033826291912774864\n",
            "Feature win_odds_4 importance: 0.030627278658140047\n",
            "Feature win_odds_3 importance: 0.029765814323145744\n",
            "Feature win_odds_9 importance: 0.02744598093301855\n",
            "Feature place_odds_5 importance: 0.02631526144972493\n",
            "Feature place_odds_6 importance: 0.025295941999975494\n",
            "Feature place_odds_4 importance: 0.024486717466408847\n",
            "Feature place_odds_10 importance: 0.024241156103418683\n",
            "Feature win_odds_12 importance: 0.023989994410590035\n",
            "Feature place_odds_12 importance: 0.022697216793748413\n",
            "Feature place_odds_3 importance: 0.020209184055063906\n",
            "Feature place_odds_9 importance: 0.019605083047131468\n",
            "Feature win_odds_8 importance: 0.019106641894691537\n",
            "Feature win_odds_11 importance: 0.01903291390189986\n",
            "Feature place_odds_2 importance: 0.018697088049990943\n",
            "Feature place_odds_8 importance: 0.01865930839916309\n",
            "Feature place_odds_11 importance: 0.016448471362471892\n",
            "Feature place_odds_14 importance: 0.015946216539302525\n",
            "Feature win_odds_14 importance: 0.015566053900198216\n",
            "Feature declared_weight_10 importance: 0.015451622036884136\n",
            "Feature declared_weight_5 importance: 0.014655277725342957\n",
            "Feature declared_weight_6 importance: 0.013626903564466662\n",
            "Feature declared_weight_8 importance: 0.013046677109491582\n",
            "Feature place_odds_13 importance: 0.012952955900693217\n",
            "Feature declared_weight_2 importance: 0.012863875534576604\n",
            "Feature declared_weight_7 importance: 0.011994143160188238\n",
            "Feature declared_weight_11 importance: 0.010980660952156465\n",
            "Feature declared_weight_4 importance: 0.010945733173135401\n",
            "Feature win_odds_13 importance: 0.010716583025823451\n",
            "Feature declared_weight_12 importance: 0.010020363072092007\n",
            "Feature declared_weight_13 importance: 0.009739382718921653\n",
            "Feature declared_weight_3 importance: 0.009579056236014212\n",
            "Feature declared_weight_1 importance: 0.009142586499185524\n",
            "Feature declared_weight_14 importance: 0.008609371711695735\n",
            "Feature declared_weight_9 importance: 0.007942466706795517\n",
            "Feature actual_weight_12 importance: 0.006662651198057799\n",
            "Feature race_class_4 importance: 0.005817987580699358\n",
            "Feature actual_weight_13 importance: 0.005762861785488175\n",
            "Feature actual_weight_10 importance: 0.0054370512741216785\n",
            "Feature actual_weight_5 importance: 0.005432613686356403\n",
            "Feature actual_weight_3 importance: 0.005381481813295186\n",
            "Feature actual_weight_11 importance: 0.00509964290246586\n",
            "Feature actual_weight_2 importance: 0.004917738855764975\n",
            "Feature actual_weight_6 importance: 0.004463703294384873\n",
            "Feature actual_weight_1 importance: 0.004364859055476613\n",
            "Feature actual_weight_8 importance: 0.004364366831566062\n",
            "Feature distance importance: 0.0042927400173539375\n",
            "Feature actual_weight_9 importance: 0.004280048146432937\n",
            "Feature actual_weight_4 importance: 0.0037654030140899086\n",
            "Feature actual_weight_7 importance: 0.0033840980918832314\n",
            "Feature actual_weight_14 importance: 0.003269496977412878\n",
            "Feature horse_ratings_60-40 importance: 0.0029057095867644\n",
            "Feature horse_country_AUS_13 importance: 0.0024716915188916083\n",
            "Feature horse_country_NZ_13 importance: 0.002028487673870631\n",
            "Feature horse_country_AUS_8 importance: 0.001924026726183305\n",
            "Feature horse_country_NZ_11 importance: 0.0018000605374951398\n",
            "Feature horse_country_AUS_2 importance: 0.0017547304958161285\n",
            "Feature horse_country_AUS_10 importance: 0.0016700683112155959\n",
            "Feature horse_country_AUS_3 importance: 0.0016531285037092009\n",
            "Feature horse_country_NZ_5 importance: 0.0015922186526972696\n",
            "Feature horse_country_NZ_8 importance: 0.0013474086877684228\n",
            "Feature horse_country_NZ_6 importance: 0.0013416179180680493\n",
            "Feature going_GOOD importance: 0.0012889012213360166\n",
            "Feature horse_country_NZ_2 importance: 0.0012672325829062047\n",
            "Feature horse_country_AUS_1 importance: 0.0012635804985575328\n",
            "Feature horse_country_NZ_1 importance: 0.0012033822714139432\n",
            "Feature horse_country_NZ_14 importance: 0.0011982676396630656\n",
            "Feature horse_country_NZ_9 importance: 0.0011888766481946422\n",
            "Feature horse_country_AUS_11 importance: 0.001058725000222585\n",
            "Feature horse_country_AUS_4 importance: 0.0010569732325585305\n",
            "Feature horse_country_AUS_14 importance: 0.0010481252648602457\n",
            "Feature horse_country_AUS_5 importance: 0.0010251041664956077\n",
            "Feature horse_country_NZ_4 importance: 0.0010044779714767768\n",
            "Feature horse_country_AUS_7 importance: 0.0010027974309848376\n",
            "Feature horse_country_NZ_10 importance: 0.000805736261333295\n",
            "Feature horse_country_NZ_3 importance: 0.0007397482723202861\n",
            "Feature horse_country_AUS_9 importance: 0.0007138766399466757\n",
            "Feature race_class_3 importance: 0.0006669097958277205\n",
            "Feature horse_country_NZ_7 importance: 0.000638419290596611\n",
            "Feature horse_ratings_80-60 importance: 0.0005036865048433718\n",
            "Feature horse_country_AUS_12 importance: 0.0005036833776659039\n",
            "Feature horse_country_NZ_12 importance: 0.00047145190696525063\n",
            "Feature horse_country_AUS_6 importance: 0.0004140561959341442\n",
            "Feature config_C+3 importance: 0.0003975324792345515\n",
            "Feature horse_rating_5 importance: 0.00038246182541711694\n",
            "Feature config_A+3 importance: 0.00035130776812857055\n",
            "Feature horse_rating_11 importance: 0.00034299224334531387\n",
            "Feature horse_country_IRE_11 importance: 0.00031362035832826205\n",
            "Feature horse_rating_3 importance: 0.00029594928499410866\n",
            "Feature horse_country_IRE_10 importance: 0.00028109289301571166\n",
            "Feature horse_country_IRE_9 importance: 0.00022921483381840323\n",
            "Feature config_B+2 importance: 0.00021936843802438215\n",
            "Feature horse_rating_9 importance: 0.00017923073754481377\n"
          ]
        }
      ],
      "source": [
        "feature_importances = model_rf_tuned.feature_importances_\n",
        "\n",
        "feature_importances_index = np.argsort(-feature_importances)\n",
        "\n",
        "for i in feature_importances_index[0:100]:\n",
        "\n",
        "    name = X.columns[i]\n",
        "    print(f\"Feature {name} importance: {feature_importances[i]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "1PL_9WoqZraP",
        "T0OSjvXIaTD7"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
